Introduction to Multi-threaded Architectures and Systems in OS
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Introduction to Multi-threaded Architectures and Systems in OS - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
According to me, multithreaded architecture is actually a trend and reliable and easy to applicable solution for upcoming microprocessor design, so I studied four research papers on this topic just to familiarize myself with
the technology involved in this subject.
In today’s world, there is a rapid progress of Very Large Scale Integrated circuit(VLSI) technology that allows processors’ designers to assimilate more functions on a single chip, in future microprocessors will soon be
able to issue more than 12 instructions per machine cycle.
Existing superscalar and Very Large Instruction Word (VLIW) microprocessors utilize multiple functional units in order to exploit instruction-level parallelism from a single thread of control flow. These kinds of
microprocessors are depending on the compiler or the hardware of the system in order to excerpt instruction-level parallelism from programs and to then afterward schedule independent instructions on multiple functional
units on each machine cycle. And when the issue rate of future microprocessors increases, however, the compiler or the hardware will have to extract some more instruction-level parallelism from programs by analyzing a
larger instruction buffer (for example -Reservation Station Entry).
It is very tough to extract enough parallelism with a single thread of control even for a small number of functional units.

Limitation of Single-threaded sequencing mechanism are as follow:
1. In order to exploit instruction-level parallelism, independent instructions from different-different basic building-blocks in a single instruction stream needed to be examined and issued together.
2. As the issue rate increases, a larger instruction buffer (for example -Reservation Station Entry) is needed to contain several basic building-blocks, which will be control dependency on different branch conditions, but
their examination must be hit together.
3. And Instruction-level analytical execution with branch prediction is also needed to move independent instructions across basic block boundaries. This problem is especially serious when a compiler attempts to
pipeline a loop with many conditional branches.
4. Here, the accuracy of systems is very low.
5. Here, the speed of the system is quite low as compare to multithreaded architectures.
Features of Multithreaded Architecture are as follow:
1. In its regular form, a multithreaded processor is made up of many numbers of thread processing elements that are connected to each other with a unidirectional ring which helps in performing the system in a better
way.
2. All the multiple thread processing elements have their own private level-one instruction cache, but they’ll share the level-one data cache and the level-two cache.
3. In the multithreaded systems also some shared register files present that used for maintaining some global registers and a lock register.
4. During run-time, the multiple thread processing elements, each has its own program counter and instruction execution path, which can then easily fetch and execute instructions from multiple program locations
simultaneously.
5. Each of the present thread processing element always has a private memory buffer to cache speculative stores and which is also used to support run-time data dependency checking.
6. Fortunately, the compiler statically divides the control flow graph of a program into threads that coincide with a portion of the control flow graph. A thread performs a round of computation on a set of data that has
no, or only a few, dependencies with other concurrent threads. The compiler will later determine the granularity (it’ll directly affect the performance) of the threads, which are hardly one or several iterations of a
loop.
7. The real execution of a program initiates from its entry thread. This entry thread can then transfers the systems’ processing to a successor thread on some another thread processing element. The successor thread can
further transfer the systems’ processing to its own successor thread. This process will continue until all thread processing elements are busy with some appropriate tasks.
8. When the multiple threads are executed on the multithreaded processor, the primeval thread in the sequential order is referred to as the head thread. And then all of the other threads which are derived from it are
called successor threads. Once the head thread completes its computation, then further it will retire and release the thread processing element and then its (just) successor thread then becomes the new head thread.
This completion and retirement of the threads must have to follow the original sequential execution order.
9. In the multithreaded system (or model), a thread can transfer the systems’ processing one of its successor threads with or without control speculation. When the transfer of the systems’ processing from head to its
successor thread without control speculation, the thread performing the fork operation must ensure that all of the control dependencies of the newly generated successor thread have been satisfied.
10. If the transfer of the systems’ processing from the head to its successor thread with control speculation, however, it must be later on verifying all of the speculated control dependencies. If any of the speculated
control dependencies are not true, the thread must have to issue a command to kill the successor thread and all of its subsequent threads.
11. In general, the multithreaded architecture will use a thread pipelining execution model in order to enforce data dependencies between concurrent threads. Unlike the instruction pipelining mechanism in a superscalar
processor, where instruction sequencing, data dependencies checking, and forwarding are performed by processor hardware automatically, the multithreaded architecture performs thread initiation and data
forwarding through explicit thread management and communication instructions.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/introduction-to-multi-threaded-architectures-and-systems-in-os/
✍
Write a Testimonial

Difference between Virtual Machines and Containers
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Virtual Machines and Containers - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Virtual machines and Containers are two ways of deploying multiple, isolated services on a single platform.
Virtual Machine:
It runs on top of an emulating software called the hypervisor which sit between the hardware and the virtual machine. The hypervisor is the key to enable virtualization. It manages the sharing of physical resources into
virtual machines. Each virtual machine runs its own guest operating system. They are less agile and have low portability than containers.
Container:
It sits on the top of a physical server and its host operating system. They share a common operating system that requires care and feeding for bug fixes and patches. They are more agile and have high portability than
virtual machines.

Let’s see the difference between Virtual machines and Containers.
SNo.

2.
3.
4.
5.
6.
7.

Virtual Machines(VM)
VM is piece of software that allows you to install other software inside of it so you basically control it virtually as opposed to
installing the software directly on the computer.
Applications running on VM system can run different OS.
VM virtualizes the computer system.
VM size is very large.
VM takes minutes to run, due to large size.
VM uses a lot of system memory.
VM is more secure.

8.

VM’s are useful when we require all of OS resources to run various applications.

9.

Examples of VM are: KVM, Xen, VMware.

1

Containers
While a container is a software that allows different functionalities of an
application independently.
While applications running in a container environment share a single OS.
While containers virtualize the operating system only.
While the size of container is very light; i.e. a few megabytes.
While containers take a few seconds to run.
While containers require very less memory.
While containers are less secure.
While containers are useful when we are required to maximise the running
applications using minimal servers.
While examples of containers are:RancherOS, PhotonOS, Containers by Docker.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-virtual-machines-and-containers/
✍
Write a Testimonial

Difference between Multi-tasking and Multi-threading
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Multi-tasking and Multi-threading - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Multiprogramming, multitasking, multithreading and multiprocessing
Multitasking:
Multitasking is when a CPU is provided to execute multiple tasks at a time. Multitasking involves often CPU switching between the tasks, so that users can collaborate with each program together. Unlike multithreading,
In multitasking, the processes share separate memory and resources. As multitasking involves CPU switching between the tasks rapidly, So the little time is needed in order to switch from the one user to next.

Multithreading:
Multithreading is a system in which many threads are created from a process through which the computer power is increased. In multithreading, CPU is provided in order to execute many threads from a process at a time,
and in multithreading, process creation is performed according to cost. Unlike multitasking, multithreading provides the same memory and resources to the processes for execution.

Let’s see the difference between multitasking and multithreading:
S.NO
1.
2.
3.
4.
5.
6.

Multitasking

Multithreading

In multitasking, users are allowed to perform many tasks by CPU.
While in multithreading, many threads are created from a process through which computer power is increased.
Multitasking involves often CPU switching between the tasks.
While in multithreading also, CPU switching is often involved between the threads.
In multitasking, the processes share separate memory.
While in multithreading, processes are allocated same memory.
Multitasking component involves multiprocessing.
While multithreading component does not involve multiprocessing.
In multitasking, CPU is provided in order to execute many tasks at a time.
While in multithreading also, CPU is provided in order to execute many threads from a process at a time.
In multitasking, processes don’t share same resources, each process is allocated separate resources. While in multithreading, each process share same resources.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-multi-tasking-and-multi-threading/
✍
Write a Testimonial

Difference between Long-Term and Medium-Term Scheduler
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Long-Term and Medium-Term Scheduler - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Process Schedulers
Long-Term Scheduler:
Long-term schedulers are called job schedulers. The long-term scheduler controls the programs that are selected within the system for processing. In this, programs are found out during a queue and therefore the best one
job is chosen as per the need and it selects the processes from the job pool and these process are loaded into memory so as to execute. It provides the restraint on the degree of multi-programming.
Medium-Term Scheduler:
Medium-term scheduler called as process swapping scheduler as it is a part of swapping. Through this scheduler, processes are removed from memory. Medium-term scheduler cut down the degree of degree of multiprogramming. In this scheduler, if a process requests I/O, it can be suspended and it cannot make any progress towards the completion of the suspended process. During this condition, to get rid of the method from
memory and make space for other processes, the suspended process is moved to the auxiliary storage. This process is named swapping, and therefore the process is claimed to be swapped out or unrolled. Swapping could
also be necessary to enhance the process mix.

Let’s see the difference between Long-Term and Medium-Term Scheduler:
S.NO
1.

Long-Term Scheduler

Medium-Term Scheduler
Whereas medium-term scheduler is called as process swapping scheduler.
While in this, process can be revived in the memory as well as process execution can also be
carried out.

4.

Long-term scheduler is called as job scheduler.
In long-term scheduler, the process are selected from the job pool and these process are loaded into memory in order
to execute.
Long-term scheduler is can be or can’t be a part of a time sharing system. if it is then it is a nominal in time sharing
system.
The speed of long -term scheduler is less than medium-term scheduler.

5.

Long-term scheduler provides the restraint on the DOM(Degree of Multi-programming).

While medium-term scheduler cut down the degree of DOM(Degree of Multi-programming).

2.
3.

While medium-term scheduler is always in a time sharing system.
While the speed of medium -term scheduler is comparatively higher than longer-term scheduler.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-long-term-and-medium-term-scheduler/
✍
Write a Testimonial

Responsibilities of a File Manager
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Responsibilities of a File Manager - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Functions of Operating System
The File Manager is a system software responsible for the creation, deletion, modification of the files and managing their access, security and the resources used by them. These functions are performed in collaboration
with the Device Manager.
The File Manager has big responsibilities in it’s hands. It is in charge of the physical components of the computer system, information resources and the policies to store and distribute the files. It’s responsibilities include :
1. Keeping track of each file.
In the system, the File Manager keeps track of each file through directories that contain the file’s name, location in secondary storage and other important information.
2. Using of the policies that determine where & how the files would be stored, in order to efficiently use the available storage and provide access to them.
It has a set of predetermined policies that decides where and how the files would be stored and how the user will be able to gain access to them. Also, it must also determine who can access what material. This
involves flexibility of access to information and it’s protection. It is done by allowing the user the access to shared files and public directories. The operating system must also protect it’s file from system
malfunctions or tampering of files.
3. Allocating each file when the user is granted access to them, and recording their use.
The File Manager allocates the files by activating the appropriate secondary storage device and loading the file into the main memory while also updating the records of who is using what file.
4. Deallocating the files when their use in finished and are not needed, and also communicating to others about it’s availability which are waiting for it.
It deallocates the file by updating the file tables and rewriting the updated file into the secondary storage, then communicating with other processes and notifying them about it’s availability.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/responsibilities-of-a-file-manager/
✍
Write a Testimonial

Conditions for Deadlock in Operating System

​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Conditions for Deadlock in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Deadlock is a situation which involves the interaction of more than one resources and processes with each other.
We can visualise the occurrence of deadlock as a situation where there are two people on a staircase. One is ascending the staircase while the other is descending. The staircase is so narrow that it can only fit one person at
a time. As a result, one has to retreat while the others moves on and uses the staircase. Once that person is finished, the other one can use that staircase. But here, none of the people is willing to retreat and waits for the
each other to retreat. None of them is able to use the staircase. The people here is the process and the staircase is the resource.
When a process requests for the resource that is been held another process which needs another resource to continue, but is been held by the first process, then it is called a deadlock.

There are 4 conditions necessary for the occurrence of a deadlock. They can be understood with the help of the above illustrated example of staircase :
1. Mutual Exclusion:
When two people meet in the landings, they can’t just walk through because there is space only for one person. This condition to allow only one person (or process) to use the step between them (or the resource) is
the first condition necessary for the occurrence of the deadlock.
2. Hold and Wait:
When the 2 people refuses to retreat and hold their grounds, it is called holding. This is the next necessary condition for the the deadlock.
3. No Preemption:
For resolving the deadlock one can simply cancel one of the processes for other to continue. But Operating System doesn’t do so. It allocates the resources to the processors for as much time needed until the task is
completed. Hence, there is no temporary reallocation of the resources. It is third condition for deadlock.
4. Circular Wait:
When the two people refuses to retreat and wait for each other to retreat, so that they can complete their task, it is called circular wait. It is the last condition for the deadlock to occur.
Note:
All the 4 conditions are necessary for the deadlock to occur. If any one is prevented or resolved, the deadlock is resolved.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/conditions-for-deadlock-in-operating-system/
✍
Write a Testimonial

Redundant Array of Independent Disks (RAID) | Set 2
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Redundant Array of Independent Disks (RAID) | Set 2 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Redundant Array of Independent Disks (RAID) is a set of several physical disk drives that Operating System see as a single logical unit. It played a significant role in narrowing the gap between increasingly fast processors
and slow disk drives.
The basic principle behind RAID is that several smaller-capacity disk drives are better in performance than some large-capacity disk drives because through distributing the data among several smaller disks, the system
can access data from them faster, resulting in improved I/O performance and improved data recovery in case of disk failure.

A typical disk array configuration consists of small disk drives connected to a controller housing the software and coordinating the transfer of data in the disks to a large capacity disk connected to I/O subsystem.
Note that this whole configuration is viewed as a single large-capacity disk by the OS.
Data is divided into segments called strips, which are distributed across the disks in the array.
A set of consecutive strips across the disks is called a stripe.
The whole process is called striping.
Besides introducing the concept of redundancy which helps in data recovery due to hardware failure, it also increases the cost of the hardware.
The whole system of RAID is divided in seven levels from level 0 to level 6. Here, the level does not indicate hierarchy, but indicate different types of configurations and error correction capabilities.
Level 0 :
RAID level 0 is the only level that cannot recover from hardware failure, as it doesn’t provide error correction or redundancy. Therefore, it can’t be called the true form of RAID. However, it sure offers the same
significant benefits as others – to the OS this group of devices appears to be a single logical unit.

As illustrated above, when the OS issues a command, that can be transferred in parallel to the strips, improving performance greatly.
Level 1 :
RAID level 1 not only uses the process of striping, but also uses mirrored configuration by providing redundancy, i.e., it creates a duplicate set of all the data in a mirrored array of disks, which as a backup in case of
hardware failure. If one drive fails, data can be retrieved immediately from the mirrored array of disks. With this, it becomes a reliable system.

As illustrated above data has been copied in yet another array of disk as a backup.
The disadvantage includes the writing of the data twice, once in main disks, and then in backup disks. However, process time can be saved by doing the copying the data in parallel to the main writing of the data.
Another disadvantage is that it requires double amount of space, and so is expensive. But, the advantage of having a backup and no worry for data loss nullifies this disadvantage.
Level 2 :
RAID level 2 makes the use of very small strips (often of the size of 1 byte) and a hamming code to provide redundancy (for the task of error detection, correction, etc.).
Hamming Code : It is an algorithm used for error detection and correction when the data is being transferred. It adds extra, redundant bits to the data. It is able to correct single-bit errors and correct double-bit errors.

This configuration has a disadvantage that it is an expensive and a complex configuration to implement because of the number of additional arrays, which depend on the size of strips, and also all the drives must be highly
synchronized.
The advantage includes that if a drive should malfunction, then only one disk would be affected and the data could be quickly recovered.
Level 3 :
RAID level 3 is a configuration that only needs one disk for redundancy. Only one parity bit is computed for each strip and is stored in designated redundant disk.
If a drive malfunctions, the RAID controller considers all the bits coming from that disk to be 0 and notes the location of that malfunctioning disk. So, if the data being read has a parity error, then the controller knows that
the bit should be 1 and corrects it.
If data is being written to the array that has a malfunctioning device, then the controller keeps the parity consistent so as to regenerate data when the array is replaced. The system returns to normal when the failed disk is
replaced and it’s contents are regenerated on the new disk(or array).

Level 4 :
RAID level 4 uses the same concept used in level 0 & level 1, but also computes a parity for each strip and stores this parity in the corresponding strip of the parity disk.
The advantage of this configuration is that if a disk fails, the data can be still recovered from the parity disk.
Parity is computed every time a write command is executed. But, when the data is to be rewritten inside the disks, the RAID controller must be able to update the data and parity disks. So, the parity disks need to be
accessed whenever a write or rewrite operations are to be executed. This creates a situation known as the bottleneck which is the main disadvantage of this configuration.

Level 5 :
RAID level 5 is a modification level 4. In level 4, only one disk is designated for parity storing parities. But in level 5, it distributes the parity disks across the disks in the array.

The advantage of this configuration is that it avoids the condition of bottleneck which was created in level 4.
The disadvantage of this configuration is that during regeneration of data when a disk fails is complicated.
Level 6 :
RAID level 6 provides an extra degree of error detection and correction. It requires 2 different parity calculations.
One calculation is the same as the one used level 4 and 5, other calculation is an independent data-check algorithm. Both the parities are stored on separate disks across the array, which corresponds to the data strips in the
array.

The advantage of this configuration is that if even 2 disks fails or malfunction, then also the data can be recovered.
The disadvantage of this configuration includes:
The redundancy increases the time required to write the data because now the data is to be also written on the second parity disk.
In this configuration another disk is designated as the parity disk, which decreases the number of data disks in the array.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/redundant-array-of-independent-disks-raid-set-2/
✍
Write a Testimonial

Implementation of Least Recently Used (LRU) page replacement algorithm using Counters
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Implementation of Least Recently Used (LRU) page replacement algorithm using Counters - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Least Recently Used (LRU) Page Replacement algorithm
Least Recently Used page replacement algorithm replaces the page which is not used recently.
Implementation:
In this article, LRU is implemented using counters, a ctime (i.e., counter) variable is used to represent the current time, it is incremented for every page of the reference array. A vector of pair is used to represent the page
frames, the 1’st variable of the pair is the page number and the second variable is the current time. When a new page is to be inserted and the frames are full, the page with minimum ctime is deleted (as it is the least
recently used page). If the page is accessed again the value of ctime is updated.
Note:
Least ctime (counter / current time) value represents the least recently used page.

Examples:
Reference array is
Output :​
When the number of
The number of page
​
Reference array is
Output :​
When the number of
The number of page

Code:
filter_none

: 0, 0, 0, 2, 3, 0, 5, 7, 1, 2, 0, 8​
frames is : 3​
faults are : 9​
: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 0, 0​
frames is : 3​
faults are : 15

edit
close
play_arrow
link
brightness_4
code
#include <bits/stdc++.h>
using namespace std;
// To calculate the number of page faults
void pageFaults(int frame_size, int* ref, int len, int ctime)
{
// Count variable to count the
// number of page faults
int cnt = 0;
// Arr to simulate frames
vector<pair<int, int> > arr;
// To initialise the array
for (int i = 0; i < frame_size; i++) {
arr.push_back(make_pair(-1, ctime));
}
int page;
for (int i = 0; i < len; i++) {
page = ref[i];
auto it = arr.begin();
for (it = arr.begin(); it != arr.end(); it++) {
if (it->first == page) {
break;
}
}
// If page is found
if (it != arr.end()) {
// update the value of
// current time
it->second = ctime;
}
// If page is not found
else {
// Find the page with min value of ctime,
// as it will be the least recently used
vector<pair<int, int> >::iterator pos;
pos = arr.begin();
int min = pos->second;
for (auto itr = arr.begin(); itr != arr.end(); itr++) {
if (itr->second < min) {
pos = itr;
min = pos->second;
}
}
// Erase this page from the frame vector
arr.erase(pos);
// Insert the new page
arr.push_back(make_pair(page, ctime));
cnt++;
}
ctime++;
}
cout << "The number of page faults is : " << cnt << endl;
}
int main()
{
// This is the reference array
int ref[] = { 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5 };
int len = sizeof(ref) / sizeof(ref[0]);
int frame_size = 3;
// Ctime represents current time,
// it is incremented for every page
int ctime = 0;
pageFaults(frame_size, ref, len, ctime);
}

chevron_right
filter_none
Output:
​
The number of page faults is : 10​

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/implementation-of-least-recently-used-lru-page-replacement-algorithm-using-counters/
✍
Write a Testimonial

Virtual Machines in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Virtual Machines in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Virtual Machine abstracts the hardware of our personal computer such as CPU, disk drives, memory, NIC (Network Interface Card) etc, into many different execution environments as per our requirements, hence giving
us a feel that each execution environment is a single computer. For example, VirtualBox.
When we run different processes on an operating system, it creates an illusion that each process is running on a different processor having its own virtual memory, with the help of CPU scheduling and virtual-memory
techniques. There are additional features of a process that cannot be provided by the hardware alone like system calls and a file system. The virtual machine approach does not provide these additional functionalities but it
only provides an interface that is same as basic hardware. Each process is provided with a virtual copy of the underlying computer system.

We can create a virtual machine for several reasons, all of which are fundamentally related to the ability to share the same basic hardware yet can also support different execution environments, i.e., different operating
systems simultaneously.

The main drawback with the virtual-machine approach involves disk systems. Let us suppose that the physical machine has only three disk drives but wants to support seven virtual machines. Obviously, it cannot allocate
a disk drive to each virtual machine, because virtual-machine software itself will need substantial disk space to provide virtual memory and spooling. The solution is to provide virtual disks.
Users are thus given their own virtual machines. After which they can run any of the operating systems or software packages that are available on the underlying machine. The virtual-machine software is concerned with
multi-programming multiple virtual machines onto a physical machine, but it does not need to consider any user-support software. This arrangement can provide a useful way to divide the problem of designing a multi-user
interactive system, into two smaller pieces.
Advantages:
1. There are no protection problems because each virtual machine is completely isolated from all other virtual machines.
2. Virtual machine can provide an instruction set architecture that differs from real computers.
3. Easy maintenance, availability and convenient recovery.
Disadvantages:
1. When multiple virtual machines are simultaneously running on a host computer, one virtual machine can be affected by other running virtual machines, depending on the workload.
2. Virtual machines are not as efficient as a real one when accessing the hardware.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/virtual-machines-in-operating-system/
✍
Write a Testimonial

Thread Control Block in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Thread Control Block in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Very similar to Process Control Blocks (PCBs) which represents processes, Thread Control Blocks (TCBs) represents threads generated in the system. It contains information about the threads, such as it’s ID and states.

The components have been defined below:

Thread ID: It is a unique identifier assigned by the Operating System to the thread when it is being created.
Thread states: These are the states of the thread which changes as the thread progresses through the system
CPU information: It includes everything that the OS needs to know about, such as how far the thread has progressed and what data is being used.
Thread Priority: It indicates the weight (or priority) of the thread over other threads which helps the thread scheduler to determine which thread should be selected next from the READY queue.
A pointer which points to the process which triggered the creation of this thread.
A pointer which points to the thread(s) created by this thread.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/thread-control-block-in-operating-system/
✍

Write a Testimonial

Thread States in Operating Systems
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Thread States in Operating Systems - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
When a thread moves through the system, it is always in one of the five states:
(1)
(2)
(3)
(4)
(5)

Ready​
Running​
Waiting​
Delayed​
Blocked

Excluding CREATION and FINISHED state.

1.
2.
3.
4.
5.

When an application is to be processed, then it creates a thread.
It is then allocated the required resources(such as a network) and it comes in the READY queue.
When the thread scheduler (like a process scheduler) assign the thread with processor, it comes in RUNNING queue.
When the process needs some other event to be triggered, which is outsides it’s control (like another process to be completed), it transitions from RUNNING to WAITING queue.
When the application has the capability to delay the processing of the thread, it when needed can delay the thread and put it to sleep for a specific amount of time. The thread then transitions from RUNNING to
DELAYED queue.
An example of delaying of thread is snoozing of an alarm. After it rings for the first time and is not switched off by the user, it rings again after a specific amount of time. During that time, the thread is put to sleep.

6. When thread generates an I/O request and cannot move further till it’s done, it transitions from RUNNING to BLOCKED queue.
7. After the process is completed, the thread transitions from RUNNING to FINISHED.
The difference between the WAITING and BLOCKED transition is that in WAITING the thread waits for the signal from another thread or waits for another process to be completed, meaning the burst time is specific.
While, in BLOCKED state, there is no specified time (it depends on the user when to give an input).
In order to execute all the processes successfully, the processor needs to maintain the information about each thread through Thread Control Blocks (TCB).

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/thread-states-in-operating-systems/
✍
Write a Testimonial

Program to show Belady’s Anomaly
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Program to show Belady's Anomaly - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Belady’s Anomaly
Belady’s Anomaly is when the number of page faults increase even after increasing the number of frames.
In this article, we demonstrate Belady’s Anomaly using FIFO page replacement algorithm.

Examples:
Reference array is: 1, 2, 3, 4, 1, 2, 5,
Output :​
When number of frames is 3, page fault :
When number of frames is 4, page fault :
​
Reference array is: 0, 1, 5, 3, 0, 1, 4,
Output :​
When number of frames is 3, page fault :
When number of frames is 4, page fault :

1, 2, 3, 4, 5 ​
9​
10​
0, 1, 5, 3, 4​
9​
10

Implementation :
FIFO page replacement algorithm is used to showcase the Belady’s Anomaly. Firstly, an array of size equal to the number of frames is used, this array simulates the page frames, the operations on this array are performed
in a circular array type fashion. A count variable is used to calculate the page fault, this variable is incremented whenever an element is inserted / rewritten in the page frame array.

In this code:
Two frame sizes 3 and 4 are tested respectively, with increase in number of frames the page faults should have decreased, but an anomaly is witnessed as 3 frames result in 9 page faults whereas 4 frames result in 10 page
faults. This is Belady’s Anomaly, it is observed in special cases of reference array and frame size.
C++
filter_none
edit
close
play_arrow
link
brightness_4
code
#include <bits/stdc++.h>
using namespace std;
void pageFault(int frame_size, int* ref, int len)
{
// To dynamically allocate an array,
// it represents the page frames.
int* arr = new int[frame_size];
// To initialize the array
for (int i = 0; i < frame_size; i++) {
arr[i] = -1;
}
// To count page faults
int cnt = 0;
int start = 0;
int flag;
int elm;
for (int i = 0; i < len; i++) {
elm = ref[i];
// Linear search to find if the page exists
flag = 0;
for (int j = 0; j < frame_size; j++) {
if (elm == arr[j]) {
flag = 1;
break;
}
}
// If the page doesn't exist it is inserted,
// count is incremented
if (flag == 0) {
if (start < frame_size) {
arr[start] = elm;
start++;
}
else if (start == frame_size) {
arr[0] = elm;
start = 1;
}
cnt++;
}
}
cout << "When the number of frames are: " << frame_size << ", ";
cout << "the number of page faults is : " << cnt << endl;
}
int main()
{
// Reference array
int ref[] = { 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5 };
int len = sizeof(ref) / sizeof(ref[0]);
// The frame size
int frame_size = 3;
pageFault(frame_size, ref, len);
// Increase value of frame size
frame_size = 4;
// The page fault increases
// even after increasing the
// the number of frames.
// This is Belady's Anomaly
pageFault(frame_size, ref, len);
}

chevron_right
filter_none
Java
filter_none
edit
close
play_arrow
link
brightness_4
code
// Java Implementation of the above approach
class GFG
{
static void pageFault(int frame_size,
int []ref, int len)
{
// To dynamically allocate an array,
// it represents the page frames.
int []arr = new int[frame_size];

// To initialize the array
for (int i = 0; i < frame_size; i++)
{
arr[i] = -1;
}
// To count page faults
int cnt = 0;
int start = 0;
int flag;
int elm;
for (int i = 0; i < len; i++)
{
elm = ref[i];
// Linear search to find
// if the page exists
flag = 0;
for (int j = 0; j < frame_size; j++)
{
if (elm == arr[j])
{
flag = 1;
break;
}
}
// If the page doesn't exist it is inserted,
// count is incremented
if (flag == 0)
{
if (start < frame_size)
{
arr[start] = elm;
start++;
}
else if (start == frame_size)
{
arr[0] = elm;
start = 1;
}
cnt++;
}
}
System.out.print("When the number of frames are: " +
frame_size + ", ");
System.out.println("the number of page faults is : " + cnt);
}
// Driver Code
public static void main (String[] args)
{
// Reference array
int ref[] = { 1, 2, 3, 4, 1, 2,
5, 1, 2, 3, 4, 5 };
int len = ref.length;
// The frame size
int frame_size = 3;
pageFault(frame_size, ref, len);
// Increase value of frame size
frame_size = 4;
// The page fault increases
// even after increasing the
// the number of frames.
// This is Belady's Anomaly
pageFault(frame_size, ref, len);
}
}
// This code is contributed by AnkitRai01

chevron_right
filter_none
Python3
filter_none
edit
close
play_arrow
link
brightness_4
code
# Python3 Implementation of the above approach
def pageFault(frame_size, ref, len):
# To dynamically allocate an array,
# it represents the page frames.
arr = [0] * frame_size;
# To initialize the array
for i in range(frame_size):
arr[i] = -1;
# To count page faults
cnt = 0;
start = 0;
for i in range(len):
elm = ref[i];
# Linear search to find if the page exists
flag = 0;
for j in range(frame_size):
if (elm == arr[j]):
flag = 1;
break;
# If the page doesn't exist it is inserted,
# count is incremented
if (flag == 0):
if (start < frame_size):
arr[start] = elm;
start += 1;
elif (start == frame_size):
arr[0] = elm;
start = 1;
cnt += 1;
print("When the number of frames are: ",
frame_size, end = ", ");
print("the number of page faults is : ", cnt);
# Driver Code

# Reference array
ref = [1, 2, 3, 4, 1, 2,
5, 1, 2, 3, 4, 5 ];
len = len(ref);
# The frame size
frame_size = 3;
pageFault(frame_size, ref, len);
# Increase value of frame size
frame_size = 4;
# The page fault increases
# even after increasing the
# the number of frames.
# This is Belady's Anomaly
pageFault(frame_size, ref, len);
# This code is contributed by 29AjayKumar

chevron_right
filter_none
C#
filter_none
edit
close
play_arrow
link
brightness_4
code
// C# Implementation of the above approach
using System;
class GFG
{
static void pageFault(int frame_size,
int []refer, int len)
{
// To dynamically allocate an array,
// it represents the page frames.
int []arr = new int[frame_size];
// To initialize the array
for (int i = 0; i < frame_size; i++)
{
arr[i] = -1;
}
// To count page faults
int cnt = 0;
int start = 0;
int flag;
int elm;
for (int i = 0; i < len; i++)
{
elm = refer[i];
// Linear search to find
// if the page exists
flag = 0;
for (int j = 0; j < frame_size; j++)
{
if (elm == arr[j])
{
flag = 1;
break;
}
}
// If the page doesn't exist it is inserted,
// count is incremented
if (flag == 0)
{
if (start < frame_size)
{
arr[start] = elm;
start++;
}
else if (start == frame_size)
{
arr[0] = elm;
start = 1;
}
cnt++;
}
}
Console.Write("When the number of frames are: " +
frame_size + ", ");
Console.WriteLine("the number of page " +
"faults is : " + cnt);
}
// Driver Code
public static void Main()
{
// Reference array
int []refer = { 1, 2, 3, 4, 1, 2,
5, 1, 2, 3, 4, 5 };
int len = refer.Length;
// The frame size
int frame_size = 3;
pageFault(frame_size, refer, len);
// Increase value of frame size
frame_size = 4;
// The page fault increases
// even after increasing the
// the number of frames.
// This is Belady's Anomaly
pageFault(frame_size, refer, len);
}
}
// This code is contributed by kanugargng

chevron_right
filter_none
Output:
When the number of frames are: 3, the number of page faults is : 9​

When the number of frames are: 4, the number of page faults is : 10

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : AnkitRai01, kanugargng, 29AjayKumar

Source
https://www.geeksforgeeks.org/program-to-show-beladys-anomaly/
✍
Write a Testimonial

Typical Multiprocessing Configuration
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Typical Multiprocessing Configuration - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Multiprocessing is the situation in which more than one processor are working in unison. So, they must be well configured so as not to generate any type of problem.
There are typically 3 types of configurations: Master / Slave Configuration, Loosely Coupled Configuration, and Symmetric Configuration. These are explained are following below.
1. Master / Slave Configuration:
The master/slave configuration is a single processor system where extra slave processors are working, managed by primary master processor. It is an asymmetrical system.

The work of the master processor is to manage the entire system consisting of files, devices, main memory and the slave processors. It maintains the status of all the processes, schedules the work for the slave processor
and executes all control programs. It is also responsible for storage management. This type of configuration is suitable for computing environments where the processing time needs to be divided between front end and
back end processor.
The advantage of this configuration is that it is simple to understand.
The disadvantages include:
It is as reliable as a single processor system, i.e., if the master processor fails the entire system fails.
It creates more overhead charges. There would be situations when the slave processors would be free before the master processor could assign them another task. Then it takes the valuable time of processing.
After each task completed by the slave processors, it interrupts the master processor for some operating system intervention, like I/O requests. This creates long queues at master level processor.
2. Loosely Coupled Configuration:
In this type of configuration, there are several complete computer systems with their own memory, I/O devices, CPU and operating system.

Each processor controls it’s own resources (I/O devices, memory, etc.) and their own commands and management tables. Each processor can also communicate and cooperate with each other.
When a job is given, it is assigned to one processor and that processor works on that task till it’s completion. So, there must be global tables to to indicate which processor has been given a specific task. Also for the system

to be well balanced, job scheduling must be done on various parameters and according to various policies.
The advantage of this configuration is that it isn’t prone to catastrophic failure. If a processor fails, other can continue their work independently.
The disadvantage of this configuration is that it is difficult to detect if a processor has failed.
3. Symmetric Configuration:
In symmetric configuration processor scheduling is decentralized. A single copy of the OS and a table listing each process and it’s status is stored in memory common and accessible to all the processors, so that each
processor can use the algorithms to decide which job to run next.

Advantages –
It is more reliable than loosely coupled configuration.
It uses the resources effectively.
It well manages the load of jobs.
It can degrade gracefully at the time of failure.
Disadvantages –
Whenever a process is interrupted, it’s processor updates the the corresponding entry in the process list and finds another process to run. This means that not only all the processors are kept busy, but also other
processors may also be executing that job (like I/O request) at the same time. This increases the chances of conflict between processors.
It is the most difficult configuration to implement, as the system must be well synchronized as to avoid any type of races or deadlocks.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/typical-multiprocessing-configuration/
✍
Write a Testimonial

Clairvoyant Shortest Job first (SJF)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Clairvoyant Shortest Job first (SJF) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
In this article, we discuss about Clairvoyant SJF. It is a theoretical concept in which the algorithm looks in the future and waits for the shortest process to arrive, this results in the least average waiting time.
Difference between Clairvoyant SJF and Shortest Job First :
Both algorithms work on the same principle of allocating CPU time to the shorter process. The difference lies in the fact that Clairvoyant can look into the future and wait for the shortest process and allocate the resource
accordingly, whereas SJF has to allocate the resources to the process which have already arrived (i.e., are waiting the ready queue).
Examples:
Input: The processes are,

Process id
p1
p2
p3
p4

Arrival time
0
1
3
4

Burst time
5
​
2
​
1
​
3

​

Output: Process scheduling according to Clairvoyant SJF is,
Process id
p3
p2
p4
p1

Arrival time
3
1
4
0

Burst time
1
​
2
​
3
​
5

​

The average waiting time is : 2.5
Output: Process scheduling according to Shortest Job First is,
Process id
p1
p3
p2
p4

Arrival time
0
3
1
4

Burst time
5
​
1
​
2
​
3

​

The average waiting time is : 2.75
Note:
Clairvoyant SJF and SJF will give the same result if all the processes arrive at the same time.
Input: The processes are,
Process id
p1
p2
p3
p4

Arrival time
0
0
0
0

Burst time
4​
9​
5​
3

​

Output: Process scheduling according to Clairvoyant SJF is,
Process id
p4

Arrival time
0

Burst time
3​

​

p1
p3
p2

0
0
0

4​
5​
9

The average waiting time is : 5.5
Output: Process scheduling according to Shortest Job First is,
Process id
p4
p1
p3
p2

Arrival time
0
0
0
0

Burst time
3​
4​
5​
9

​

The average waiting time is : 5.5
Code :
filter_none
edit
close
play_arrow
link
brightness_4
code
#include <bits/stdc++.h>
#define SIZE 4
using namespace std;
// Structure to store the information about the process
typedef struct proinfo {
string pname; // Process name
int atime; // Arrival time
int btime; // Burst time
} proinfo;
// This function schedules the process
// according to the Clairvoyant SJF scheduling algorithm.
void clairvoyantSjf(proinfo* arr)
{
// To sort the processes according to the burst time
int index = 0;
for (int i = 0; i < SIZE - 1; i++) {
index = i;
for (int j = i + 1; j < SIZE; j++) {
if (arr[j].btime < arr[index].btime) {
index = j;
}
}
swap(arr[i], arr[index]);
}
}
void display(proinfo* arr)
{
cout << endl;
cout << "Process id"
<< "\t";
cout << "Arrival time"
<< "\t";
cout << "Burst time"
<< "\t";
cout << endl;
for (int i = 0; i < SIZE; i++) {
cout << arr[i].pname << "\t\t";
cout << arr[i].atime << "\t\t";
cout << arr[i].btime << "\t\t";
cout << endl;
}
}
// To calculate the average waiting time
void avgWait(proinfo* arr)
{
int ctime = 0;
int twait = 0;
for (int i = 0; i < SIZE; i++) {
twait += abs(arr[i].atime - ctime);
ctime += arr[i].btime;
}
cout << "The average waiting time is: " << (float)twait / SIZE << endl;
}
int main()
{
// Array of process
proinfo arr[SIZE];
arr[0] = { "p1", 0,
arr[1] = { "p2", 1,
arr[2] = { "p3", 3,
arr[3] = { "p4", 4,

info structures.
5
2
1
3

};
};
};
};

cout << "Process scheduling according to Clairvoyant SJF is: " << endl;
clairvoyantSjf(arr);
// To display the schedule
display(arr);
// to calculate the Average waiting time
avgWait(arr);
}

chevron_right
filter_none
Output:
​
Process scheduling according to Clairvoyant SJF is: ​
​
Process id
Arrival time
Burst time
​
p3
3
1
​
p2
1
2
​
p4
4
3
​
p1
0
5
​
The average waiting time is: 2.5​

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/clairvoyant-shortest-job-first-sjf/
✍

Write a Testimonial

Static and Dynamic Linking in Operating Systems
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Static and Dynamic Linking in Operating Systems - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Static Linking:
When we click the .exe (executable) file of the program and it starts running, all the necessary contents of the binary file have been loaded into the process’s virtual address space. However, most programs also need to run
functions from the system libraries, and these library functions also need to be loaded.
In the simplest case, the necessary library functions are embedded directly in the program’s executable binary file. Such a program is statically linked to its libraries, and statically linked executable codes can commence
running as soon as they are loaded.
Disadvantage:
Every program generated must contain copies of exactly the same common system library functions. In terms of both physical memory and disk-space usage, it is much more efficient to load the system libraries into
memory only once. Dynamic linking allows this single loading to happen.

Dynamic Linking:
Every dynamically linked program contains a small, statically linked function that is called when the program starts. This static function only maps the link library into memory and runs the code that the function contains.
The link library determines what are all the dynamic libraries which the program requires along with the names of the variables and functions needed from those libraries by reading the information contained in sections of
the library.
After which it maps the libraries into the middle of virtual memory and resolves the references to the symbols contained in those libraries. We don’t know where in the memory these shared libraries are actually mapped:
They are compiled into position-independent code (PIC), that can run at any address in memory.
Advantage:
Memory requirements of the program are reduced. A DLL is loaded into memory only once, whereas more than one application may use a single DLL at the moment, thus saving memory space. Application support and
maintenance costs are also lowered.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/static-and-dynamic-linking-in-operating-systems/
✍
Write a Testimonial

Introduction of Shortest Remaining Time First (SRTF) algorithm
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Introduction of Shortest Remaining Time First (SRTF) algorithm - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Shortest Remaining Time First (SRTF) is the preemptive version of Shortest Job Next (SJN) algorithm, where the processor is allocated to the job closest to completion.
This algorithm requires advanced concept and knowledge of CPU time required to process the job in an interactive system, and hence can’t be implemented there. But, in a batch system where it is desirable to give
preference to short jobs, SRT algorithm is used.
However, SRT involves more overheads than SJN, as the OS is required to frequently monitor the CPU time of the jobs in the READY queue and perform context switching.

As illustrated above, for the same set of jobs, SRT algorithm is faster in execution than SJN algorithm. But, here the overhead charges, i.e., time required for context switching has been ignored. When a job is preempted,
all of it’s processing information must be saved in it’s PCB for later when it is to be continued, and the contents of the PCB of the other job to which the OS is switching are loaded into the registers in the memory. This is
known as Context Switching.
Advantages:
SRTF algorithm makes the processing of the jobs faster than SJN algorithm, given it’s overhead charges are not counted.
Disadvantages:
The context switch is done a lot more times in SRTF than in SJN, and consumes CPU’s valuable time for processing. This adds up to it’s processing time and diminishes it’s advantage of fast processing.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/introduction-of-shortest-remaining-time-first-srtf-algorithm/
✍
Write a Testimonial

Process Scheduler : PCBs and Queueing
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Process Scheduler : PCBs and Queueing - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
When the job scheduler accepts a job, it creates the job’s PCB (Process Control Blocks) and updates it throughout the execution.
These PCBs, and not the jobs, are linked to form the queues and are used to track the respective jobs.
Note:
The PCB store all of the data about the job being processed, like it’s progress in the system. This data is needed by the operating system to manage the processing of the job.

As shown above, each queue can be seen as the linked list of PCBs:
1. READY queue contains PCBs for ready jobs.
2. HOLD queue contains PCBs for the jobs entering the system.
3. WAITING queue contains PCBs for the jobs which need some resource allocation or input from the user. Depending upon their reason to be in the WAITING queue, they are linked into several queues. E.g., The
PCBs waiting for some input are in a separate queue, while asking for determining the file location of a specific file is in a separate queue.
WAITING queues are managed in a specific order according to some policies.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/process-scheduler-pcbs-and-queueing/
✍
Write a Testimonial

Process Scheduler : Job and Process Status
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Process Scheduler : Job and Process Status - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
When the jobs moves through the system and makes progress, it changes it’s states from HOLD to FINISH. When the job is being processed by the job manager and the process manager, it is always in one of these 5
states:
1. HOLD:
When a user submits a job and it accepts the job, the job is put on HOLD and placed in a queue.
2. READY:
A job is in READY state when it’s ready to run and waiting for the CPU.
3. RUNNING:
When a job is in RUNNING state, it is being executed.
4. WAITING:
When a job is in WAITING state, it means that the job can’t continue until a specified I/O operation is done or a resource is allocated.
5. FINISHED:
When a job is in FINISHED state, it means that the job is done and the output will be returned to the user.

The transition of a job from one to another or from one state to another is done according to some specific algorithms by job scheduler or process scheduler:

1.
2.
3.
4.
5.
6.

The transition from HOLD to READY is done by the job scheduler according to availability of main memory space and some specific policies.
The transition from READY to RUNNING is done by the process scheduler (to decide which job will be done first) according to some algorithms (e.g. FCFS).
The transition from RUNNING back to READY is done by process scheduler according to some criterion (e.g. Priority Interrupt).
The transition from RUNNING to WAITING is done by process scheduler when some I/O request is encountered in the job itself or some resource allocation is required.
The transition from WAITING to READY is done by the process scheduler when the requirements needed by the jobs are satisfied (I/O request satisfied).
The transition from RUNNING to FINISHED is done by the process scheduler in 2 condition:
(i) When the job is successfully done.
(ii) When an error occurs and the job is terminated prematurely.
7. When a job is FINISHED, another job from the queue enters the state of RUNNING from READY.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/process-scheduler-job-and-process-status/
✍
Write a Testimonial

Sorting larger file with smaller RAM
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Sorting larger file with smaller RAM - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Suppose we have to sort a 1GB file of random integers and the available ram size is 200 Mb, how will it be done?
The easiest way to do this is to use external sorting.
We divide our source file into temporary files of size equal to the size of the RAM and first sort these files.
Assume 1GB = 1024MB, so we follow following steps.
1. Divide the source file into 5 small temporary files each of size 200MB (i.e., equal to the size of ram).
2. Sort these temporary files one bye one using the ram individually (Any sorting algorithm : quick sort, merge sort).
Now we have small sorted temporary files as shown in the image below.

Figure – Dividing source file in smaller sorted temp files
Now we have sorted temporary files.
1.
2.
3.
4.
5.
6.

Pointers are initialized in each file
A new file of size 1GB (size of source file) is created.
First element is compared from each file with the pointer.
Smallest element is copied into the new 1GB file and pointer gets incremented in the file which pointed to this smallest element.
Same process is followed till all pointers have traversed their respective files.
When all the pointers have traversed, we have a new file which has 1GB of sorted integers.

This is how any larger file can be sorted when there is a limitation on the size of primary memory (RAM).
The basic idea is to divide the larger file into smaller temporary files, sort the temporary files and then creating a new file using these temporary files. This question was asked in Infosys interview for power programmer
profile.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/sorting-larger-file-with-smaller-ram/

✍
Write a Testimonial

Best-Fit Allocation in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Best-Fit Allocation in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
For both fixed and dynamic memory allocation schemes, the operating system must keep list of each memory location noting which are free and which are busy. Then as new jobs come into the system, the free partitions
must be allocated.
These partitions may be allocated by 4 ways:
1.
2.
3.
4.

First-Fit Memory Allocation​
Best-Fit Memory Allocation​
Worst-Fit Memory Allocation​
Next-Fit Memory Allocation

These are Contiguous memory allocation techniques.

Best-Fit Memory Allocation:
This method keeps the free/busy list in order by size – smallest to largest. In this method, the operating system first searches the whole of the memory according to the size of the given job and allocates it to the closestfitting free partition in the memory, making it able to use memory efficiently. Here the jobs are in the order from smalest job to largest job.

As illustrated in above figure, the operating system first search throughout the memory and allocates the job to the minimum possible memory partition, making the memory allocation efficient.
Advantages of Best-Fit Allocation :
Memory Efficient. The operating system allocates the job minimum possible space in the memory, making memory management very efficient. To save memory from getting wasted, it is the best method.
Disadvantages of Best-Fit Allocation :
It is a Slow Process. Checking the whole memory for each job makes the working of the operating system very slow. It takes a lot of time to complete the work.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/best-fit-allocation-in-operating-system/
✍
Write a Testimonial

First-Fit Allocation in Operating Systems
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ First-Fit Allocation in Operating Systems - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu

For both fixed and dynamic memory allocation schemes, the operating system must keep list of each memory location noting which are free and which are busy. Then as new jobs come into the system, the free partitions
must be allocated.
These partitions may be allocated by 4 ways:
1.
2.
3.
4.

First-Fit Memory Allocation​
Best-Fit Memory Allocation​
Worst-Fit Memory Allocation​
Next-Fit Memory Allocation

These are Contiguous memory allocation techniques.

First-Fit Memory Allocation:
This method keeps the free/busy list of jobs organized by memory location, low-ordered to high-ordered memory. In this method, first job claims the first available memory with space more than or equal to it’s size. The
operating system doesn’t search for appropriate partition but just allocate the job to the nearest memory partition available with sufficient size.

As illustrated above, the system assigns J1 the nearest partition in the memory. As a result, there is no partition with sufficient space is available for J3 and it is placed in the waiting list.
Advantages of First-Fit Memory Allocation:
It is fast in processing. As the processor allocates the nearest available memory partition to the job, it is very fast in execution.
Disadvantages of Fist-Fit Memory Allocation :
It wastes a lot of memory. The processor ignores if the size of partition allocated to the job is very large as compared to the size of job or not. It just allocates the memory. As a result, a lot of memory is wasted and many
jobs may not get space in the memory, and would have to wait for another job to complete.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/first-fit-allocation-in-operating-systems/
✍
Write a Testimonial

C-LOOK Disk Scheduling Algorithm
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ C-LOOK Disk Scheduling Algorithm - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite: Disk Scheduling Algorithms
Given an array of disk track numbers and initial head position, our task is to find the total number of seek operations done to access all the requested tracks if C-LOOK disk scheduling algorithm is used. Also, write a
program to find the seek sequence using C-LOOK disk scheduling algorithm.
C-LOOK (Circular LOOK) Disk Scheduling Algorithm:
C-LOOK is an enhanced version of both SCAN as well as LOOK disk scheduling algorithms. This algorithm also uses the idea of wrapping the tracks as a circular cylinder as C-SCAN algorithm but the seek time is
better than C-SCAN algorithm. We know that C-SCAN is used to avoid starvation and services all the requests more uniformly, the same goes for C-LOOK.

In this algorithm, the head services requests only in one direction(either left or right) until all the requests in this direction are not serviced and then jumps back to the farthest request on the other direction and service the
remaining requests which gives a better uniform servicing as well as avoids wasting seek time for going till the end of the disk.
Algorithm-

1.
2.
3.
4.
5.
6.
7.
8.
9.

Let Request array represents an array storing indexes of the tracks that have been requested in ascending order of their time of arrival and head is the position of the disk head.
The initial direction in which the head is moving is given and it services in the same direction.
The head services all the requests one by one in the direction it is moving.
The head continues to move in the same direction until all the requests in this direction have been serviced.
While moving in this direction, calculate the absolute distance of the tracks from the head.
Increment the total seek count with this distance.
Currently serviced track position now becomes the new head position.
Go to step 5 until we reach the last request in this direction.
If we reach the last request in the current direction then reverse the direction and move the head in this direction until we reach the last request that is needed to be serviced in this direction without servicing the
intermediate requests.
10. Reverse the direction and go to step 3 until all the requests have not been serviced.
Examples:
Input:
Request sequence = {176, 79, 34, 60, 92, 11, 41, 114}
Initial head position = 50
Direction = right (Moving from left to right)
Output:
Initial position of head: 50
Total number of seek operations = 156
Seek Sequence is
60
79
92
114
176
11
34
41
The following chart shows the sequence in which requested tracks are serviced using C-LOOK.

Therefore, the total seek count = (60 – 50) + (79 – 60) + (92 – 79) + (114 – 92) + (176 – 114) + (176 – 11) + (34 – 11) + (41 – 34) = 321
Implementation:
The implementation of the C-LOOK algorithm is given below. Note that the distance variable is used to store the absolute distance between the head and the current track position, disk_size is the size of the disk. Vectors
left and right store all the request tracks on the left-hand side and the right-hand side of the initial head position respectively.
filter_none
edit
close
play_arrow
link
brightness_4
code
// C++ implementation of the approach
#include <bits/stdc++.h>
using namespace std;
int size = 8;
int disk_size = 200;
// Function to perform C-LOOK on the request
// array starting from the given head
void CLOOK(int arr[], int head)
{
int seek_count = 0;
int distance, cur_track;
vector<int> left, right;
vector<int> seek_sequence;
// Tracks on the left of the
// head will be serviced when
// once the head comes back
// to the beggining (left end)
for (int i = 0; i < size; i++) {
if (arr[i] < head)
left.push_back(arr[i]);
if (arr[i] > head)
right.push_back(arr[i]);
}
// Sorting left and right vectors
std::sort(left.begin(), left.end());
std::sort(right.begin(), right.end());
// First service the requests
// on the right side of the
// head
for (int i = 0; i < right.size(); i++) {
cur_track = right[i];
// Appending current track to seek sequence
seek_sequence.push_back(cur_track);
// Calculate absolute distance
distance = abs(cur_track - head);
// Increase the total count
seek_count += distance;
// Accessed track is now new head
head = cur_track;
}
// Once reached the right end
// jump to the last track that
// is needed to be serviced in
// left direction
seek_count += abs(head - left[0]);
head = left[0];
// Now service the requests again

// which are left
for (int i = 0; i < left.size(); i++) {
cur_track = left[i];
// Appending current track to seek sequence
seek_sequence.push_back(cur_track);
// Calculate absolute distance
distance = abs(cur_track - head);
// Increase the total count
seek_count += distance;
// Accessed track is now the new head
head = cur_track;
}
cout << "Total number of seek operations = "
<< seek_count << endl;
cout << "Seek Sequence is" << endl;
for (int i = 0; i < seek_sequence.size(); i++) {
cout << seek_sequence[i] << endl;
}
}
// Driver code
int main()
{
// Request array
int arr[size] = { 176, 79, 34, 60,
92, 11, 41, 114 };
int head = 50;
cout << "Initial position of head: " << head << endl;
CLOOK(arr, head);
return 0;
}

chevron_right
filter_none
Output:
​
Initial position of head: 50​
Total number of seek operations = 321​
Seek Sequence is​
60​
79​
92​
114​
176​
11​
34​
41​

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/c-look-disk-scheduling-algorithm/
✍
Write a Testimonial

Difference between Concurrency and Parallelism
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Concurrency and Parallelism - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Concurrency:
Concurrency relates to an application that is making progress more than one task at the same time. Concurrency is a approach that is used for decreasing the response time of the system by using the single processing unit.
Concurrency is that the illusion of parallelism, however in actual the chunks of a task aren’t parallelly processed, but inside the application, there are more than one task is being processed at a time. It doesn’t fully end one
task before it begins ensuing.
Concurrency is achieved through the interleaving operation of processes on the central processing unit(CPU) or in other words by the context switching. that’s rationale it’s like parallel processing. It increases the amount
of work finished at a time.

In the above figure, we can see that there is multiple tasks making progress at the same time. This figure shows the concurrency because concurrency is the technique that deals with the lot of things at a time.
Parallelism:
Parallelism related to an application in which the tasks are divided into smaller sub-tasks that are processed simultaneously or parallel. It is used for increasing the throughput and computational speed of the system by
using the multiple processors. It is the technique that do lot of things simultaneously.
Parallelism leads to overlapping of central processing unit and input-output tasks in one process with the central processing unit and input-output tasks of another process. Whereas in concurrency the speed is increased by
overlapping the input-output activities of one process with CPU process of another process.

In the above figure, we can see that the tasks are divided into smaller sub-tasks that are processing simultaneously or parallel. This figure shows the parallelism because parallelism is the technique that do lot of
things simultaneously.
Difference between Concurrency and Parallelism:S.NO
1.
2.

Concurrency
Concurrency is the task of running and managing the multiple computations at the same time.
Concurrency is achieved through the interleaving operation of processes on the central processing unit(CPU) or in other words by
the context switching.

3.

Concurrency can be done by using a single processing unit.

4.
5.
6.
7.

Concurrency increases the amount of work finished at a time.
Concurrency deals lot of things simultaneously.
Concurrency is the non-deterministic control flow approach.
In concurrency debugging is very hard.

Parallelism
While parallelism is the task of running multiple computations simultaneously.
While it is achieved by through multiple central processing units(CPUs).
While this can’t be done by using a single processing unit. it needs multiple
processing units.
While it improves the throughput and computational speed of the system.
While it do lot of things simultaneously.
While it is deterministic control flow approach.
While in this debugging is also hard but simple than concurrency.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-concurrency-and-parallelism/
✍
Write a Testimonial

Microsoft Windows (10) Vs macOS (Mojave)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Microsoft Windows (10) Vs macOS (Mojave) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Progressing into the 21st century, we have all witnessed the growth of both Microsoft’s Windows series of operating systems as well as Apple’s much reputed macOS. Both of these have come a long way from where they
started. Microsoft’s Windows, however, to this date holds a major market share in the world as compared to macOS. Nevertheless, the debate on which of these operating system is better has been going on since their
inception and seems to be never ending. This is an attempt to clear the reader’s mind about which one is the better choice between the two, considering that Apple’s offering rarely comes at a cheap price.

Design:
While Windows has become more and more slick over the years, macOS really is the better choice when it comes down to design, user interface, actual usability. Apple is the go to choice when it comes to refinement of
their existing products. While using macOS, you feel the work that Apple has put in into making the operating system better through the years. On the other hand, Windows has come a long, long way but still design
features that macOS has had since a very long time. When it comes down to the basics, macOS beats windows with its dock. The dock can be easily accessed from anywhere, even if one is in a fullscreen app as compared
to windows’ “peek” feature which I think is not of much use.
In the design perspective, MacOS has a slight edge over windows.

Virtual Assistants:
Apple took the world by storm when it released its virtual assistant, Siri in 2010 with Microsoft joining the party late in 2015 with its Cortana. By the time Cortana was released, Siri had got much more refined and was
everyone’s choice when it came to choose between the two. After giving both of these a try, I came to the conclusion that Apple’s Siri is the way to go here. Siri works almost magically with the Apple ecosystem while
Microsoft’s Coratana looked average and has no integration with any mobile operating system, that too in 2019. This will be a dealbreaker for many.
When it came to tricky questions, no virtual assistant was perfect but Siri seemed to handle the questions fairly better while Cortana just led me to webpages.
Need a virtual assistant? Siri is the way to go.

Dark Mode:
With Dark Mode being the talk of the town in 2019, both Apple and Microsoft have equal to offer in this aspect. Apple brought a system wide dark mode to its macOS with the release of macOS Mojave. Microsoft also
brought a dark mode to its Windows in a later Windows 10 update. Both of these dark modes look polished, with them extending to system apps and other UI elements.
Its a tie here.

Gaming:
Windows crushes Apple’s macOS when it comes to gaming. With a few average games available on the AppStore and a few from other developers, macOS really has nothing to offer when it comes to gaming.
Apple is known for not equipping their base line Macs with powerful internals which leads to these base line Macs becoming incapable of gaming. Due to this, most developers tend to not release their games for macOS. I
tried playing a few games on my MacBook Pro (mid 2019) and the experience was horrible with the machine dropping a lot of frames even on medium settings.
While options such as WineBottler and external GPUs remain, I don’t feel that many would be attracted by these. Also, kudos to Microsoft for the seamless integration between XBOX and Windows.

Microsoft’s Windows takes the gaming round.

Finder vs Windows Explorer:
This comes down to personal preference. While the Finder in macOS feels quite faster than Windows Explorer, some people tend to like Windows explorer more. I, personally have faced problems with Windows explorer
like having to click exactly on the name of a file to delete it or else the file wouldn’t get selected, this has been a little annoying but Windows explorer has become really better through the years.

Durability and Updates:
We all know that macOS comes bundled with Apple’s Macs and MacBooks and Windows can be installed mostly on any machine. While Hackintoshes can be built, but considering that, that is illegal, it is not really a
good idea. Apple’s MacBooks tend to last much longer than machines running Microsoft Windows which tend to get slow with time. This has been a personal experience.
Apple has been really good with its software updates, for example, even late 2011 MacBooks received an update to the last macOS, macOS High Sierra.
Value for money:
When it comes to value for money, macOS and Apple products are considered more on the premium side. There is no doubt that Microsoft’s Windows offers a better bang for the buck.
Windows takes this round.
Privacy:
Apple’s strong take on user privacy has led to iOS and macOS becoming one of the safest operating systems out there. Microsoft certainly has improved on the privacy front but it stands nowhere near Apple when it
comes down to user privacy.
Conclusion:
If user privacy, a better overall experience and a good virtual assistant are important for you, then go for macOS. However, if you’re not much about user privacy, do gaming and need a better bang for your buck, there is
no better option than Microsoft’s Windows series of operating systems.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/microsoft-windows-10-vs-macos-mojave/
✍
Write a Testimonial

Most asked Computer Science Subjects Interview Questions in Amazon, Microsoft, Flipkart
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Most asked Computer Science Subjects Interview Questions in Amazon, Microsoft, Flipkart - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
This article contains a list of most asked questions from Operating Systems, Computer Networks and DBMS in the interviews of the top product based companies like Amazon, Microsoft, Flipkart, Paytm etc.

Operating System

Process Introduction
1. What is a microprocessor?
2. Explain the internal architecture of a RAM.

3. How compiler compiles the interlinked libraries?
4. Explain the implementation of virtual methods, dynamic binding, vtables etc.
Multithreading
1. What is Multithreading?
2. What is the difference between a thread and a process?
Process Scheduling
1. FCFS Scheduling.
2. Shortest Job First Scheduling.
3. SRTF Scheduling.
4. LRTF Scheduling.
5. Priority Scheduling.
6. Round Robin scheduling
Process Synchronization & Deadlock
1. What is a Semaphore and a Mutex?
2. Explain the Producer-Consumer problem.
3. What is Deadlock?
4. What are the four necessary conditions for Deadlock?
5. What is Critical Section?
6. Explain the Banker’s Algorithm.
7. What are Spinlocks?
Memory Management
1. What is Cache?
2. Where does cache lies in an Operating System?
3. Difference between Cache and HashMap.
4. Exlpain Demand paging and thrashing.
5. What is Segmentation?
6. In which memory, the laptop password is being saved?
7. How will you analyze Out of memory exceptions in your application?
8. Explain internal fragmentation and external fragmentation.
9. Difference between the associative mapping and direct mapping in a cache.
10. If RAM size is 4GB, if 4 processes of size 2GB are launched! What happens?
(Ans: This can be done using Virtual Memory)
11. If process size is not limited by the size of main memory then what is its limitation?
(Ans: This can be done using Logical Address Space)
12. Explain how memory location is accessed
13. What is Paging and Why do we need Paging?
14. What is a Page Table?
15. What is TLB?
DBMS
Properties of RDBMS?
ACID properties
Keys in DBMS.
Difference between Vertical and Horizontal Scaling.
Sharding
DML, DCL, DDL, TCL and their commands.
Indexing in DBMS.
What is normalization and de-normalization and why do we need it?
Normal Forms
Conflict Serializability
Can Primary key contain two entities?
(Ans: No, there is one and only one primary key in any relationship. Refer this)
Concurrency Control
SQL queries (related to nested query).
Insertion in B trees
Types of JOIN in DBMS.
Difference between INNER and OUTER JOIN.
Write a SQL query to retrieve furniture from database whose dimensions(Width, Height, Length) match with the given dimension.
Ans.
​
SELECT *​
FROM Furnitures​
WHERE Furnitures.Length = GivenLength​
AND Furnitures.Breadth = GivenBreadth​
AND Furnitures.Height = GivenHeight​

Print the second largest number in the table.
Explain 3 tier architecture and 2 tier architectures.
Write a SQL query to find the 4th maximum element from a table
Computer Networks
What is TCP?
Name layers of the OSI Model with protocols belonging to the layers
What is the significance of Data Link Layer
What is Access Points APs model?
What does the network layer do
In which layer are the Routers?
What are the different types of delays?
Explain Firewalls?
What are the different types of firewall?
What does transport layer do
IPv4 vs IPv6
What is the difference b/w private IP and Public IP?
Explain in detail 3 way Handshaking
What is Cryptography and what are the Encryption Methods ?
What are the Application layer protocols?
Explain DNS
On entering a URL in a browser, explain the detailed procedure in which the request is handled by the browser and the result is obtained for the given search query.
How will you create persistent connections between the server and the client?
Explain server-side loadbalancer
What is FTP? How is FTP different from Secure FTP?
What is SMTP
Explain the Working of HTTP and HTTPs.
Where are ports?
What Port numbers of different protocols
How to prevent SYN DDoS attack?
You may also check our, paid course on CS subjects priced at only INR 4500. Hurry up, and avail an additional 50% OFF for a limited period using Coupon Code: FIRST100.

This course will help you prepare topics like Operating System, DMBS, Computer Networks and SQL for interview in top-notch companies like Google, Microsoft, Amazon, etc. The course has pre-recorded premium
lecture videos by Mr. Sandeep Jain and theoretical concepts designed by experts. The course also has objective questions for practice to provide the ultimate learning experience.
This is a self-paced course which implies that you can complete the course at your own pace!

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/most-asked-computer-science-subjects-interview-questions-in-amazon-microsoft-flipkart/
✍
Write a Testimonial

Difference between Deadlock and Starvation in OS
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Deadlock and Starvation in OS - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Deadlock:
Deadlock occurs when each process holds a resource and wait for other resource held by any other process. Necessary conditions for deadlock to occur are Mutual Exclusion, Hold and Wait, No Preemption and Circular
Wait. In this no process holding one resource and waiting for another get executed. For example, in the below diagram, Process 1 is holding Resource 1 and waiting for resource 2 which is acquired by process 2, and
process 2 is waiting for resource 1. Hence both process 1 and process 2 are in deadlock.

Starvation:
Starvation is the problem that occurs when high priority processes keep executing and low priority processes get blocked for indefinite time. In heavily loaded computer system, a steady stream of higher-priority processes
can prevent a low-priority process from ever getting the CPU. In starvation resources are continuously utilized by high priority processes. Problem of starvation can be resolved using Aging. In Aging priority of long
waiting processes is gradually increased.

Difference between Deadlock and Starvation:
S.NO
Deadlock
Starvation
1.
All processes keep waiting for each other to complete and none get executed
High priority processes keep executing and low priority processes are blocked
2.
Resources are blocked by the processes
Resources are continuously utilized by high priority processes
3.
Necessary conditions Mutual Exclusion, Hold and Wait, No preemption, Circular Wait Priorities are assigned to the processes
4.
Also known as Circular wait
Also know as lived lock
5.
It can be prevented by avoiding the necessary conditions for deadlock
It can be prevented by Aging

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-deadlock-and-starvation-in-os/
✍
Write a Testimonial

Difference Between Security and Protection
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference Between Security and Protection - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Security:
The security systems covers the safety of their system resources (saved data, memory, disks, etc) across malignant alteration, illegal access, and disparity or inconsistency. The security gives a mechanism (authentication
and encryption) to analyze the user to permit for using the system.
For example, in a corporation that the info is obtained by completely different workers however, it can’t be obtained by a user that doesn’t exist in this explicit organization or a user operating in different business
enterprises. Security is the vital task for a corporation to provide some safety mechanism in order that no outside user will access the knowledge of the organization.
Protection:
protection deals with the access to the system resources. It determines that what files can be accessed or permeated by a special user. The protection of the system should confirm the approval of the process and users. Due
to this, these licensed users and processes will care for the central processing unit, memory and alternative sources. The protection mechanism ought to provide a path for specifying the controls to be obligatory, beside
how of implementing them.

Example of protection can be given from the security, any organization will have many departments below that several staff operate. the assorted departments will share frequent info with one another however not
sensitive info. So, completely different employees have different access rights the info in step with that they will access the define data.
Let’s see the difference between security and protection:
S.NO
1.
2.
3.
4.
5.

Comparison based on

Security

Protection

Basic
Security grants the system access to the appropriate users only.
While protection deals with the access to the system resources.
Type of threats involved of the system In security, external threats are involved.
While in protection, internal threats are involved.
Queries handle
In security, more convoluted queries are handled.
Whereas in protection, simple queries are handled.
Policy
Security illustrates that which person is granted for using the system.
Whereas protection determines that what files can be accessed or permeated by a special user.
Mechanism
In security, encryption and certification(authentication) mechanisms are used. Whereas in protection, authorization mechanism is implemented.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-security-and-protection/
✍
Write a Testimonial

Difference between Application Software and Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Application Software and Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​

Suggest a Topic

Select a Category​
menu
Application Software:
Application Software is one of the type of software which runs or executes as per user request. High level languages such as java, c, c++ etc are used to develop the application software. Application software is a specific
purpose software which is intended to perform some task grouped together. Without an operating system application software can not be installed. It’s examples are Photoshop, VLC media player, Mozilla Firefox, Opera,
Google chrome etc.
Operating System:
An operating system is a computer program, works as interface between user and hardware and provides common services for computer programs. The entire process or functionality of computer system depends on the
operating system. It is developed by using c++, c, assembly languages.
An operating system performs some variety of tasks like, It manages files and directory creation and deletion, process creation, deletion, synchronization, memory allocation and deallocation. An operating system also
prevents the computer system from unauthorized access and secures the resources, information and data. It’s examples are Microsoft Windows, Linux, Unix, DOS. Overall, we can say that without an operating system a
computer system is nothing.

In above diagram, we can clearly see that the system and application program or software depend upon the operating system which is act as the interface between user and computer hardware.
Difference between Application software and Operating system:
S.NO

Application software

2.
3.
4.

A computer program which is intended to perform some task classified
along.
Application software is downloaded form internet.
It is developed by using virtual basic, c++, c, java.
It is usually in Megabytes(MB).

5.

It is built to perform some specific tasks.

6.
7.
8.

It always depends upon operating system.
It runs when the user desires to run the application.
It’s examples are Photoshop, VLC player etc.

1.

Operating System
A system computer program that manages hardware and software resources and provides common services for computer programs.
Operating system comes installed on the device purchased.
It is developed by using c++, c, assembly languages.
While it is usually is Gigabytes(GB).
It works as interface between user and hardware and perform some variety of tasks like memory management, scheduling, process
management etc.
But it does not depend upon application software.it provides the path to execute or to run the application software.
it boots up when the user wants and run until the user switches off the machine.
It’s examples are Microsoft Windows, Linux, Unix, DOS.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-application-software-and-operating-system/
✍
Write a Testimonial

Difference Between Paging and Segmentation
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference Between Paging and Segmentation - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Paging:
Paging is a method or techniques which is used for non-contiguous memory allocation. It is a fixed size partitioning theme (scheme). In paging, both main memory and secondary memory are divided into equal fixed size
partitions. The partitions of secondary memory area unit and main memory area unit known as as pages and frames respectively.
Paging is a memory management method accustomed fetch processes from the secondary memory into the main memory in the form of pages. in paging, each process is split into parts wherever size of every part is same
as the page size. The size of the last half could also be but the page size. The pages of process area unit hold on within the frames of main memory relying upon their accessibility.

Segmentation:
Segmentation is another non-contiguous memory allocation scheme like paging. like paging, in segmentation, process isn’t divided indiscriminately into mounted(fixed) size pages. It is variable size partitioning theme.
like paging, in segmentation, secondary and main memory are not divided into partitions of equal size. The partitions of secondary memory area unit known as as segments. The details concerning every segment are hold
in a table known as as segmentation table. Segment table contains two main data concerning segment, one is Base, which is the bottom address of the segment and another is Limit, which is the length of the segment.
In segmentation, CPU generates logical address that contains Segment number and segment offset. If the segment offset is a smaller amount than the limit then the address called valid address otherwise it throws
miscalculation because the address is invalid.

The above figure shows the translation of logical address to physical address.
Difference between Paging and Segmentation:
S.NO
1.
2.
3.
4.
5.
6.
7.
8.

Paging

Segmentation

In paging, program is divided into fixed or mounted size pages.
In segmentation, program is divided into variable size sections.
For paging operating system is accountable.
For segmentation compiler is accountable.
Page size is determined by hardware.
Here, the section size is given by the user.
It is faster in the comparison of segmentation.
Segmentation is slow.
Paging could result in internal fragmentation.
Segmentation could result in external fragmentation.
In paging, logical address is split into page number and page offset.
Here, logical address is split into section number and section offset.
Paging comprises a page table which encloses the base address of every page. While segmentation also comprises the segment table which encloses segment number and segment offset.
Page table is employed to keep up the page data.
Section Table maintains the section data.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-paging-and-segmentation/
✍
Write a Testimonial

Difference between Internal and External fragmentation
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Internal and External fragmentation - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
There are two types of fragmentation in OS which are given as: Internal fragmentation, and External fragmentation.
Internal Fragmentation:
Internal fragmentation happens when the memory is split into mounted sized blocks. Whenever a method request for the memory, the mounted sized block is allotted to the method. just in case the memory allotted to the
method is somewhat larger than the memory requested, then the distinction between allotted and requested memory is that the Internal fragmentation.

The above diagram clearly shows the internal fragmentation because the difference between memory allocated and required space or memory is called Internal fragmentation.
External Fragmentation:
External fragmentation happens when there’s a sufficient quantity of area within the memory to satisfy the memory request of a method. however the process’s memory request cannot be fulfilled because the memory
offered is during a non-contiguous manner. Either you apply first-fit or best-fit memory allocation strategy it’ll cause external fragmentation.

In above diagram, we can see that, there is enough space (55 KB) to run a process-07 (required 50 KB) but the memory (fragment) is not contiguous. Here, we use compaction, paging or segmentation to use the free space
to run a process.
Difference between Internal fragmentation and External fragmentation:S.NO
1.
2.
3.
4.
5.

Internal fragmentation
In internal fragmentation fixed-sized memory, blocks square measure appointed to
process.
Internal fragmentation happens when the method or process is larger than the memory.
The solution of internal fragmentation is best-fit block.
Internal fragmentation occurs when memory is divided into fixed sized partitions.
The difference between memory allocated and required space or memory is called Internal
fragmentation.

External fragmentation
In external fragmentation, variable-sized memory blocks square measure appointed to method.
External fragmentation happens when the method or process is removed.
Solution of external fragmentation is compaction, paging and segmentation.
External fragmentation occurs when memory is divided into variable size partitions based on the size of processes.
The unused spaces formed between non-contiguous memory fragments are too small to serve a new process, is called
External fragmentation .

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : HarshitAgarwall

Source
https://www.geeksforgeeks.org/difference-between-internal-and-external-fragmentation/
✍
Write a Testimonial

User View Vs Hardware View Vs System View of Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ User View Vs Hardware View Vs System View of Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu

User View of Operating System:
The Operating System is an interface, hides the details which must be performed and presents a virtual machine to the user that makes easier to use. Operating System provides the following services to the user.
Execution of a program
Access to I/O devices
Controlled access to files
Error detection (Hardware failures, and software errors)
Hardware View of Operating System:
The Operating System manages the resources efficiently in order to offer the services to the user programs. Operating System acts as a resource managers:
Allocation of resources
Controlling the execcution of a program
Control the operationss of I/O devices
Protecftion of resources
Monitors the data
System View of Operating System:
Operating System is a program that functions in the same way as other programs . It is a set of instructions that are executed by the processor. Operating System acts as a program to perform the following.

Hardware upgrades
New services
Fixes the issues of resources
Controls the user and hardware operations

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/user-view-vs-hardware-view-vs-system-view-of-operating-system/
✍
Write a Testimonial

Stack Implementation in Operating System uses by Processor
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Stack Implementation in Operating System uses by Processor - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
A stack is an associate ordered a set of components, only one of that (last added) are often accessed at a time. The point of access is named the highest of the stack. The number of components within the stack, or length of
the stack, is variable. Items could solely be side to or deleted from the highest of the stack. For this reason, a stack is additionally referred to as a pushdown list or a last-in-first-out (LIFO) list.
The implementation of a stack needs that there be some set of locations accustomed store the stack components. A typical approach is illustrated in the below diagram.

The location of the continuous block is reserved in main memory (or virtual memory) for the stack. Most of the time, the block is part full of stack components and also the remainder is accessible for stack growth. Three
addresses area unit required for correct operation, and these area units typically keep in processor registers:
Stack Pointer:
It contains the address of this prime of the stack. If the associated item is appended to (PUSH) or deleted from (POP) the stack, the pointer is decremented or incremented to contain the address of the new prime of
the stack.
Stack Base:
It contains the address of a very cheap location within the reserved block. this can be the primary location to be used once the associated item is side to associate empty stack. If an effort is formed to POP a
component once the stack is empty, an error is reported.
Stack Limit:
It contains the address of the opposite finish, or top, of the reserved block. If an effort is formed to PUSH a component once the stack is full, an error is reported.
Traditionally, and on most processors, these days, the base of the stack is at the high address finish of the reserved stack block, and also the limit is at the low-address finish. so, the stack will grow from higher addresses to
lower addresses.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/stack-implementation-in-operating-system-uses-by-processor/
✍
Write a Testimonial

Difference between Operating System and Kernel
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Operating System and Kernel - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Operating System:
It is a system program that provides interface between user and computer. When computer boots up Operating System is the first program that loads.
Kernel:
A kernel is the core component of an operating system. It is also a system program. It is the part of Operating System which coverts user command into machine language.
Difference between Operating System and Kernel:

Operating System
Kernel
Operating System is a system software.
Kernel is system software which is part of operating system.
Operating System provides interface b/w user and hardware.
kernel provides interface b/w application and hardware.
It also provides protection and security.
It’s main purpose is memory management, disk management, process management and task management.
All system needs operating system to run.
All operating system needs kernel to run.
Type of operating system includes single and multiuser OS, multiprocessor OS, realtime OS, Distributed OS.Type of kernel includes Monolithic and Micro kernel.
It is the first program to load when computer boots up.
It is the first program to load when operating system loads.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-operating-system-and-kernel/
✍
Write a Testimonial

Inode in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Inode in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
In Unix based operating system each file is indexed by an Inode. Inode are special disk blocks they are created when the file system is created. The number of Inode limits the total number of files/directories that can be
stored in the file system.
The Inode contains the following information:
Administrative information (permissions, timestamps, etc).
A number of direct blocks (typically 12) that contains to the first 12 blocks of the files.
A single indirect pointer that points to a disk block which in turn is used as an index block, if the file is too big to be indexed entirely by the direct blocks.
A double indirect pointer that points to a disk block which is a collection of pointers to disk blocks which are index blocks, used if the file is too big to beindexed by the direct and single indirect blocks.
A triple indirect pointer that points to an index block of index blocks of index blocks.
Inode Total Size:
Number of disk block address possible to store in 1 disk block = (Disk Block Size / Disk Block Address).
Small files need only the direct blocks, so there is little waste in space or extra disk reads in those cases. Medium sized files may use indirect blocks. Only large files make use of the double or triple indirect blocks,
and that is reasonable since those files are large anyway.The disk is now broken into two different types of blocks: Inode and Data Blocks.
There must be some way to determine where the Inodes are, and to keep track of free Inodes and disk blocks. This is done by a Superblock. Superblock is located at a fixed position in the file system. The
Superblock is usually replicated on the disk to avoid catastrophic failure in case of corruption of the main Superblock.
Index allocation schemes suffer from some of the same performance problems. As does linked allocation. For example, the index blocks an be cached in memory, but the data blocks may be spread all over a
partition.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/inode-in-operating-system/
✍
Write a Testimonial

Booting and Dual Booting of Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Booting and Dual Booting of Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
After an operating system is generated, it must be available for the use by the hardware. But how the hardware know where the kernel is, or how to load that kernel? The procedure of starting a computer by loading the
kernel is known as Booting the system. Hence it needs a special program, stored in ROM to do this job known as the Bootstrap loader. Example: BIOS (boot input output system). A modern PC BIOS (Basic Input/Output
System) supports booting from various devices.Typically, the BIOS will allow the user to configure a boot order. If the boot order is set to:
CD Drive
Hard Disk Drive
Network
Then the BIOS will try to boot from the CD drive first, and if that fails then it will try to boot from the hard disk drive, and if that fails then it will try to boot from the network, and if that fails then it won’t boot at all.
Booting is a startup sequence that starts the operating system of a computer when it is turned on. A boot sequence is the initial set of operations that the computer performs when it is switched on. Every computer has a
boot sequence. Bootstrap loader locates the kernel, loads it into main memory and starts its execution.In some systems, a simple bootstrap loader fetches a more complex boot program from disk, which in turn loads the
kernel.

Dual Booting:
When two operating system are installed on the computer system then it is called dual booting. In fact multiple operating systems can be installed on such a system. But how system knows which operating system is to
boot? A boot loader that understand multiple file systems and multiple operating system can occupy the boot space.Once loaded, it can boot one of the operating systems available on the disk.The disk can have multiple
partitions, each containing a different type of operating system. When a computer system turn on, a boot manager program displays a menu, allowing user to choose the operating system to use.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/booting-and-dual-booting-of-operating-system/
✍
Write a Testimonial

Implementation of Non-Preemptive Shortest Job First using Priority Queue
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Implementation of Non-Preemptive Shortest Job First using Priority Queue - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Read here for Shortest Job First Scheduling algorithm for same arrival times.
Shortest job first (SJF) or shortest job next, is a scheduling policy that selects the waiting process with the smallest execution time to execute next.
In this article, we will implement the Shortest Job First Scheduling algorithm (SJF) using a priority queue, so that we can handle processes at different arrival time.

Examples:

​
Input: The processes are​
Process id
Arrival time
Burst time
​
p1
4
3
​
p2
0
8
​
p3
5
4
​
p4
9
2
​
​
Output: Process scheduling according to SJF is​
Process id
Arrival time
Burst time
​
p2
0
8
​
p1
4
3
​
p4
9
2
​
p3
5
4
​
​
​
Input: The processes are​
Process id
Arrival time
Burst time
​
p1
0
3
​
p2
0
8
​
p3
5
4
​
p4
9
2​
​
Output: Process scheduling according to SJF is​
Process id
Arrival time
Burst time
​
p1
0
3
​
p2
0
8
​
p4
9
2
​
p3
5
4
​

In this program, the task is to schedule the processes according to SJF scheduling algorithm, which states that the process with minimum burst time will be given priority, which can simply be implemented by sorting the
burst time of the processes in ascending order. The problem arises when we have to handle the processes at different arrival time, then we simply can’t sort the processes according to burst time as we need to consider the
arrival time of the process so that the processor doesn’t stay idle.
Example:
If a process with more burst time arrives before a process with less burst time, then we have to allow the processor time to the process that arrived first so that the processor doesn’t stay idle.
Approach:
To handle processes with a different arrival time in case of SJF scheduling:
First, sort the processes according to the arrival time.
Maintain a wait queue, which keeps the process with minimum burst time at the top.
Maintain the current run time, that is the sum of burst times of the executed processes.
A process enters the wait queue according to it’s arrival time if a new process has arrival time less than
equal to the current running time, it is pushed in the wait queue.
A process is popped from the wait queue when it is to be executed. It’s burst time is added to the current run time, it’s arrival time is updated to -1 so that it doesn’t enter the wait queue again.
Below is the implementation of the above approach:
filter_none
edit
close
play_arrow
link
brightness_4
code
// C++ implementation of SJF
#include <bits/stdc++.h>
using namespace std;
// number of process
#define SIZE 4
// Structure to store the
// process information
typedef struct proinfo {
string pname; // process name
int atime; // arrival time
int btime; // burst time
} proinfo;
// This
// wait
// time
typedef
int

structure maintains the
queue, using burst
to compare.
struct cmpBtime {
operator()(const proinfo& a,
const proinfo& b)

{
return a.btime > b.btime;
}
} cmpBtime;
// This function schedules the
// process according to the SJF
// scheduling algorithm.
void sjfNonpremetive(proinfo* arr)
{
// Used to sort the processes
// according to arrival time
int index = 0;
for (int i = 0; i < SIZE - 1; i++) {
index = i;
for (int j = i + 1; j < SIZE; j++) {
if (arr[j].atime
< arr[index].atime) {
index = j;
}
}
swap(arr[i], arr[index]);
}
// ctime stores the current run time
int ctime = arr[0].atime;
// priority queue, wait, is used
// to store all the processes that
// arrive <= ctime (current run time)
// this is a minimum priority queue
// that arranges values according to
// the burst time of the processes.
priority_queue<proinfo, vector<proinfo>,
cmpBtime>
wait;
int temp = arr[0].atime;
// The first process is
// pushed in the wait queue.
wait.push(arr[0]);
arr[0].atime = -1;
cout <<
<<
cout <<
<<
cout <<
<<

"Process id"
"\t";
"Arrival time"
"\t";
"Burst time"
"\t";

cout << endl;
while (!wait.empty()) {
cout
cout
cout
cout
cout

<<
<<
<<
<<
<<

"\t";
wait.top().pname << "\t\t";
wait.top().atime << "\t\t";
wait.top().btime << "\t\t";
endl;

// ctime is increased with
// the burst time of the
// currently executed process.
ctime += wait.top().btime;
// The executed process is
// removed from the wait queue.
wait.pop();
for (int i = 0; i < SIZE; i++) {
if (arr[i].atime <= ctime
&& arr[i].atime != -1) {
wait.push(arr[i]);
// When the process once
// enters the wait queue
// its arrival time is
// assigned to -1 so that
// it doesn't enter again
// int the wait queue.
arr[i].atime = -1;
}
}
}
}
// Driver Code
int main()
{
// an array of process info structures.
proinfo arr[SIZE];
arr[0]
arr[1]
arr[2]
arr[3]

=
=
=
=

{
{
{
{

"p1",
"p2",
"p3",
"p4",

4,
0,
5,
9,

3
8
4
2

};
};
};
};

cout << "Process scheduling ";
cout << "according to SJF is: \n"
<< endl;
sjfNonpremetive(arr);
}

chevron_right
filter_none
Output:
​
Process scheduling according to SJF is: ​
​
Process id
Arrival time
Burst time
p2
0
8
​
p1
4
3
​
p4
9
2
​
p3
5
4​

​

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/implementation-of-non-preemptive-shortest-job-first-using-priority-queue/
✍
Write a Testimonial

Difference between Asymmetric and Symmetric Multiprocessing
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Asymmetric and Symmetric Multiprocessing - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Multiprocessing is the use of two or more central processing units within a single computer system. Asymmetric Multiprocessing and Symmetric Multiprocessing are two types of multiprocessing.
Asymmetric Multiprocessing:
Asymmetric Multiprocessing system is a multiprocessor computer system where not all of the multiple interconnected central processing units (CPUs) are treated equally. In asymmetric multiprocessing, only a master
processor runs the tasks of the operating system.
For example, AMP can be used in assigning specific tasks to CPU based on priority and importance of task completion.

Symmetric Multiprocessing:
It involves a multiprocessor computer hardware and software architecture where two or more identical processors are connected to a single, shared main memory, have full access to all input and output devices, In other
words, Symmetric Multiprocessing is a type of multiprocessing where each processor is self-scheduling.
For example, SMP applies multiple processors to that one problem, known as parallel programming.

Difference Between Asymmetric and Symmetric Multiprocessing:
Asymmetric Multiprocessing

Symmetric Multiprocessing

In asymmetric multiprocessing, the processors are not treated equally.
Tasks of the operating system are done by master processor.
No Communication between Processors as they are controlled by the master processor.
In asymmetric multiprocessing, process are master-slave.
Asymmetric multiprocessing systems are cheaper.
Asymmetric multiprocessing systems are easier to design

In symmetric multiprocessing, all the processors are treated equally.
Tasks of the operating system are done individual processor
All processors communicate with another processor by a shared memory.
In symmetric multiprocessing, the process is taken from the ready queue.
Symmetric multiprocessing systems are costlier.
Symmetric multiprocessing systems are complex to design

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-asymmetric-and-symmetric-multiprocessing/
✍
Write a Testimonial

LOOK Disk Scheduling Algorithm
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ LOOK Disk Scheduling Algorithm - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite: Disk Scheduling Algorithms
Given an array of disk track numbers and initial head position, our task is to find the total number of seek operations done to access all the requested tracks if LOOK disk scheduling algorithm is used. Also, write a
program to find the seek sequence using LOOK disk scheduling algorithm.
LOOK Disk Scheduling Algorithm:
LOOK is the advanced version of SCAN (elevator) disk scheduling algorithm which gives slightly better seek time than any other algorithm in the hierarchy (FCFS->SRTF->SCAN->C-SCAN->LOOK). The LOOK
algorithm services request similarly as SCAN algorithm meanwhile it also “looks” ahead as if there are more tracks that are needed to be serviced in the same direction. If there are no pending requests in the moving
direction the head reverses the direction and start servicing requests in the opposite direction.

The main reason behind the better performance of LOOK algorithm in comparison to SCAN is because in this algorithm the head is not allowed to move till the end of the disk.
Algorithm:

1.
2.
3.
4.
5.
6.
7.
8.
9.

Let Request array represents an array storing indexes of tracks that have been requested in ascending order of their time of arrival. ‘head’ is the position of disk head.
The intial direction in which head is moving is given and it services in the same direction.
The head services all the requests one by one in the direction head is moving.
The head continues to move in the same direction untill all the request in this direction are not finished.
While moving in this direction calculate the absolute distance of the track from the head.
Increment the total seek count with this distance.
Currently serviced track position now becomes the new head position.
Go to step 5 until we reach at last request in this direction.
If we reach where no requests are needed to be serviced in this direction reverse the direction and go to step 3 until all tracks in request array have not been serviced.

Examples:
Input: ​
Request sequence = {176, 79, 34, 60, 92, 11, 41, 114}​
Initial head position = 50​
Direction = right (We are moving from left to right)​
​
Output:​
Initial position of head: 50​
Total number of seek operations = 291​
Seek Sequence is​
60​
79​
92​
114​
176​
41​
34​
11

The following chart shows the sequence in which requested tracks are serviced using LOOK.

Therefore, the total seek count is calculated as:
= (60-50)+(79-60)+(92-79)​
+(114-92)+(176-114)​
+(176-41)+(41-34)+(34-11)

Implementation:
Implementation of LOOK algorithm is given below.
Note: The distance variable is used to store the absolute distance between the head and current track position. disk_size is the size of the disk. Vectors left and right stores all the request tracks on the left-hand side and the
right-hand side of the initial head position respectively.
filter_none
edit
close
play_arrow
link
brightness_4
code
#include <bits/stdc++.h>
using namespace std;
// Code by Vikram Chaurasia
// C++ program to demonstrate
// SCAN Disk Scheduling algorithm
int size = 8;
int disk_size = 200;
void LOOK(int arr[], int head, string direction)
{
int seek_count = 0;
int distance, cur_track;
vector<int> left, right;
vector<int> seek_sequence;
// appending values which are
// currently at left and right
// direction from the head.
for (int i = 0; i < size; i++) {
if (arr[i] < head)
left.push_back(arr[i]);
if (arr[i] > head)
right.push_back(arr[i]);
}
// sorting left and right vectors
// for servicing tracks in the
// correct sequence.
std::sort(left.begin(), left.end());
std::sort(right.begin(), right.end());
// run the while loop two times.
// one by one scanning right
// and left side of the head
int run = 2;
while (run--) {
if (direction == "left") {
for (int i = left.size() - 1; i >= 0; i--) {
cur_track = left[i];
// appending current track to seek sequence
seek_sequence.push_back(cur_track);
// calculate absolute distance
distance = abs(cur_track - head);
// increase the total count
seek_count += distance;
// accessed track is now the new head
head = cur_track;
}
// reversing the direction
direction = "right";
}

else if (direction == "right") {
for (int i = 0; i < right.size(); i++) {
cur_track = right[i];
// appending current track to seek sequence
seek_sequence.push_back(cur_track);
// calculate absolute distance
distance = abs(cur_track - head);
// increase the total count
seek_count += distance;
// accessed track is now new head
head = cur_track;
}
// reversing the direction
direction = "left";
}
}
cout << "Total number of seek operations = "
<< seek_count << endl;
cout << "Seek Sequence is" << endl;
for (int i = 0; i < seek_sequence.size(); i++) {
cout << seek_sequence[i] << endl;
}
}
// Driver code
int main()
{
// request array
int arr[size] = { 176, 79, 34, 60,
92, 11, 41, 114 };
int head = 50;
string direction = "right";
cout << "Initial position of head: "
<< head << endl;
LOOK(arr, head, direction);
return 0;
}

chevron_right
filter_none
Output:
Initial position of head: 50​
Total number of seek operations = 291​
Seek Sequence is​
60​
79​
92​
114​
176​
41​
34​
11

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/look-disk-scheduling-algorithm/
✍
Write a Testimonial

Difference between Magnetic Disk and Optical Disk
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Magnetic Disk and Optical Disk - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
The Magnetic disk and Optical disk are the storage devices that provide a way to store data for a long duration. Both are categorized under the category of secondary storage devices.
Magnetic disk:
A magnetic disk is a storage device that uses a magnetization process to read, write, rewrite and access data. The Magnetic disk is made of a set of circular platters. It is covered with a magnetic coating and stores data in
the form of tracks, spots, and sectors. Hard disks, zip disks, and floppy disks are common examples of magnetic disks. The number of bits stored on each track does not change by using the simplest constant angular
velocity.
Optical disk
An optical disk is any computer disk that uses optical storage techniques and technology to read and write data. It is a storage device in which optical (light) energy is used. It is a computer storage disk that stores data
digitally and uses laser beams to read and write data. It uses the optical technology in which laser light is centred to the spinning disks.

Difference Between Magnetic Disk and Optical Disk:
S.NO.
1
2
3
4
5
6
7
8

MAGNETIC DISK

OPTICAL DISK

Media type used is Muiltiple fixed disk
Intermediate signal to noise ratio
Sample rate is Low
Implementated where data is randomly accessed.
Only one disk can be used at a time
Tracks in the magnetic disk are generally circular
The data in the magnetic disk is randomly accessed.
In the magnetic disk, only one disk is accessed at a time.

Media type used is Single removable disk
Excellent signal to noise ratio
Sample rate is High
Implementated in streaming files.
Mass replication is possible
In optical disk the tracks are constructed spirally.
In the optical disk, the data is sequentially accessed.
Optical disk allows mass replication

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-magnetic-disk-and-optical-disk/
✍
Write a Testimonial

Access matrix in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Access matrix in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Access Matrix is a security model of protection state in computer system. It is represented as a matrix. Access matrix is used to define the rights of each process executing in the domain with respect to each object. The
rows of matrix represent domains and columns represent objects. Each cell of matrix represents set of access rights which are given to the processes of domain means each entry(i, j) defines the set of operations that a
process executing in domain Di can invoke on object Oj.
F1

F2

F3

Printer

D1 read
read
D2
print
D3
read execute
D4 read write
read write

According to the above matrix: there are four domains and four objects- three files(F1, F2, F3) and one printer. A process executing in D1 can read files F1 and F3. A process executing in domain D4 has same rights as D1
but it can also write on files. Printer can be accessed by only one process executing in domain D2. The mechanism of access matrix consists of many policies and semantic properties. Specifically, We must ensure that a
process executing in domain Di can access only those objects that are specified in row i.
Policies of access matrix concerning protection involve which rights should be included in the (i, j)th entry. We must also decide the domain in which each process executes. This policy is usually decided by the operating
system. The Users decide the contents of the access-matrix entries.
Association between the domain and processes can be either static or dynamic. Access matrix provides an mechanism for defining the control for this association between domain and processes. When we switch a process
from one domain to another, we execute a switch operation on an object(the domain). We can control domain switching by including domains among the objects of the access matrix. Processes should be able to switch
from one domain (Di) to another domain (Dj) if and only is a switch right is given to access(i, j).
F1

F2

F3

Printer D1

D1 read
read
D2
print
D3
read execute
D4 read write
read write

D2

D3

D4

switch
switch switch
switch

According to the matrix: a process executing in domain D2 can switch to domain D3 and D4. A process executing in domain D4 can switch to domain D1 and process executing in domain D1 can switch to domain D2.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/access-matrix-in-operating-system/
✍
Write a Testimonial

System Protection in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ System Protection in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Protection refers to a mechanism which controls the access of programs, processes, or users to the resources defined by a computer system. We can take protection as a helper to multi programming operating system, so
that many users might safely share a common logical name space such as directory or files.
Need of Protection:
To prevent the access of unauthorized users and
To ensure that each active programs or processes in the system uses resources only as the stated policy,
To improve reliability by detecting latent errors.

Role of Protection:
The role of protection is to provide a mechanism that implement policies which defines the uses of resources in the computer system.Some policies are defined at the time of design of the system, some are designed by
management of the system and some are defined by the users of the system to protect their own files and programs.

Every application has different policies for use of the resources and they may change over time so protection of the system is not only concern of the designer of the operating system. Application programmer should also
design the protection mechanism to protect their system against misuse.
Policy is different from mechanism. Mechanisms determine how something will be done and policies determine what will be done.Policies are changed over time and place to place. Separation of mechanism and policy is
important for the flexibility of the system.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/system-protection-in-operating-system/
✍
Write a Testimonial

C-SCAN Disk Scheduling Algorithm
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ C-SCAN Disk Scheduling Algorithm - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite: Disk Scheduling Algorithms and SCAN Disk Scheduling Algorithm
Given an array of disk track numbers and initial head position, our task is to find the total number of seek operations done to access all the requested tracks if C-SCAN disk scheduling algorithm is used.
What is C-SCAN (Circular Elevator) Disk Scheduling Algorithm?
Circular SCAN (C-SCAN) scheduling algorithm is a modified version of SCAN disk scheduling algorithm that deals with the inefficiency of SCAN algorithm by servicing the requests more uniformly. Like SCAN
(Elevator Algorithm) C-SCAN moves the head from one end servicing all the requests to the other end. However, as soon as the head reaches the other end, it immediately returns to the beginning of the disk without
servicing any requests on the return trip (see chart below) and starts servicing again once reaches the beginning. This is also known as the “Circular Elevator Algorithm” as it essentially treats the cylinders as a circular list
that wraps around from the final cylinder to the first one.

Algorithm:
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.

Let Request array represents an array storing indexes of tracks that have been requested in ascending order of their time of arrival. ‘head’ is the position of disk head.
The head services only in the right direction from 0 to size of the disk.
While moving in the left direction do not service any of the tracks.
When we reach at the beginning(left end) reverse the direction.
While moving in right direction it services all tracks one by one.
While moving in right direction calculate the absolute distance of the track from the head.
Increment the total seek count with this distance.
Currently serviced track position now becomes the new head position.
Go to step 6 until we reach at right end of the disk.
If we reach at the right end of the disk reverse the direction and go to step 3 until all tracks in request array have not been serviced.

Examples:
​
Input: ​
Request sequence = {176, 79, 34, 60, 92, 11, 41, 114}​
Initial head position = 50​
​
Output:​
Initial position of head: 50​
Total number of seek operations = 190​
Seek Sequence is​
60​
79​
92​
114​
176​
199​
0​
11​
34​
41​

The following chart shows the sequence in which requested tracks are serviced using SCAN.

Therefore, the total seek count is calculated as:
= (60-50)+(79-60)+(92-79)​
+(114-92)+(176-114)+(199-176)+(199-0)​
+(11-0)+(34-11)+(41-34)

Implementation:
Implementation of C-SCAN algorithm is given below.
Note:
The distance variable is used to store the absolute distance between the head and current track position. disk_size is the size of the disk. Vectors left and right stores all the request tracks on the left-hand side and the righthand side of the initial head position respectively.
filter_none
edit
close
play_arrow
link
brightness_4
code
#include <bits/stdc++.h>
using namespace std;
// Code by Vikram Chaurasia
// C++ program to demonstrate
// C-SCAN Disk Scheduling algorithm
int size = 8;
int disk_size = 200;
void CSCAN(int arr[], int head)
{
int seek_count = 0;
int distance, cur_track;
vector<int> left, right;
vector<int> seek_sequence;
// appending end values
// which has to be visited
// before reversing the direction
left.push_back(0);
right.push_back(disk_size - 1);
// tracks on the left of the
// head will be serviced when
// once the head comes back
// to the beggining (left end).
for (int i = 0; i < size; i++) {
if (arr[i] < head)
left.push_back(arr[i]);
if (arr[i] > head)
right.push_back(arr[i]);
}
// sorting left and right vectors
std::sort(left.begin(), left.end());
std::sort(right.begin(), right.end());
// first service the requests
// on the right side of the
// head.
for (int i = 0; i < right.size(); i++) {
cur_track = right[i];
// appending current track to seek sequence
seek_sequence.push_back(cur_track);
// calculate absolute distance
distance = abs(cur_track - head);
// increase the total count
seek_count += distance;
// accessed track is now new head
head = cur_track;
}
// once reached the right end
// jump to the beggining.
head = 0;
// Now service the requests again
// which are left.
for (int i = 0; i < left.size(); i++) {
cur_track = left[i];
// appending current track to seek sequence
seek_sequence.push_back(cur_track);
// calculate absolute distance
distance = abs(cur_track - head);
// increase the total count
seek_count += distance;
// accessed track is now the new head
head = cur_track;
}
cout << "Total number of seek operations = "
<< seek_count << endl;
cout << "Seek Sequence is" << endl;
for (int i = 0; i < seek_sequence.size(); i++) {
cout << seek_sequence[i] << endl;
}
}

// Driver code
int main()
{
// request array
int arr[size] = { 176, 79, 34, 60,
92, 11, 41, 114 };
int head = 50;
cout << "Initial position of head: " << head << endl;
CSCAN(arr, head);
return 0;
}

chevron_right
filter_none
Output:​
Initial position of head: 50​
Total number of seek operations = 190​
Seek Sequence is​
60​
79​
92​
114​
176​
199​
0​
11​
34​
41​

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : wilsonmun

Source
https://www.geeksforgeeks.org/c-scan-disk-scheduling-algorithm/
✍
Write a Testimonial

Linux vs Unix
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Linux vs Unix - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Linux is an operating system which is developed by Linus Torvalds in 1991. The name “Linux” originates from the Linux kernel. It is open-source and free to use the operating system. It is used for computer hardware
and software, game development, mainframes, etc. It can run various client programs.
Unix is a portable, multi-tasking, a multi-user operating system developed by AT&T. It started as a one-man venture under the initiative of Ken Thompson of Bell Labs. It proceeded to turn out to become the most widely
used operating systems. It is used in web servers, workstations, and PCs. Many business applications are accessible in it.
Linux
Linux is Open Source, and a large number of programmer work together online and contribute to its development.
It is an open-source operating system which is freely accessible to everyone.
Threat recognition and solution is very fast because Linux is mainly community-driven. So, if any Linux client posts any
sort of threat, a team of qualified developers starts working to resolve this threat.
It supports more file system than Unix.
File system supports – Ext2, Ext3, Ext4, Jfs, ReiserFS, Xfs, Btrfs, FAT, FAT32, NTFS
Linux provides two GUIs, KDE and Gnome. But there are many other options. For example, LXDE, Xfce, Unity, Mate,
and so on.
It is used everywhere from servers, PCs, smartphones, tablets to mainframes.
The default interface is BASH (Bourne Again SHell).
Anybody can use Linux whether a home client, developer or a student.
The source is accessible to the general public.
Originally developed for Intel’s x86 hardware processors. It is available for more than twenty different types of CPU
which also includes an ARM.
It has about 60-100 viruses listed till date.
Some Linux versions are Ubuntu, Debian GNU, Arch Linux, etc.
Interesting Facts:
Linux is only the kernel, and is not the full system that is used.
More than 90% of current Linux source code is composed by other developers.
Initially, Linux was compiled using GNU C compiler.
There are more than 10 Linux based Mobile operating Systems like Sailfish OS, Ubuntu Touch, Ubuntu Mobile, etc.
Linux is used by every major space program.
Nine out of top ten public clouds run on Linux.

Unix
Unix was developed by AT&T Labs, different commercial vendors, and non-profit
organizations.
It is an operating system which can be only utilized by its copywriters.
Unix clients require longer hold up time, to get the best possible bug fixing patch.
It also supports file system however lesser than Linux.
File system supports – jfs, gpfs, hfs, hfs+, ufs, xfs, zfs
Initially Unix was a command based OS, however later a GUI was created called Common
Desktop Environment. Most distributions now ship with Gnome.
It is used in servers, workstations, and PCs.
It initially used Bourne shell. But is also compatible with other GUIs.
Developed mainly for servers, workstations, and mainframes.
The source is not accessible to the general public.
It is available on PA-RISC and Itanium machines.
It has about 85-120 viruses listed till date (rough estimate).
Some Unix versions are SunOS, Solaris, SCO UNIX, AIX, HP/UX, ULTRIX, etc.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/linux-vs-unix/
✍
Write a Testimonial

Difference between Loading and Linking
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Loading and Linking - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Linking and Loading are the utility programs that play a important role in the execution of a program. Linking intakes the object codes generated by the assembler and combines them to generate the executable module.
On the other hand, the loading loads this executable module to the main memory for execution.
Loading:
Bringing the program from secondary memory to main memory is called Loading.
Linking:
Establishing the linking between all the modules or all the functions of the program in order to continue the program execution is called linking.

Differences between Linking and Loading:
1. The key difference between linking and loading is that the linking generates the executable file of a program whereas, the loading loads the executable file obtained from the linking into main memory for execution.
2. The linking intakes the object module of a program generated by the assembler. However, the loading intakes the executable module generated by the linking.
3. The linking combines all object modules of a program to generate executable modules it also links the library function in the object module to built-in libraries of the high-level programming language. On the other
hand, loading allocates space to an executable module in main memory.
Loading and Linking are further categorized into 2 types:
Static
Loading the entire program into the main memory before start of the program execution is called as static
loading.
Inefficent utilization of memory because whether it is required or not required entire program is brought into
the main memory.
Program execution will be faster.
Statically linked program takes constant load time every time it is loaded into the memory for execution.
If the static loading is used then accordingly static linking is applied.
Static linking is performed by programs called linkers as the last step in compiling a program. Linkers are also
called link editors.
In static linking if any of the external programs has changed then they have to be recompiled and re-linked
again else the changes won’t reflect in existing executable file.

Dynamic
Loading the program into the main memory on demand is called as dynamic loading.
Efficent utilization of memory.
Program execution will be slower.
Dynamic linking is performed at run time by the operating system.
If the dynamic loading is used then accordingly dynamic linking is applied.
In dynamic linking this is not the case and individual shared modules can be updated and recompiled.This
is one of the greatest advantages dynamic linking offers.
In dynamic linking load time might be reduced if the shared library code is already present in memory.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : muhammadfouadalharoon

Source
https://www.geeksforgeeks.org/difference-between-loading-and-linking/
✍
Write a Testimonial

Difference between File and Folder
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between File and Folder - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
File:
A File is defined as a set of related data or information that is being stored in secondary storage. A file is data file or a program file where former contains data and information in the form of alphanumeric, numeric or
binary and latter containing the program code and can also be executed, is a program file.
Example:

Folder:
It is used to contain many other folders and files. We can have any number of folders, and each folder can have different/numerous entries depending on the files created where each file has a position in a parent folder.

Example:

Difference between File and Folder:
S.No.
Comparison
File
Folder
1.
Extensions
Files can have extensions.
Folders does not have any extensions.
2.
Organizations
Serial, sequential, indexed sequential and direct file organizations.
Single directory per user and multiple directory per user organization.
3.
Contain other same entity No.
Yes.
4.
Basic
Collection of data.
A place to store a group of related files and folders.
5.
Space consumption
There is a specific size of a file.
Folder does not consume space in the memory.
6.
Properties
It has Name, Extension, Date, Time, Length and Protection attributes.
It has Name, Date, Time and Protection attributes.
7.
After Creation
After creation, We can open, save, rename, print, email and modify file content. After creation, We can move, rename and delete folders.
8.
Share on Network
We can not share file on network.
We can share folder on network.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-file-and-folder/
✍
Write a Testimonial

Reader-Writer problem using Monitors (pthreads)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Reader-Writer problem using Monitors (pthreads) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Monitors, Readers-Writers Problem
There is a shared resource which is accessed by multiple processes i.e. readers and writers. Any number of readers can read from the shared resource simultaneously, but only one writer can write to the shared resource at a
time. When a writer is writing data to the resource, no other process can access the resource. A writer cannot write to the resource if there are any readers accessing the resource at that time. Similarly, a reader can not read
if there is a writer accessing the resource or if there are any waiting writers.
The Reader-Writer problem using monitor can be implemented using pthreads. The POSIX threads (or pthread) libraries are a standards based thread API for C/C++. The library provides following synchronization
mechanisms:
Mutexes (pthread_mutex_t) – Mutual exclusion lock:
Block access to variables by other threads. This enforces exclusive access by a thread to a variable or set of variables.
Condition Variables – (pthread_cond_t):
The condition variable mechanism allows threads to suspend execution and relinquish the processor until some condition is true.
Implementation of Reader-Writer solution using pthread library:

Execute the program using following command in your Linux system
$g++ -pthread program_name.cpp​
$./a.out​
or​
$g++ -pthread program_name.cpp -o object_name​
$./object_name​

Code:
filter_none
edit
close
play_arrow
link
brightness_4
code
// Reader-Writer problem using monitors
#include <iostream>
#include <pthread.h>
#include <unistd.h>
using namespace std;
class monitor {
private:

// no. of readers
int rcnt;
// no. of writers
int wcnt;
// no. of readers waiting
int waitr;
// no. of writers waiting
int waitw;
// condition variable to check whether reader can read
pthread_cond_t canread;
// condition variable to check whether writer can write
pthread_cond_t canwrite;
// mutex for synchronisation
pthread_mutex_t condlock;
public:
monitor()
{
rcnt = 0;
wcnt = 0;
waitr = 0;
waitw = 0;
pthread_cond_init(&canread, NULL);
pthread_cond_init(&canwrite, NULL);
pthread_mutex_init(&condlock, NULL);
}
// mutex provide synchronisation so that no other thread
// can change the value of data
void beginread(int i)
{
pthread_mutex_lock(&condlock);
// if there are active or waiting writers
if (wcnt == 1 || waitw > 0) {
// incrementing waiting readers
waitr++;
// reader suspended
pthread_cond_wait(&canread, &condlock);
waitr--;
}
// else reader reads the resource
rcnt++;
cout << "reader " << i << " is reading\n";
pthread_mutex_unlock(&condlock);
pthread_cond_broadcast(&canread);
}
void endread(int i)
{
// if there are no readers left then writer enters monitor
pthread_mutex_lock(&condlock);
if (--rcnt == 0)
pthread_cond_signal(&canwrite);
pthread_mutex_unlock(&condlock);
}
void beginwrite(int i)
{
pthread_mutex_lock(&condlock);
// a writer can enter when there are no active
// or waiting readers or other writer
if (wcnt == 1 || rcnt > 0) {
++waitw;
pthread_cond_wait(&canwrite, &condlock);
--waitw;
}
wcnt = 1;
cout << "writer " << i << " is writing\n";
pthread_mutex_unlock(&condlock);
}
void endwrite(int i)
{
pthread_mutex_lock(&condlock);
wcnt = 0;
// if any readers are waiting, threads are unblocked
if (waitr > 0)
pthread_cond_signal(&canread);
else
pthread_cond_signal(&canwrite);
pthread_mutex_unlock(&condlock);
}
}
// global object of monitor class
M;
void* reader(void* id)
{
int c = 0;
int i = *(int*)id;
// each reader attempts to read 5 times
while (c < 5) {
usleep(1);
M.beginread(i);
M.endread(i);
c++;
}
}
void* writer(void* id)
{
int c = 0;
int i = *(int*)id;
// each writer attempts to write 5 times
while (c < 5) {
usleep(1);
M.beginwrite(i);
M.endwrite(i);
c++;
}
}
int main()
{
pthread_t r[5], w[5];
int id[5];
for (int i = 0; i < 5; i++) {
id[i] = i;
// creating threads which execute reader function

pthread_create(&r[i], NULL, &reader, &id[i]);
// creating threads which execute writer function
pthread_create(&w[i], NULL, &writer, &id[i]);
}
for (int i = 0; i < 5;
pthread_join(r[i],
}
for (int i = 0; i < 5;
pthread_join(w[i],
}

i++) {
NULL);
i++) {
NULL);

}

chevron_right
filter_none
Output:

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/reader-writer-problem-using-monitors-pthreads/
✍
Write a Testimonial

SetUID, SetGID, and Sticky Bits in Linux File Permissions
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ SetUID, SetGID, and Sticky Bits in Linux File Permissions - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
As explained in the article Permissions in Linux, Linux uses a combination of bits to store the permissions of a file. We can change the permissions using the chmod command, which essentially changes the ‘r’, ‘w’ and
‘x’ characters associated with the file.
Further, the ownership of files also depends on the uid (user ID) and the gid (group ID) of the creator, as discussed in this article. Similarly, when we launch a process, it runs with the uid and gid of the user who
launched it.
1. The setuid bit
This bit is present for files which have executable permissions. The setuid bit simply indicates that when running the executable, it will set its permissions to that of the user who created it (owner), instead of setting it to
the user who launched it. Similarly, there is a setgid bit which does the same for the gid.

To locate the setuid, look for an ‘s’ instead of an ‘x’ in the executable bit of the file permissions.
An example of an executable with setuid permission is passwd, as can be seen in the following output.
​
ls -l /etc/passwd​

This returns the following output:
​
-rwsr-xr-x root root 2447 Aug 29

2018 /etc/passwd​

As we can observe, the ‘x’ is replaced by an ‘s’ in the user section of the file permissions.
To set the setuid bit, use the following command.
​
chmod u+s ​

To remove the setuid bit, use the following command.

​
chmod u-s ​

2. The setgid bit
The setgid affects both files as well as directories. When used on a file, it executes with the privileges of the group of the user who owns it instead of executing with those of the group of the user who executed it.
When the bit is set for a directory, the set of files in that directory will have the same group as the group of the parent directory, and not that of the user who created those files. This is used for file sharing since they can
be now modified by all the users who are part of the group of the parent directory.
To locate the setgid bit, look for an ‘s’ in the group section of the file permissions, as shown in the example below.
​
-rwxrwsr-x root root 1427 Aug 2 2019 sample_file​

To set the setgid bit, use the following command.

​
chmod g+s ​

To remove the setgid bit, use the following command.
​
chmod g-s ​

Security Risks
The setuid bit is indeed quite useful in various applications, however, the executable programs supporting this feature should be carefully designed so as to not compromise on any security risks that follow, such as buffer
overruns and path injection. If a vulnerable program runs with root privileges, the attacker could gain root access to the system through it. To dodge such possibilities, some operating systems ignore the setuid bit for
executable shell scripts.
3. The sticky bit
The sticky bit was initially introduced to ‘stick’ an executable program’s text segment in the swap space even after the program has completed execution, to speed up the subsequent runs of the same program. However,
these days the sticky bit means something entirely different.
When a directory has the sticky bit set, its files can be deleted or renamed only by the file owner, directory owner and the root user. The command below shows how the sticky bit can be set.
​
chmod +t ​

Simply look for a ‘t’ character in the file permissions to locate the sticky bit. The snippet below shows how we can set the sticky bit for some directory “Gatos”, and how it prevents the new user from deleting a file in the
directory.

To remove the sticky bit, simply use the following command.
​
chmod -t ​

Since deleting a file is controlled by the write permission of the file, practical uses of the sticky bit involve world-writable directories such as ‘/tmp’ so that the delete permissions are reserved only for the owners of the
file.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/setuid-setgid-and-sticky-bits-in-linux-file-permissions/
✍
Write a Testimonial

Classical problems of Synchronization with Semaphore Solution
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Classical problems of Synchronization with Semaphore Solution - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
In this article, we will see number of classical problems of synchronization as examples of a large class of concurrency-control problems. In our solutions to the problems, we use semaphores for synchronization, since that
is the traditional way to present such solutions. However, actual implementations of these solutions could use mutex locks in place of binary semaphores.
These problems are used for testing nearly every newly proposed synchronization scheme. The following problems of synchronization are considered as classical problems:
1.
2.
3.
4.

Bounded-buffer (or Producer-Consumer) Problem,​
Dining-Philosphers Problem,​
Readers and Writers Problem,​
Sleeping Barber Problem

These are summarized, for detailed explanation, you can view the linked articles for each.

1. Bounded-buffer (or Producer-Consumer) Problem:

Bounded Buffer problem is also called producer consumer problem. This problem is generalized in terms of the Producer-Consumer problem. Solution to this problem is, creating two counting semaphores “full” and
“empty” to keep track of the current number of full and empty buffers respectively. Producers produce a product and consumers consume the product, but both use of one of the containers each time.
2. Dining-Philosphers Problem:
The Dining Philosopher Problem states that K philosophers seated around a circular table with one chopstick between each pair of philosophers. There is one chopstick between each philosopher. A philosopher may
eat if he can pickup the two chopsticks adjacent to him. One chopstick may be picked up by any one of its adjacent followers but not both. This problem involves the allocation of limited resources to a group of
processes in a deadlock-free and starvation-free manner.

3. Readers and Writers Problem:
Suppose that a database is to be shared among several concurrent processes. Some of these processes may want only to read the database, whereas others may want to update (that is, to read and write) the database.
We distinguish between these two types of processes by referring to the former as readers and to the latter as writers. Precisely in OS we call this situation as the readers-writers problem. Problem parameters:
One set of data is shared among a number of processes.
Once a writer is ready, it performs its write. Only one writer may write at a time.
If a process is writing, no other process can read it.
If at least one reader is reading, no other process can write.
Readers may not write and only read.
4. Sleeping Barber Problem:
Barber shop with one barber, one barber chair and N chairs to wait in. When no customers the barber goes to sleep in barber chair and must be woken when a customer comes in. When barber is cutting hair new
custmers take empty seats to wait, or leave if no vacancy.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/classical-problems-of-synchronization-with-semaphore-solution/
✍
Write a Testimonial

SCAN (Elevator) Disk Scheduling Algorithms
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ SCAN (Elevator) Disk Scheduling Algorithms - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite-Disk scheduling algorithms.
Given an array of disk track numbers and initial head position, our task is to find the total number of seek operations done to access all the requested tracks if SCAN disk scheduling algorithm is used.
SCAN (Elevator) algorithm
In SCAN disk scheduling algorithm, head starts from one end of the disk and moves towards the other end, servicing requests in between one by one and reach the other end. Then the direction of the head is reversed and
the process continues as head continuously scan back and forth to access the disk. So, this algorithm works as an elevator and hence also known as the elevator algorithm. As a result, the requests at the midrange are
serviced more and those arriving behind the disk arm will have to wait.

Algorithm1.
2.
3.
4.

Let Request array represents an array storing indexes of tracks that have been requested in ascending order of their time of arrival. ‘head’ is the position of disk head.
Let direction represents whether the head is moving towards left or right.
In the direction in which head is moving service all tracks one by one.
Calculate the absolute distance of the track from the head.

5.
6.
7.
8.

Increment the total seek count with this distance.
Currently serviced track position now becomes the new head position.
Go to step 3 until we reach at one of the ends of the disk.
If we reach at the end of the disk reverse the direction and go to step 2 until all tracks in request array have not been serviced.

Example:
​
Input: ​
Request sequence = {176, 79, 34, 60, 92, 11, 41, 114}​
Initial head position = 50​
Direction = left (We are moving from right to left)​
​
Output:​
Total number of seek operations = 226​
Seek Sequence is​
41​
34​
11​
0​
60​
79​
92​
114​
176​

The following chart shows the sequence in which requested tracks are serviced using SCAN.

Therefore, the total seek count is calculated as:
​
= (50-41)+(41-34)+(34-11)​
+(11-0)+(60-0)+(79-60)​
+(92-79)+(114-92)+(176-114)​
= 226​

Implementation:
Implementation of SCAN is given below. Note that distance is used to store the absolute distance between the head and current track position. disk_size is the size of the disk. Vectors left and right stores all the request
tracks on the left-hand side and the right-hand side of the initial head position respectively.
filter_none
edit
close
play_arrow
link
brightness_4
code
// C++ program to demonstrate
// SCAN Disk Scheduling algorithm
#include <bits/stdc++.h>
using namespace std;
int size = 8;
int disk_size = 200;
void SCAN(int arr[], int head, string direction)
{
int seek_count = 0;
int distance, cur_track;
vector<int> left, right;
vector<int> seek_sequence;
//
//
//
if

appending end values
which has to be visited
before reversing the direction
(direction == "left")
left.push_back(0);
else if (direction == "right")
right.push_back(disk_size - 1);
for (int i = 0; i < size; i++) {
if (arr[i] < head)
left.push_back(arr[i]);
if (arr[i] > head)
right.push_back(arr[i]);
}
// sorting left and right vectors
std::sort(left.begin(), left.end());
std::sort(right.begin(), right.end());
// run the while loop two times.
// one by one scanning right
// and left of the head
int run = 2;
while (run--) {
if (direction == "left") {
for (int i = left.size() - 1; i >= 0; i--) {
cur_track = left[i];
// appending current track to seek sequence
seek_sequence.push_back(cur_track);
// calculate absolute distance
distance = abs(cur_track - head);
// increase the total count
seek_count += distance;
// accessed track is now the new head
head = cur_track;
}
direction = "right";
}
else if (direction == "right") {
for (int i = 0; i < right.size(); i++) {
cur_track = right[i];
// appending current track to seek sequence
seek_sequence.push_back(cur_track);
// calculate absolute distance
distance = abs(cur_track - head);

// increase the total count
seek_count += distance;
// accessed track is now new head
head = cur_track;
}
direction = "left";
}
}
cout << "Total number of seek operations = "
<< seek_count << endl;
cout << "Seek Sequence is" << endl;
for (int i = 0; i < seek_sequence.size(); i++) {
cout << seek_sequence[i] << endl;
}
}
// Driver code
int main()
{
// request array
int arr[size] = { 176, 79, 34, 60,
92, 11, 41, 114 };
int head = 50;
string direction = "left";
SCAN(arr, head, direction);
return 0;
}

chevron_right
filter_none
Output:
​
Total number of seek operations = 226​
Seek Sequence is​
41​
34​
11​
0​
60​
79​
92​
114​
176​

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/scan-elevator-disk-scheduling-algorithms/
✍
Write a Testimonial

FCFS Disk Scheduling Algorithms
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ FCFS Disk Scheduling Algorithms - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite: Disk scheduling algorithms.
Given an array of disk track numbers and initial head position, our task is to find the total number of seek operations done to access all the requested tracks if First Come First Serve (FCFS) disk scheduling algorithm is
used.
First Come First Serve (FCFS)
FCFS is the simplest disk scheduling algorithm. As the name suggests, this algorithm entertains requests in the order they arrive in the disk queue. The algorithm looks very fair and there is no starvation (all requests are
serviced sequentially) but generally, it does not provide the fastest service.

Algorithm:
1.
2.
3.
4.
5.

Let Request array represents an array storing indexes of tracks that have been requested in ascending order of their time of arrival. ‘head’ is the position of disk head.
Let us one by one take the tracks in default order and calculate the absolute distance of the track from the head.
Increment the total seek count with this distance.
Currently serviced track position now becomes the new head position.
Go to step 2 until all tracks in request array have not been serviced.

Example:
​
Input: ​
Request sequence = {176, 79, 34, 60, 92, 11, 41, 114}​
Initial head position = 50​
​
Output:​
Total number of seek operations = 510​
Seek Sequence is​
176​
79​
34​
60​
92​
11​
41​
114​

The following chart shows the sequence in which requested tracks are serviced using FCFS.

Therefore, the total seek count is calculated as:
​
= (176-50)+(176-79)+(79-34)+(60-34)+(92-60)+(92-11)+(41-11)+(114-41)​
= 510​

Implementation:
Implementation of FCFS is given below. Note that distance is used to store absolute distance between head and current track position.
C++
filter_none
edit
close
play_arrow
link
brightness_4
code
// C++ program to demonstrate
// FCFS Disk Scheduling algorithm
#include <bits/stdc++.h>
using namespace std;
int size = 8;
void FCFS(int arr[], int head)
{
int seek_count = 0;
int distance, cur_track;
for (int i = 0; i < size; i++) {
cur_track = arr[i];
// calculate absolute distance
distance = abs(cur_track - head);
// increase the total count
seek_count += distance;
// accessed track is now new head
head = cur_track;
}
cout << "Total number of seek operations = "
<< seek_count << endl;
// Seek sequence would be the same
// as request array sequence
cout << "Seek Sequence is" << endl;
for (int i = 0; i < size; i++) {
cout << arr[i] << endl;
}
}
// Driver code
int main()
{
// request array
int arr[size] = { 176, 79, 34, 60, 92, 11, 41, 114 };
int head = 50;
FCFS(arr, head);
return 0;
}

chevron_right
filter_none
Java
filter_none
edit
close
play_arrow
link
brightness_4
code
// Java program to demonstrate
// FCFS Disk Scheduling algorithm
class GFG
{
static int size = 8;
static void FCFS(int arr[], int head)
{
int seek_count = 0;
int distance, cur_track;
for (int i = 0; i < size; i++)
{
cur_track = arr[i];
// calculate absolute distance
distance = Math.abs(cur_track - head);
// increase the total count
seek_count += distance;
// accessed track is now new head
head = cur_track;
}

System.out.println("Total number of " +
"seek operations = " +
seek_count);
// Seek sequence would be the same
// as request array sequence
System.out.println("Seek Sequence is");
for (int i = 0; i < size; i++)
{
System.out.println(arr[i]);
}
}
// Driver code
public static void main(String[] args)
{
// request array
int arr[] = { 176, 79, 34, 60,
92, 11, 41, 114 };
int head = 50;
FCFS(arr, head);
}
}
// This code is contributed by 29AjayKumar

chevron_right
filter_none
Python3
filter_none
edit
close
play_arrow
link
brightness_4
code
# Python program to demonstrate
# FCFS Disk Scheduling algorithm
size = 8;
def FCFS(arr, head):
seek_count = 0;
distance, cur_track = 0, 0;
for i in range(size):
cur_track = arr[i];
# calculate absolute distance
distance = abs(cur_track - head);
# increase the total count
seek_count += distance;
# accessed track is now new head
head = cur_track;
print("Total number of seek operations = ",
seek_count);
# Seek sequence would be the same
# as request array sequence
print("Seek Sequence is");
for i in range(size):
print(arr[i]);
# Driver code
if __name__ == '__main__':
# request array
arr = [ 176, 79, 34, 60,
92, 11, 41, 114 ];
head = 50;
FCFS(arr, head);
# This code contributed by Rajput-Ji

chevron_right
filter_none
C#
filter_none
edit
close
play_arrow
link
brightness_4
code
// C# program to demonstrate
// FCFS Disk Scheduling algorithm
using System;
class GFG
{
static int size = 8;
static void FCFS(int []arr, int head)
{
int seek_count = 0;
int distance, cur_track;
for (int i = 0; i < size; i++)
{
cur_track = arr[i];
// calculate absolute distance
distance = Math.Abs(cur_track - head);
// increase the total count
seek_count += distance;
// accessed track is now new head
head = cur_track;
}
Console.WriteLine("Total number of " +

"seek operations = " +
seek_count);
// Seek sequence would be the same
// as request array sequence
Console.WriteLine("Seek Sequence is");
for (int i = 0; i < size; i++)
{
Console.WriteLine(arr[i]);
}
}
// Driver code
public static void Main(String[] args)
{
// request array
int []arr = { 176, 79, 34, 60,
92, 11, 41, 114 };
int head = 50;
FCFS(arr, head);
}
}
// This code is contributed by PrinciRaj1992

chevron_right
filter_none
Output:
​
Total number of seek operations = 510​
Seek Sequence is​
176​
79​
34​
60​
92​
11​
41​
114​

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : mehul_02, 29AjayKumar, princiraj1992, Rajput-Ji

Source
https://www.geeksforgeeks.org/fcfs-disk-scheduling-algorithms/
✍
Write a Testimonial

Drawback of Resource Preemption
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Drawback of Resource Preemption - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
We know that when the deadlock has occurred in the operating system, then for removing it we are going to preempt some process and give resources to others processes, so whenever the preemption is required to remove
the deadlock, some drawbacks are arises as: Select the victim, Rollback of processes, and Starvation.
1. Select the victim:
First drawback is that when we want to select the resources and which process needs to be preempted, we must pre-determine the processes that in which order the processes are to use the resources to minimize the
cost, so selecting a victim is a drawback during the resource preemption.

2. Rollback of processes:
Second drawback is that when we preempt one resource from the process then that time one question is raised that what should we do with that process and the only possible answer is that we should get to roll back
the process and place it on some safe state and restart it. But in reality, it is very difficult to get a safe state, so the better way is to roll back completely abort the process and restart it again. It is the more effective
way to roll back the process only as for as to remove the deadlock.
3. Starvation:
Third drawback is how we can ensure that starvation will further not occur is the system, that is how we can ensure that resources will not be preempted. In an operating system, we know that selection a victim is a
cost-effective process, so it may happen when we select the same victim as the same process, and the result will be as the process will never be completed and which lead to starvation.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source

https://www.geeksforgeeks.org/drawback-of-resource-preemption/
✍
Write a Testimonial

Hardware Protection and Type of Hardware Protection
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Hardware Protection and Type of Hardware Protection - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
In this article, we are going to learn about hardware protection and it’s the type. so first let’s see the type of hardware which is used in a computer system. we know that a computer system contains the hardware like
processor, monitor, RAM and many more, and one thing that the operating system ensures that these devices can not directly accessible by the user.
Basically, hardware protection is divided into 3 categories: CPU protection, Memory Protection, and I/O protection. These are explained as following below.
1. CPU Protection:
CPU protection is referred to as we can not give CPU to a process forever, it should be for some limited time otherwise other processes will not get the chance to execute the process. So for that, a timer is used to get
over from this situation. which is basically give a certain amount of time a process and after the timer execution a signal will be sent to the process to leave the CPU. hence process will not hold CPU for more time.
2. Memory Protection:
In memory protection, we are talking about that situation when two or more processes are in memory and one process may access the other process memory. and to protecting this situation we are using two registers
as:
1. Bare register​
2. Limit register

So basically Bare register store the starting address of program and limit register store the size of the process, so when a process wants to access the memory then it is checked that it can access or can not access the
memory.
3. I/O Protection:
So when we ensuring the I/O protection then some cases will never have occurred in the system as:
1. Termination I/O of other process
2. View I/O of other process
3. Giving priority to a particular process I/O
We know that when an application process wants to access any I/O device it should be done through system call so that the Operating system will monitor the task.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/hardware-protection-and-type-of-hardware-protection/
✍
Write a Testimonial

Network File System (NFS)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Network File System (NFS) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
The advent of distributed computing was marked by the introduction of distributed file systems. Such systems involved multiple client machines and one or a few servers. The server stores data on its disks and the clients
may request data through some protocol messages.
Advantages of a distributed file system:
Allows easy sharing of data among clients.
Provides centralized administration.
Provides security, i.e. one must only secure the servers to secure data.
Distributed File System Architecture:

Even a simple client/server architecture involves more components than the physical file systems discussed previously in OS. The architecture consists of a client-side file system and a server-side file system. A client
application issues a system call (e.g. read(), write(), open(), close() etc.) to access files on the client-side file system, which in turn retrieves files from the server. It is interesting to note that to a client application, the
process seems no different than requesting data from a physical disk, since there is no special API required to do so. This phenomenon is known as transparency in terms of file access. It is the client-side file system that
executes commands to service these system calls.
For instance, assume that a client application issues the read() system call. The client-side file system then messages the server-side file system to read a block from the server’s disk and return the data back to the client.
Finally, it buffers this data into the read() buffer and completes the system call.
The server-side file system is also simply called the file server.
Sun’s Network File System:
The earliest successful distributed system could be attributed to Sun Microsystems, which developed the Network File System (NFS). NFSv2 was the standard protocol followed for many years, designed with the goal of
simple and fast server crash recovery. This goal is of utmost importance in multi-client and single-server based network architectures because a single instant of server crash means that all clients are unserviced. The entire
system goes down.

Stateful protocols make things complicated when it comes to crashes. Consider a client A trying to access some data from the server. However, just after the first read, the server crashed. Now, when the server is up and
running, client A issues the second read request. However, the server does not know which file the client is referring to, since all that information was temporary and lost during the crash.
Stateless protocols come to our rescue. Such protocols are designed so as to not store any state information in the server. The server is unaware of what the clients are doing — what blocks they are caching, which files
are opened by them and where their current file pointers are. The server simply delivers all the information that is required to service a client request. If a server crash happens, the client would simply have to retry the
request. Because of their simplicity, NFS implements a stateless protocol.
File Handles:
NFS uses file handles to uniquely identify a file or a directory that the current operation is being performed upon. This consists of the following components:
Volume Identifier – An NFS server may have multiple file systems or partitions. The volume identifier tells the server which file system is being referred to.
Inode Number – This number identifies the file within the partition.
Generation Number – This number is used while reusing an inode number.
File Attributes:
“File attributes” is a term commonly used in NFS terminology. This is a collective term for the tracked metadata of a file, including file creation time, last modified, size, ownership permissions etc. This can be accessed by
calling stat() on the file.
NFSv2 Protocol:
Some of the common protocol messages are listed below.
Message

Description

NFSPROC_GETATTR Given a file handle, returns file attributes.
NFSPROC_SETATTR Sets/updates file attributes.
NFSPROC_LOOKUP Given file handle and name of the file to look up, returns file handle.
NFSPROC_READ
Given file handle, offset, count data and attributes, reads the data.
NFSPROC_WRITE
Given file handle, offset, count data and attributes, writes data into the file.
NFSPROC_CREATE Given the directory handle, name of file and attributes, creates a file.
NFSPROC_REMOVE Given the directory handle and name of file, deletes the file.
NFSPROC_MKDIR Given directory handle, name of directory and attributes, creates a new directory.
The LOOKUP protocol message is used to obtain the file handle for further accessing data. The NFS mount protocol helps obtain the directory handle for the root (/) directory in the file system. If a client application
opens a file /abc.txt, the client-side file system will send a LOOKUP request to the server, through the root (/) file handle looking for a file named abc.txt. If the lookup is successful, the file attributes are returned.
Client-Side Caching:
To improve performance of NFS, distributed file systems cache the data as well as the metadata read from the server onto the clients. This is known as client-side caching. This reduces the time taken for subsequent client
accesses. The cache is also used as a temporary buffer for writing. This helps improve efficiency even more since all writes are written onto the server at once.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/network-file-system-nfs/
✍
Write a Testimonial

Journaling or write-ahead logging
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Journaling or write-ahead logging - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Journaling, or write-ahead logging is a sophisticated solution to the problem of file system inconsistency in operating systems. Inspired by database management systems, this method first writes down a summary of the
actions to be performed into a “log” before actually writing them to the disk. Hence the name, “write-ahead logging”. In the case of a crash, the OS can simply check this log and pick up from where it left off. This saves
multiple disk scans to fix inconsistency, as is the case with FSCK.
Good examples of systems that implement data journaling include Linux ext3 and ext4 file systems, and Windows NTFS.
Data Journaling:
A log is stored in a simple data structure called the journal. The figure below shows its structure, which comprises of three components.

1. TxB (Transaction Begin Block):
This contains the transaction ID, or the TID.
2. Inode, Bitmap and Data Blocks (Metadata):
These three blocks contain a copy of the contents of the blocks to be updated in the disk.
3. TxE (Transaction End Block)
This simply marks the end of the transaction identified by the TID.
As soon as an update is requested, it is written onto the log, and thereafter onto the file system. Once all these writes are successful, we can say that we have reached the checkpoint and the update is complete.
What if a crash occurs during journaling ?
One could argue that journaling, itself, is not atomic. Therefore, how does the system handle an un-checkpointed write ? To overcome this scenario, journaling happens in two steps: simultaneous writes to TxB and the
following three blocks, and then write of the TxE. The process can be summarized as follows.
1. Journal Write:
Write TxB, inode, bitmap and data block contents to the journal (log).
2. Journal Commit:
Write TxE to the journal (log).
3. Checkpoint:
Write the contents of the inode, bitmap and data block onto the disk.
A crash may occur at different points during the process of journaling. If a crash occurs at step 1, i.e. before the TxE, we can simply skip this transaction altogether and the file system stays consistent.

If a crash occurs at step 2, it means that although the transaction has been logged, it hasn’t been written onto the disk completely. We cannot be sure which of the three blocks (inode, bitmap and data block) were actually
updated and which ones suffered a crash. In this case, the system scans the log for recent transactions, and performs the last transaction again. This does lead to redundant disk writes, but ensures consistency. This process
is called redo logging.
Using the Journal as a Circular Buffer:
Since many transactions are made, the journal log might get used up. To address this issue, we can use the journal log as a circular buffer wherein newer transactions keep replacing the old ones in a circular manner. The
figure below shows an overall view of the journal, with tr1 as the oldest transaction and tr5 the newest.

The super block maintains pointers to the oldest and the newest transactions. As soon as the transaction is complete, it is marked as “free” and the super block is updated to the next transaction.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/journaling-or-write-ahead-logging/
✍
Write a Testimonial

File System Consistency Checker (FSCK)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ File System Consistency Checker (FSCK) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
File system inconsistency is a major issue in operating systems. FSCK is one of the standard solutions adopted.
File System Consistency Checker (FSCK):
FSCK is one approach still used by older Linux-based systems to find and repair inconsistencies. It is not a complete solution and may still have inodes pointing to garbage data. The major focus is to make the metadata
internally consistent.
The following are the checks that FSCK performs to achieve consistency:

Superblock Checks:
FSCK performs a sanity check to see if the file size is greater than the number of blocks allocated. In this case, it tries to find the suspect superblock and use an alternate copy instead.
Free Block Checks:
FSCK also scans inodes to ensure that blocks in inodes are marked allocated.
Inode State Checks:
FSCK checks inodes for corruption. Corrupted inodes are simply cleared.
Inode Link Checks:
FSCK counts the number of links to an inode, and modifies the inode count. If an allocated inode has no directory or file referring to it, FSCK moves it to the lost and found directory.
Duplicate Pointers:
FSCK checks duplicate pointers. For example, if two inodes have pointers to the same data block, one of the inode can be deleted.
Bad Blocks:
A bad pointer is simply one that points to a memory address which is out of range. In this case, FSCK deletes the pointer.
Directory Checks:
FSCK makes sure the directory format is correct, e.g. they should start with “.” and “..”.
Advantages of FSCK:
It requires little overhead space.
Disadvantages of FSCK:
Scanning the disk, again and again, is slow and infeasible for large disk sizes.
It requires a heavy understanding and prior knowledge of the file system. As file systems continue to evolve, it is difficult to keep track of each and every nuance.
fsck Command in Linux:
The fsck command in Linux allows us to manually check for file system inconsistencies. Below is the sample usage of the command.
sudo fsck /dev/sda2

The above command simply checks the file system mounted onto the /dev/sda2 partition. If the file system may have some inconsistencies, fsck prompts us with possible actions.
fsck.fat 4.1 (2017-01-24)

0x41: Dirty bit is set. Fs was not properly unmounted and some data may be corrupt: Remove dirty bit, and No action.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/file-system-consistency-checker-fsck/
✍
Write a Testimonial

Cache Memory Design
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Cache Memory Design - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Cache Memory
A detailed discussion of the cache style is given in this article. The key elements are concisely summarized here. we are going to see that similar style problems should be self-addressed in addressing storage and cache
style. They represent the subsequent categories: Cache size, Block size, Mapping function, Replacement algorithm, and Write policy. These are explained as following below.

1. Cache Size:
It seems that moderately tiny caches will have a big impact on performance.
2. Block Size:
Block size is the unit of information changed between cache and main memory.
As the block size will increase from terribly tiny to larger sizes, the hit magnitude relation can initially increase as a result of the principle of locality.the high chance that knowledge within the neck of the woods of a
documented word square measure possible to be documented within the close to future. As the block size increases, a lot of helpful knowledge square measure brought into the cache.
The hit magnitude relation can begin to decrease, however, because the block becomes even larger and also the chance of victimization the new fetched knowledge becomes but the chance of reusing the information
that ought to be abstracted of the cache to form area for the new block.
3. Mapping Function:
When a replacement block of data is scan into the cache, the mapping performs determines that cache location the block will occupy. Two constraints have an effect on the planning of the mapping perform. First,
once one block is scan in, another could be replaced.
We would wish to do that in such the simplest way to minimize the chance that we are going to replace a block which will be required within the close to future. A lot of versatile the mapping perform, a lot of scopes
we’ve to style a replacement algorithmic rule to maximize the hit magnitude relation. Second, a lot of versatile the mapping perform, a lot of advanced is that the electronic equipment needed to look the cache to see
if a given block is within the cache.
4. Replacement Algorithm:
The replacement algorithmic rule chooses, at intervals, the constraints of the mapping perform, which block to interchange once a replacement block is to be loaded into the cache and also the cache already has all
slots full of alternative blocks. We would wish to replace the block that’s least possible to be required once more within the close to future. Although it’s impossible to spot such a block, a fairly effective strategy is
to interchange the block that has been within the cache longest with no relevance.
This policy is spoken because of the least-recently-used (LRU) algorithmic rule. Hardware mechanisms square measure required to spot the least-recently-used block
5. Write Policy:
If the contents of a block within the cache square measure altered, then it’s necessary to write down it back to main memory before exchange it. The written policy dictates once the memory write operation takes
place. At one extreme, the writing will occur whenever the block is updated.
At the opposite extreme, the writing happens only if the block is replaced. The latter policy minimizes memory write operations however leaves the main memory in associate obsolete state. This can interfere with
the multiple-processor operation and with direct operation by I/O hardware modules.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/cache-memory-design/
✍
Write a Testimonial

How to test all Linux distributions without downloading them?
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ How to test all Linux distributions without downloading them? - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
There are many Linux distribution available to use but finding perfect distribution is very difficult. You have to download almost 2GB of the file for each distribution and during testing, you don’t find anything good.
What if you could test a Linux distribution without actually downloading it and setting up a VMware or wasting a lot of time in installing it as a primary Operating system? Distrotest.net is the website which provides
you the facility to test more than 700 versions with 225 Operating systems. It has each and every Operating system with all different versions.

Distrotest.net works on a service based website where you can choose among a variety of different Linux distributions to test. Just choose your distribution from the A-Z list and start your system. The website provides all
the features of the operating system, including install, partition and everything like you do in your actual system. After choosing your desired Operating system you enter in a queue where you are given a slot number and
you have to wait a little bit till your turn comes. The slots are given due to many users running systems and makes server loaded. Sometimes heavy usage kills the website and servers go offline but in a few hours, this
issue is fixed. Also if you face any issue in the website you can contact their support.

Using distrotest.net is very easy but there are some problems which you can face while using it:
1. Slow system: The Operating system launched in the server runs very slow. After moving your cursor you may need to wait for a second for the response which kills a lot of time.
2. Waiting Queue: Once you tab “system start” you enter a queue in which you are given a number, and no one knows how much time will it take till the other users shut down systems, however, for each user there is
a time limit of 30 Minutes.

3. Server offline: when there is more traffic and heavy usage, the servers can go offline, but the support tries to fix the issue soon.

A great project has some problems but those problems soon get a solution. If you face any problems or if you have any suggestion you can always reach their support and they are happy to help. Below are some
screenshots of the system that I tried while writing this article:

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/how-to-test-all-linux-distributions-without-downloading-them/
✍
Write a Testimonial

Python | How to lock Critical Sections
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Python | How to lock Critical Sections - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
This article aims how to lock the threads and critical sections in the given program to avoid race conditions. So, using Lock object in the threading library to make mutable objects safe to use by multiple threads.
Code #1 :
filter_none
edit
close
play_arrow
link
brightness_4
code
import threading
class counter_share:
'''
multiple threads can share.
'''
def __init__(self, initial_key = 0):
self._key = initial_key
self._key_lock = threading.Lock()
def incr(self, delta = 1):
with self._key_lock:
# Increasing the counter with lock
self._key += delta
def decr(self, delta = 1):
with self._key_lock:
# Decreasing the counter with lock
self._key -= delta

chevron_right
filter_none
Using a with statement along with the lock ensures the mutual exclusion. By exclusion, it is meant that at a time only one thread (under with statement) is allowed to execute the block of a statement.
The lock for the duration of intended statements is acquired and is released when the control flow exits the indented block. Thread scheduling is inherently nondeterministic. Because of this, randomly corrupted data and
‘race condition’ can result as there is a failure to use locks. So, whenever a shared mutable state is accessed by multiple threads, locks should always be used to avoid this.

In older Python code, it is common to see locks explicitly acquired and released.
Code #2 : Variant of code 1
filter_none
edit
close
play_arrow
link
brightness_4
code
import threading
class counter_share:
# multiple threads can share counter objects
def __init__(self, initial_key = 0):
self._key = initial_key
self._key_lock = threading.Lock()
def incr(self, delta = 1):
# Increasing the counter with lock
self._key_lock.acquire()
self._key += delta
self._key_lock.release()
def decr(self, delta = 1):
# Decreasing the counter with lock
self._key_lock.acquire()
self._key -= delta
self._key_lock.release()

chevron_right
filter_none
In situations where release() method is not called or while holding a lock no exception is raised, the with statement is much less prone to error.
Each thread in a program is not allowed to acquire one lock at a time, this can potentially avoid the cases of deadlock. Introduce more advanced deadlock avoidance into the program if it is not possible.
Synchronization primitives, such as RLock and Semaphore objects are found in the threading library.
Except the simple module locking, there is some more special purpose being solved :
An RLock or re-entrant lock object is a lock that can be acquired multiple times by the same thread.
It implements code primarily on the basis of locking or synchronization a construct known as a “monitor.” Only one thread is allowed to use an entire function or the methods of a class while the lock is held, with
this type of locking.
Code #3 : Implementing the SharedCounter class.
filter_none
edit
close

play_arrow
link
brightness_4
code
import threading
class counter_share:
# multiple threads can share counter objects
_lock = threading.RLock()
def __init__(self, initial_key = 0):
self._key = initial_key
def incr(self, delta = 1):
# Increasing the counter with lock
with SharedCounter._lock:
self._key += delta
def decr(self, delta = 1):
# Decreasing the counter with lock
with SharedCounter._lock:
self.incr(-delta)

chevron_right
filter_none
The lock is meant to synchronize the methods of the class, inspite of lock being tied to the per-instance mutable state.
There is just a single class-level lock shared by all instances of the class in this code variant.
Only one thread is allowed to be using the methods of the class at once is ensured.
it is OK for methods to call other methods that also use the lock if they already have the locks. (for example the decr() method).
If there are a large number of counters, it is much more memory-efficient. However, it may cause more lock contention in programs that use a large number of threads and make frequent counter updates.
A Semaphore item is a synchronization crude dependent on a mutual counter. The counter is increased upon the finish of the with square. On the off chance that the counter is zero, advance is obstructed until the counter
is increased by another string. On the off chance that the counter is nonzero, the with explanation decrements the tally and a string is permitted to continue.
Rather than straightforward locking, Semaphore items are increasingly valuable for applications including motioning between strings or throttling. Despite the fact that a semaphore can be utilized in a similar way as a
standard Lock, the additional multifaceted nature in usage contrarily impacts execution.
Code #4 : To limit the amount of concurrency in a part of code, use a semaphore.
filter_none
edit
close
play_arrow
link
brightness_4
code
from threading import Semaphore
import urllib.request
# five threads are allowed to run at once (at max)
_fetch_url_sema = Semaphore(5)
def fetch_url(url):
with _fetch_url_sema:
return urllib.request.urlopen(url)

chevron_right
filter_none

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/python-how-to-lock-critical-sections/
✍
Write a Testimonial

Interprocess Communication in Distributed Systems
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Interprocess Communication in Distributed Systems - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Interprocess Communication is a process of exchanging the data between two or more independent process in a distributed environment is called as Interprocess communication. Interprocess communication on the
internet provides both Datagram and stream communication.
Examples Of Interprocess Communication:
1. N number of applications can communicate with the X server through network protocols.
2. Servers like Apache spawn child processes to handle requests.
3. Pipes are a form of IPC: grep foo file | sort

It has two functions:
1. Synchronization:
Exchange of data is done synchronously which means it has a single clock pulse.
2. Message Passing:
When processes wish to exchange information. Message passing takes several forms such as: pipes, FIFO, Shared Memory, and Message Queues.
Characteristics Of Inter-process Communication:
There are mainly five characteristics of inter-process communication in a distributed environment/system.
1. Synchronous System Calls:
In the synchronous system calls both sender and receiver use blocking system calls to transmit the data which means the sender will wait until the acknowledgment is received from the receiver and receiver waits
until the message arrives.
2. Asynchronous System Calls:
In the asynchronous system calls, both sender and receiver use non-blocking system calls to transmit the data which means the sender doesn’t wait from the receiver acknowledgment.
3. Message Destination:
A local port is a message destination within a computer, specified as an integer. Aport has exactly one receiver but many senders. Processes may use multiple ports from which to receive messages. Any process that
knows the number of a port can send the message to it.
4. Reliability:
It is defined as validity and integrity.
5. Integrity:
Messages must arrive without corruption and duplication to the destination.
6. Validity:
Point to point message services are defined as reliable, If the messages are guaranteed to be delivered without being lost is called validity.
7. Ordering:
It is the process of delivering messages to the receiver in a particular order. Some applications require messages to be delivered in the sender order i.e the order in which they were transmitted by the sender.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/interprocess-communication-in-distributed-systems/
✍
Write a Testimonial

Difference between Volatile Memory and Non-Volatile Memory
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Volatile Memory and Non-Volatile Memory - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Volatile Memory:
It is that the quite hardware that stores information quickly. it’s additionally referred as temporary memory. The information within the volatile memory is hold on solely till the ability is provided to the system, once the
system is turned off the information gift within the volatile memory is deleted mechanically. RAM (Random Access Memory) and Cache Memory are the common example of the volatile memory. It’s quite quick and
economical in nature and may be accessed apace.
Non-Volatile Memory:
It is the type of memory in which data or information remains keep within the memory albeit power is completed. ROM (Read Only Memory) is the most common example of non-volatile memory. it’s not that a lot of
economical and quick in nature as compare to volatile memory however stores information for the longer amount. Non-volatile memory is slow concerning accessing. All such information that must be hold on for good or
for a extended amount is hold on in non-volatile memory. Non-volatile memory has a huge impact on a system’s storage capacity.
Let’s see that the difference between volatile and non-volatile memory:

S.NO
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.

Volatile Memory
Volatile memory is the type of memory in which data isn’t keep in memory as before long as
power is gone.
Volatile memory is not a permanent memory.
It is faster than non-volatile memory.
RAM is the example of volatile memory.
In volatile memory, data can be easily transferred in comparison of non-volatile memory.
Volatile memory can read and write.
Volatile memory has less storage.
In volatile memory, the program’s data are stored which are currently in process by the CPU.
Volatile memory is more costly per unit size.
Volatile memory has a huge impact on the system’s performance.

Non-Volatile Memory
Non-volatile memory is the type of memory in which data or information remains keep within the memory albeit
power is completed.
Non-volatile memory is a permanent memory.
It is slow than volatile memory.
ROM is the example of non-volatile memory.
In non-volatile memory, data can not be easily transferred in comparison of volatile memory.
Non-volatile memory can’t write, it only read.
Non-volatile memory has more storage than volatile memory.
In non-volatile memory, any kind of data which has to be saved permanently are stored.
Non-volatile memory is less costly per unit size.
Non-volatile memory has a huge impact on a system’s storage capacity.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-volatile-memory-and-non-volatile-memory/
✍
Write a Testimonial

Comparison on using Java for an Operating System instead of C
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Comparison on using Java for an Operating System instead of C - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Java is a managed language which offers memory safety.
In Java, pointers do not exist, so we cannot do pointer arithmetic on a function pointer. An application can invoke a method defined in a class through the class object. In Java, we cannot do unsafe typecast of an object to
overwrite the method pointer with something else. An out of bound array access throws a runtime exception in Java, so return address corruption is not possible.
Now, let us look at some important aspects of an Operating System implemented through Java.
Memory isolation:
In Java, memory can only be accessed via objects. A Java application cannot access memory outside an object. The virtual memory is not abstracted to the application. Due to these reasons, we do not need additional
hardware support (segmentation/page tables) for process allocation. There is no need for page tables or privilege rings. The kernel is itself a Java process, so both kernel and untrusted applications execute in ring-0.
System Calls:
To execute system calls, we need to switch to the kernel process. Switching to a different process does not require a page table switch. So, the system call handler is just a context switch routine that copies system
call arguments in kernel address space and directly jumps to the system call handler.
Inter Process Communication (IPC):
IPC is similar to the system call handler. The context switch method needs to invoke the receive routine in the target process after copying messages. So, this is better in comparison to Linux and Windows, where we
have to do two ring transitions and one context switch to schedule the target routine.
Device drivers:
Each device driver can be assigned to a separate Java process. Device drivers can talk to kernel via system calls.
But, the existing operating systems are not written in Java. It is because, Java is not an efficient language particularly due to garbage collection that can cause arbitrary latencies.
Therefore all the Operating Systems use C language extensively instead of Java.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/comparison-on-using-java-for-an-operating-system-instead-of-c/
✍
Write a Testimonial

Read-Copy Update (RCU)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Read-Copy Update (RCU) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
A Lock Primitive which is widely used in Linux kernel is read-copy update (RCU) lock. It is a synchronization mechanism which was added to the Linux Kernel in October 2002. It has achieved improvements by
enabling reads to occur simultaneously with updates. It supports concurrency between multiple readers and a single updater. There are no overheads in the RCU’s read-side primitive. It is one of the safest data structure
designed to operate safely during simultaneous access because it uses cache line and memory efficiently.
Read-Copy Update (CPU) locks have a lock-free read-critical section. RCU locks work for workloads where a writer is compatible with the lock-free readers. Preemption is not allowed in the read critical section.
Consider the following code:

filter_none
edit
close
play_arrow
link
brightness_4
code
struct Node
{
int key, val;
/* pointer to the "next" and "prev"(previous) node */
struct Node *next, *prev;
}
/* Searches for a given key node in a list li and,
returns the value corresponding to that key*/
int search(struct Node *li, int key)
{
int ret= -1;
preempt_disable();
/* Disabling the preempt */
while(li!=NULL)

{
/* The required key is found */
if(li->key == key)
{
/* Assigning "ret" the value of that particular key */
ret = li->val;
break;
}
/* moving on to the next list value */
li=li->next;
}
/* Enabling the interrupt which was disabled earlier */
preempt_enable();
/* Return the value of the key that is found */
return ret;
}
/* Deletes a given node */
void delete (struct Node *node)
{
/* Take a spin lock on the given node */
spin_lock(&lock);
node->next->prev = node->prev;
node->prev->next = node->next;
/* Unlock the lock on the node,
because the deletion has been done */
spin_unlock(&lock);
free(node);
}

chevron_right
filter_none
In the above code, the delete update of next (i.e., node->prev->next = node->next) is atomic, because the store operations are atomic. If a delete is executing concurrently, the search routine will either see the new list or
the old list depending on whether next is updated or not. Also, the delete operation can execute concurrently with the search operation because the updates that matter for search is the update of the next field of a linked
list node.
In read-copy update scheme, a free operation is delayed until the updater is 100 % sure that the other threads do not have a reference to the object that is going to be deleted. As preemption is not allowed in the read-critical
section, invocation of the RCU schedule suggests that the particular core is not executing a read-critical section.
In the RCU scheme, a writer calls rcu_free instead of free .rcu_free ensures that all the readers have exited atleast once from their critical section after the rcu_free is called. This check ensures that the other threads do not
have an active reference to the object which is going to be deleted.
This is done, when rcu_free calls wait_for_rcu which forces a quiescent state on every core.wait_for_rcu schedules the current thread on all cores to enforce a safe point for a pending free.
Advantages and Disadvantages:
RCU locks cannot be used for a data structure like Tree because a Tree search may depend on multiple updates that cannot be done atomically.
One disadvantage of read-copy update locks is that it requires disabling of preemption due to which they cannot be used in user-mode.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/read-copy-update-rcu/
✍
Write a Testimonial

Difference between Blu-ray and DVD
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Blu-ray and DVD - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Blu-ray:
Blu-ray could be a high definition storage device which might hold a bigger space for storing as compared to different optical storage devices. it had been principally devised to store high definition videos, and this wasn’t
doable within the previous DVD technology. Blu-ray will store the large quantity of information as compared to different disk formats, and therefore the reason behind this can be the less gap between the information
layer on the disk and therefore the optical device.
It generates tighter focus, less distortion Associate in smaller pits (and tracks) as an outcome of putting the optical device nearer to the disk. the utmost space for storing of Blu-ray is twenty five GB on one facet. The Bluray name is originated by adding blue and optical “ray” and this as a result of it uses blue-violet optical device rather than a red optical device. because the blue laser’s wavelength is shorter, which helps for achieving the
higher bit density.
DVD:
DVD stands for Digital Versatile Disk provides another for the videotape utilized in tape recorder (Video container Recorder) and fixed storage utilized in computer because the videodisc will acquire seven times larger
quantity of the info relative to CD.

It renders videos with wonderful image quality and random access. A videodisc is constructed from a similar material because the CD however the method and therefore the layers area unit completely different, it’s used
from each of the edges like 2 CDs area unit projected along. In DVD, RS-PC and EFMplus are used as the error correction codes.
Let’s see that the difference b/w DVD and Blu-ray:
S.NO
1.
2.
3.
4.
5.
6.
7.

Blu-ray

DVD

The single layer size of Blu-ray is 25 GB.
While the single layer size of DVD is 4.7 GB.
In Blu-ray, the recording or metal layer is situated closer to the reading mechanism’s objective lens. While in DVD, the recording or metal layer is situated in middle of disk.
There are double layers of pits in blu-ray.
While there are also double layers of pits in DVD.
In blu-ray, there is 0.30 micrometer space between the spiral’s loops.
While in DVD, there is 0.74 micrometer space between the spiral’s loops.
It holds the 0.15 micrometer space between the pits.
While it holds the 0.4 micrometer space between the pits.
While In DVD, RS-PC and EFMplus are used as the error correction codes.
In blu-ray, picket code is used as the correction code.
In Blu-ray, the wavelength of laser in 405 nm.

While in DVD, the wavelength of laser is 650 nm.

S.NO
Blu-ray
8.
In Blu-ray, the data transfer rate is 36 Mb/sec.
9.
The double layer size of Blu-ray is 50 GB.
10. The numeric aperture of blu-ray is 0.8.
11. The thickness of blu-ray is 1.1 mm.

DVD
While in DVD, the data transfer rate is 11 Mb/sec.
While the double layer size of DVD is 8.5 GB.
While the numeric aperture of DVD is 0.6.
While the thickness of DVD is 0.6 mm.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-blu-ray-and-dvd/
✍
Write a Testimonial

Difference between CLI and GUI
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between CLI and GUI - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
CLI is that the word form used for Command Line Interface. CLI permits users to put in writing commands associate degree exceedingly in terminal or console window to interact with an operating system. CLI is a
platform or medium wherever users answer a visible prompt by writing a command and get the response from system, for this users have to be compelled to kind command or train of command for performing the task. CLI
is suitable for the pricey computing wherever input exactitude is that the priority.

GUI stands for Graphical User Interface. GUI permits users to use the graphics to interact with an operating system. In graphical user interface, menus are provided such as : windows, scrollbars, buttons, wizards,
painting pictures, alternative icons etc. It’s intuitive, simple to find out and reduces psychological feature load. In GUI, the information is shown or presented to the user in any form such as: plain text, videos, images, etc.

Let’s see that the difference between GUI and CLI:
S.NO
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.

CLI

GUI

CLI is difficult to use.
It consumes low memory.
In CLI we can obtain high precision.
CLI is faster than GUI.
CLI operating system needs only keyboard.
CLI’s appearance can not be modified or changed.
In CLI, input is entered only at command prompt.
In CLI, the information is shown or presented to the user in plain text and files.
In CLI, there are no menus provided.
There are no graphics in CLI.
CLR do not use any pointing devices.
In CLI, spelling mistakes and typing errors are not avoided.

Whereas it is easy to use.
While consumes more memory.
While in it, low precision is obtained.
The speed of GUI is slower than CLI.
While GUI operating system need both mouse and keyboard.
While it’s appearance can be modified or changed.
While in GUI, input can be entered anywhere on the screen.
While in GUI, the information is shown or presented to the user in any form such as: plain text, videos, images, etc.
While in GUI, menus are provided.
While in GUI, graphics are used.
While it uses pointing devices for selecting and choosing items.
Whereas in GUI, spelling mistakes and typing errors are avoided.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-cli-and-gui/
✍
Write a Testimonial

Difference between PROM and EPROM
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between PROM and EPROM - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
PROM stands for Programmable Read Only Memory is the type of ROM is written only. It was meant to fulfil the requirement of a group of ROMs which may contain a selected memory content. It’s memory is written
just the once and programmed electrically by the user at the time or when the initial chip fabrication. the required content file is equipped by the user and inserted within the machine referred to as storage coder. There
exist a fuse at every programmable association and it’s blown once the association isn’t required.
EPROM stands for Erasable Programmable Read Only Memory is also the type of ROM is read and written optically. To write associate EPROM, its storage cells should stay within the same initial state. EPROM
provides reduced storage permanency as compared to PROM as a result of the EPROM is receptive to radiation and electrical noise. in the construction of EPROM, MOS transistors are used.

The main distinction between PROM and EPROM is that, PROM may be programmed just the once implies that it may be written only 1 time whereas EPROM is erasable; therefore it may be reprogrammed.
Let’s see that the difference between PROM and EPROM:
S.NO
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.

PROM

EPROM

PROM is not reusable.
While EPROM is reusable multiple times.
PROM is inexpensive.
While it is costlier than PROM.
The processes of PROMS is irreversible, means it’s memory is permanent.
Whereas EPROM’s processes can be reversed.
The storage endurance of PROM is high.
While EPROM’s storage endurance of PROM is less than PROM.
PROM is totally sheathed during a plastic cowl.
While EPROM is boxed in during a rock crystal window so the ultraviolet radiation rays will transfer through it.
PROM is the type of ROM is written only.
While EPROM is also the type of ROM is read and written optically.
If there’s miscalculation or error or bug while writing on PROM, it becomes unusable. Whereas if there’s miscalculation or error or bug while writing on EPROM, it will still be used once more.
PROM is the older version of EPROM.
While EPROM is modern version of PROM.
PROM is better than EPROM in terms of flexibility and scale.
While EPROM have less flexibility and scalability.
In PROM, bipolar transistor is used.
While in EPROM, MOS transistor is used.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-prom-and-eprom/
✍
Write a Testimonial

Difference between RPC and RMI
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between RPC and RMI - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
RPC stands for Remote Procedure Call which supports procedural programming. Tt’s almost like IPC mechanism wherever the software permits the processes to manage shared information Associated with an
environment wherever completely different processes area unit death penalty on separate systems and essentially need message-based communication.

The above diagram shows the working steps in PRC implementation.

RMI stands for Remote Method Invocation, is a similar to PRC but it supports object oriented programming which is the java’s feature. A thread is allowable to decision the strategy on a foreign object. In RMI, objects
are passed as parameter rather than ordinary data.

This diagram shows the client-server architecture of RMI protocol.
RPC and RMI both are similar but the basic difference between RPC and RMI is that, RPC supports procedural programming on the other hand, RMI supports object-oriented programming.
Let’s see that the difference between RPC and RMI:
S.NO
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.

RPC

RMI

RPC is a library and OS dependent platform.
Whereas it is a java platform.
RPC supports procedural programming.
RMI supports object oriented programming.
RPC is less efficient in comparison of RMI.
While RMI is more efficient than RPC.
RPC creates more overhead.
While it creates less overhead than RPC.
The parameters which are passed in RPC are ordinary or nomal data. While in RMI, objects are passed as parameter.
RPC is the older version of RMI.
While it is the successor version of RPC.
There is high Provision of ease of programming in RPC.
While there is low Provision of ease of programming in RMI.
RPC does not provide any security.
While it provides client level security.
It’s development cost is huge.
While it’s development cost is fair or reasonable.
There is a huge problem of versioning in RPC.
While there is possible versioning using RDMI.
There is multiple codes are needed for simple application in RPC. While there is multiple codes are not needed for simple application in RMI.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-rpc-and-rmi/
✍
Write a Testimonial

Difference between CD and DVD
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between CD and DVD - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
CD stands for Compact Disc was the primary step towards the thought of digital coding of the info. It uses a novel methodology of coding within which a 14-bit code indicates a computer memory unit and this coding
technique conjointly helps in error detection. it had been an acceptable replacement for the memory device because it offered the low-priced answer for storing a big quantity of knowledge.
DVD stands for Digital Versatile Disk provides another for the videotape utilized in tape recorder (Video container Recorder) and fixed storage utilized in computer because the videodisc will acquire seven times larger
quantity of the info relative to CD. It renders videos with wonderful image quality and random access. A videodisc is constructed from a similar material because the CD however the method and therefore the layers area
unit completely different, it’s used from each of the edges like 2 CDs area unit projected along. In DVD, RS-PC and EFMplus are used as the error correction codes.
There are few differences between CD and DVD, which are given below:

S.NO
1.
2.

CD
The acquire size of CD is 700 MB.
In CD, the recording or metal layer is situated on the top of disk.

DVD
While the acquire size of DVD is 4.7 GB to 17 GB.
While in DVD, the recording or metal layer is situated in middle of disk.

S.NO
3.
There is only single pit layer in CD.
4.
5.
6.
7.
8.
9.
10.
11.

CD

DVD
While there are double layers of pits in DVD.

In CD, there is 1.6 micrometer space between the spiral’s loops.
While in DVD, there is 0.74 micrometer space between the spiral’s loops.
A CD holds the 0.834 micrometer space between the pits.
While it holds the 0.4 micrometer space between the pits.
In CD, CIRC and EFMP are used as the error correction codes.
While In DVD, RS-PC and EFMplus are used as the error correction codes.
There can cause the damage in metal layer after the Removal of the adhesive labels, in CD. While there is caused the imbalance in the spin after the Removal of the adhesive labels, in CD.
In CD, the data transfer rate is 1.4 MB to 1.6 MB/sec.
While in DVD, the data transfer rate is 11 MB/sec.
The channel bit length in Cd is 300 nanometer.
While the channel bit length in DVD is 113 nanometer which is less than CD’s channel bit length.
The numeric aperture of CD is 0.45.
While the numeric aperture of DVD is 0.6.
The thickness of CD is 1.2 mm.
While the thickness of DVD is 0.6 mm.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-cd-and-dvd/
✍
Write a Testimonial

Difference between Time Sharing OS and Real-Time OS
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Time Sharing OS and Real-Time OS - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Types of Operating Systems
Time sharing operating system is usually works on the concurrent execution ideas wherever multiple jobs area unit executes at identical (same) time through switch them oftentimes. In this operating system Switching
method/function is available. This switching is incredibly quick in order that the users will move with every program whereas it’s running while not knowing that, the system is being shared.
Real Time operating system, computation tasks are emphasized before its nominative point. Real time operating system is incredibly helpful for the temporal order applications, in different words wherever tasks ought to
be accomplished inside a definite deadline. The time period in operation systems not solely need correct results however conjointly the timely results, which implies beside the correctness of the results it should be created
in an exceedingly sure deadline otherwise the system can fail.

The main difference between time sharing and the real-time operating system is that, In time sharing OS, the response is provided to the user within a second. While in real time OS, the response is provided to the user
within time constraint.
Let’s see that the difference between Time Sharing and Real-Time Operating System:
S.NO
1.
2.
3.
4.
5.
6.

Time Sharing Operating System

Real-Time Operating System

In time sharing operating system, quick response is emphasized for a request. While in real time operating system, computation tasks are emphasized before its nominative point.
In this operating system Switching method/function is available.
While in this operating system Switching method/function is not available.
In this operating system any modification in the program can be possible.
While in this modification does not take place.
In this OS, computer resources are shared to the external.
But in this OS, computer resources are not shared to the external.
It deals with more than processes or applications simultaneously.
Whereas it deals with only one process or application at a time.
In this OS, the response is provided to the user within a second.
While in real time OS, the response is provided to the user within time constraint.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-time-sharing-os-and-real-time-os/
✍
Write a Testimonial

Difference between Interrupt and Polling
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Interrupt and Polling - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Interrupt:
Interrupt is a hardware mechanism in which, the device notices the CPU that it requires its attention. Interrupt can take place at any time. So when CPU gets an interrupt signal trough the indication interrupt-request line,
CPU stops the current process and respond to the interrupt by passing the control to interrupt handler which services device.
Polling:
In polling is not a hardware mechanism, its a protocol in which CPU steadily checks whether the device needs attention. Wherever device tells process unit that it desires hardware processing, in polling process unit keeps
asking the I/O device whether or not it desires CPU processing. The CPU ceaselessly check every and each device hooked up thereto for sleuthing whether or not any device desires hardware attention.
Each device features a command-ready bit that indicates the standing of that device, i.e., whether or not it’s some command to be dead by hardware or not. If command bit is ready one, then it’s some command to be dead
else if the bit is zero, then it’s no commands.

Let’s see that the difference between interrupt and polling:

S.NO
1.
2.
3.
4.
5.
6.

Interrupt

Polling

In interrupt, the device notices the CPU that it requires its attention.
An interrupt is not a protocol, its a hardware mechanism.
In interrupt, the device is serviced by interrupt handler.
Interrupt can take place at any time.
In interrupt, interrupt request line is used as indication for indicating that device
requires servicing.
In interrupts, processor is simply disturbed once any device interrupts it.

Whereas, in polling, CPU steadily checks whether the device needs attention.
Whereas it isn’t a hardware mechanism, its a protocol.
While in polling, the device is serviced by CPU.
Whereas CPU steadily ballots the device at regular or proper interval.
While in polling, Command ready bit is used as indication for indicating that device requires servicing.
On the opposite hand, in polling, processor waste countless processor cycles by repeatedly checking the command-ready little
bit of each device.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-interrupt-and-polling/
✍
Write a Testimonial

Difference between Linux and Windows
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Linux and Windows - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Linux:
Linux could be a free and open supply OS supported operating system standards. It provides programming interface still as programme compatible with operating system primarily based systems and provides giant
selection applications. A UNIX operating system system additionally contains several severally developed parts, leading to UNIX operating system that is totally compatible and free from proprietary code.
Windows:
Windows may be a commissioned OS within which ASCII text file is inaccessible. it’s designed for the people with the angle of getting no programming information and for business and alternative industrial users. it’s
terribly straightforward and simple to use.
The distinction between Linux and Windows package is that Linux is completely freed from price whereas windows is marketable package and is expensive. Associate operating system could be a program meant to
regulate the pc or computer hardware Associate behave as an treater between user and hardware.

Linux is a open supply package wherever users will access the ASCII text file and might improve the code victimisation the system. On the opposite hand, in windows, users can’t access ASCII text file, and it’s a
authorized OS.
Let’s see that the difference between Linux and windows:
S.NO
1.
2.
3.
4.
5.
6.
7.

Linux

Windows

Linux is a open source operating system.
While windows are the not the open source operating system.
Linux is free of cost.
While it is costly.
It’s file name case-sensitive.
While it’s file name is case-insensitive.
In linux, monolithic kernel is used.
While in this, micro kernel is used.
Linux is more efficient in comparison of windows.
While windows are less efficient.
There is forward slash is used for Separating the directories. While there is back slash is used for Separating the directories.
Linux provides more security than windows.
While it provides less security than linux.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-linux-and-windows/
✍
Write a Testimonial

Multilevel Paging in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Multilevel Paging in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Paging
Multilevel Paging is a paging scheme which consist of two or more levels of page tables in a hierarchical manner. It is also known as hierarchical paging. The entries of the level 1 page table are pointers to a level 2 page
table and entries of the level 2 page tables are pointers to a level 3 page table and so on. The entries of the last level page table are stores actual frame information. Level 1 contain single page table and address of that table
is stored in PTBR (Page Table Base Register).
Virtual address:

In multilevel paging whatever may be levels of paging all the page tables will be stored in main memory.So it requires more than one memory access to get the physical address of page frame. One access for each level
needed. Each page table entry except the last level page table entry contains base address of the next level page table.

Reference to actual page frame:
Reference to PTE in level 1 page table = PTBR value + Level 1 offset present in virtual address.
Reference to PTE in level 2 page table = Base address (present in Level 1 PTE) + Level 2 offset (present in VA).
Reference to PTE in level 3 page table= Base address (present in Level 2 PTE) + Level 3 offset (present in VA).
Actual page frame address = PTE (present in level 3).
Generally the page table size will be equal to the size of page.
Assumptions:
Byte addressable memory, and n is the number of bits used to represent virtual address.
Important formula:
Number of entries in page table: ​
= (virtual address space size) / (page size) ​
= Number of pages​
​
Virtual address space size: ​
= 2n B ​
​
Size of page table: ​
<>= (number of entries in page table)*(size of PTE)

If page table size > desired size then create 1 more level.
Disadvantage:
Extra memory references to access address translation tables can slow programs down by a factor of two or more. Use translation look aside buffer (TLB) to speed up address translation by storing page table entries.
Example:
Q.Consider a virtual memory system with physical memory of 8GB, a page size of 8KB and 46 bit virtual address. Assume every page table exactly fits into a single page. If page table entry size is 4B then how many
levels of page tables would be required.
Explanation:

Page size = 8KB = 213 B​
Virtual address space size = 246 B​
PTE = 4B = 2 2 B​
​
Number of pages or number of entries in page table, ​
= (virtual address space size) / (page size) ​
= 246B/246 B ​
= 233

Size of page table,
= (number of entries in page table)*(size of PTE) ​
= 233*2 2 B ​
= 235 B

To create one more level,
Size of page table > page size​
​
Number of page tables in last level, ​
= 235 B / 213 B ​
= 222

Base address of these tables are stored in page table [second last level].

Size of page table [second last level] ​
= 222*2 2 B ​
= 224B

To create one more level,
Size of page table [second last level] > page size
Number of page tables in second last level ​
= 224B/213 B ​
= 211

Base address of these tables are stored in page table [third last level]
Size of page table [third last level] ​
= 211*2 2 B ​
= 213 B ​
= page size

∴ 3 levels are required.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/multilevel-paging-in-operating-system/
✍
Write a Testimonial

Difference between DOS and Windows
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between DOS and Windows - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
DOS and Windows square measure the various varieties of Operating Systems.
DOS stands for Disk Operating System. It is a smaller amount probably employed in the current state of affairs whereas windows may be a wide used in operation system. It consumes less memory and power than
windows.
Window has no full form but it is widely used operating system than DOS operating system. It consumes more memory and power than DOS operating system.

DOS and Windows square measure principally differentiated by the actual fact that DOS may be a single tasking, single user, interface primarily based OS developed within the year of 1979. On the opposite hand, all the
windows version square measure multitasking, multiuser and graphical user interface primarily based OS.

Let’s see the difference between DOS and Windows:
S.NO
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.

DOS

WINDOW

DOS is single tasking operating system.
While windows are multitasking operating systems.
It consumes low power.
While windows consume high power.
It consumes less memory in comparison of windows.
While it consumes more memory.
DOS does not support networking.
While window supports networking.
DOS is complex in terms of using.
Whereas it is simple for using.
DOS does not share time.
While window can share time.
DOS is a command line operating system.
Whereas windows are the graphical operating systems.
DOS operating system is less preferred than windows.
While windows are more preferred by the users in comparison of DOS.
In DOS operating system multimedia is not supported such as: Games, movies,songs etc. While windows support multimedia such as: Games, movies,songs etc.
In DOS operation systems, operation are performed speedily than windows OS.
While in windows OS, operation are performed slowly than DOS OS.
There is only one window opened at a time in DOS.
While in windows, multiple windows can be opened at a time.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-dos-and-windows/
✍
Write a Testimonial

Difference between Mainframe and Minicomputer
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Mainframe and Minicomputer - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Classification of Computers
Mainframe Computers and Minicomputers are the categories of a laptop wherever mainframe computers give rather more options than minicomputer and high capability for memory and process speed.
At the beginning, Mainframe Computers were made to produce responsibleness for dealing process and are primarily used as servers.
Whereas, Minicomputers are mid-size, tiny computers which individuals will use either individually or as a high-end device likewise.
Let’s see the difference between Mainframe Computer and Minicomputer:

S.NO
1.
2.
3.
4.
5.
6.

Mainframe

Minicomputer

In mainframe computer, large size of disk is used.
While in minicomputer, small size of disk is used.
Mainframe computers have large memory storage.
While minicomputers have small or less memory storage than mainframe computer.
The processing speed of mainframe computer is faster than minicomputer. While the processing speed of minicomputer is slower than mainframe computer.
Mainframe computer is costlier than minicomputers.
Whereas supercomputers’s cost is less or it is Inexpensive.
The first microcomputer was invented by the team leader Bill Pentz .
The first successful mainframe computer is invented by IBM.
Mainframe computers support thousand or millions of users simultaneously. Whereas minicomputers support hundreds of users at a time.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-mainframe-and-minicomputer/
✍
Write a Testimonial

Chandy-Misra-Haas’s Distributed Deadlock Detection Algorithm
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Chandy-Misra-Haas's Distributed Deadlock Detection Algorithm - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Chandy-Misra-Haas’s distributed deadlock detection algorithm is an edge chasing algorithm to detect deadlock in distributed systems.
In edge chasing algorithm, a special message called probe is used in deadlock detection. A probe is a triplet (i, j, k) which denotes that process Pi has initiated the deadlock detection and the message is being sent by the
home site of process Pj to the home site of process Pk .
The probe message circulates along the edges of WFG to detect a cycle. When a blocked process receives the probe message, it forwards the probe message along its outgoing edges in WFG. A process Pi declares the
deadlock if probe messages initiated by process P i returns to itself.

Other terminologies used in the algorithm:
1. Dependent process:
A process Pi is said to be dependent on some other process P j, if there exists a sequence of processes Pi, Pi1, Pi2, Pi3…, Pim, Pj such that in the sequence, each process except Pj is blocked and each process except Pi
holds a resource for which previous process in the sequence is waiting.
2. Locally dependent process:

A process Pi is said to be locally dependent on some other process P j if the process P i is dependent on process Pj and both are at same site.
Data structures:
A boolean array, dependenti. Initially, dependenti[j] is false for all value of i and j. dependenti[j] is true if process P j is dependent on process P i.
Algorithm:
Process of sending probe:
1. If process P i is locally dependent on itself then declare a deadlock.
2. Else for all P j and Pk check following condition:
(a). Process Pi is locally dependent on process P j
(b). Process Pj is waiting on process P k
(c). Process Pj and process Pk are on different sites.
If all of the above conditions are true, send probe (i, j, k) to the home site of process Pk .
On the receipt of probe (i, j, k) at home site of process Pk:
1. Process Pk checks the following conditions:

(a). Process Pk is blocked.
(b). dependentk [i] is false.
(c). Process Pk has not replied to all requests of process Pj
/li>
If all of the above conditions are found to be true then:
1. Set dependentk [i] to true.
2. Now, If k == i then, declare the Pi is deadlocked.
3. Else for all Pm and Pn check following conditions:
(a). Process Pk is locally dependent on process P m and
(b). Process Pm is waiting upon process P n and
(c). Process Pm and process Pn are on different sites.
4. Send probe (i, m, n) to the home site of process Pn if above conditions satisfy.
Thus, the probe message travels along the edges of transaction wait-for (TWF) graph and when the probe message returns to its initiating process then it is said that deadlock has been detected.
Performance:
Algorithm requires at most exchange of m(n-1)/2 messages to detect deadlock. Here, m is number of processes and n is the number of sites.
The delay in detecting the deadlock is O(n).
Advantages:
There is no need for special data structure. A probe message, which is very small and involves only 3 integers and a two dimensional boolean array dependent is used in the deadlock detection process.
At each site, only a little computation is required and overhead is also low
Unlike other deadlock detection algorithm, there is no need to construct any graph or pass nor to pass graph information to other sites in this algoirthm.
Algorithm does not report any false deadlock (also called phantom deadlock).
Disadvantages:
The main disadvantage of a distributed detection algorithms is that all sites may not aware of the processes involved in the deadlock this makes resolution difficult. Also, proof of correction of the algorithm is difficult.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/chandy-misra-haass-distributed-deadlock-detection-algorithm/
✍
Write a Testimonial

Analyzing BufferOverflow with GDB
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Analyzing BufferOverflow with GDB - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Pre-requisite: GDB (Step by Step Introduction)
A BufferOverflow often occurs when the content inside the defined variable is copied to another variable without doing Bound Checks or considering the size of the buffer. Let’s analyze buffer overflow with the help
GNU Debugger (GDB) which is inbuilt every Linux system.
The motive of this exercise is to get comfortable with debugging code and understand how does buffer overflow works in action.

filter_none
edit
close
play_arrow
link
brightness_4
code
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
int main(int argc, char** argv)

{
volatile int cantoverflowme;
char buffer[64];
cantoverflowme = 0;
gets(buffer);
if (cantoverflowme != 0) {
printf("You OVERFLOW'ed Me\n");
}
else {
printf("Can't Overflow Me\n");
}
}

chevron_right
filter_none
Step 1
Let’s compile this code with the following flags :
gcc overflow.c -o overflow

-fno-stack-protector -z execstack -no-pie

The above code is going to create a compiled binary that disables various stack protections
-z execstack : Disables Executable Stack​
-fno-stack-protector : Disables Stack Canaries​
-no-pie : Disables Position Independent Executables

Step 2
Now that stack protections are disabled we can load the code in GDB by typing
gdb ./overflow

Step 3
Once the code is open we can look at the functions that are inside the binary by using typing
info functions

We can see there’s a gets call which is being used which is vulnerable in nature as it doesn’t do any bound checks.
Step 4
Let’s type
disas main

and disassemble the main function

Step 5
Let’s put a breakpoint by typing
b * main+39

so that we can analyze the content of stack when the program hits the breakpoint.
Step 6
Type
r

to run the code and input any number of A’s as we already know from the code above.
Let’s input 63 A’s and 78 A’s and see the change in the result.
Step 7
You can use python code to print A’s by typing after leaving the GDB.
python -c "print 'A' * 63"

Step 8
Now that we have 63 A’s let’s run the code and paste it when it ask’s us for the input.

Let’s try the whole process again and this time let’s input any number of A’s let’s say 78.
A cool way to do this can be
python -c "print 'A' * 78" | ./overflow

As we can see once the overflow occurs it changes the variable because of memory being leaked on the stack and changing values of variables
Step 9
Let’s check the stack which it over writes, so we have to set a break point at
main+39

then type
r

and then we can type
x/20s $rsp
x : eXamine​
20s : 20 values in string​
$rsp : for register RSP (Stack Pointer)

Hence we can see how 78 A’s are written on the stack and are overflowing the memory.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/analyzing-bufferoverflow-with-gdb/
✍
Write a Testimonial

Difference between Process and Thread
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Process and Thread - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x

×​
Suggest a Topic

Select a Category​
menu
Process:
Process means any program is in execution. Process control block controls the operation of any process. Process control block contains the information about processes for example: Process priority, process id, process
state, CPU, register etc. A process can creates other processes which are known as Child Processes. Process takes more time to terminate and it is isolated means it does not share memory with any other process.
Thread:
Thread is the segment of a process means a process can have multiple threads and these multiple threads are contained within a process. A thread have 3 states: running, ready, and blocked.
Thread takes less time to terminate as compared to process and like process threads do not isolate.

Difference between Process and Thread:
S.NO
Process
Thread
1.
Process means any program is in execution.
Thread means segment of a process.
2.
Process takes more time to terminate.
Thread takes less time to terminate.
3.
It takes more time for creation.
It takes less time for creation.
4.
It also takes more time for context switching.
It takes less time for context switching.
5.
Process is less efficient in term of communication. Thread is more efficient in term of communication.
6.
Process consume more resources.
Thread consume less resources.
7.
Process is isolated.
Threads share memory.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-process-and-thread/
✍
Write a Testimonial

Difference between System Software and Application Software
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between System Software and Application Software - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Software Concepts
System Software:
System Software is the type of software which is the interface between application software and system. Low level languages are used to write the system software. System Software maintain the system resources and give
the path for application software to run. An important thing is that without system software, system can not run. It is a general purpose software.
Application Software:
Application Software is he type of software which runs as per user request. It runs on the platform which is provide by system software. High level languages are used to write the application software. Its a specific
purpose software.
The main difference between System Software and Application Software is that without system software, system can not run on the other hand without application software, system always runs.

S.NO
System Software
Application Software
1.
System Software maintain the system resources and give the path for application software to run. Application software is built for specific tasks.
2.
Low level languages are used to write the system software.
While high level languages are used to write the application software.
3.
Its a general purpose software.
While its a specific purpose software.
4.
Without system software, system can’t run.
While without application software system always runs.
5.
System software runs when system is turned on and stop when system is turned off.
While application software runs as per the user’s request.
6.
Example of system software are operating system, etc.
Example of application software are Photoshop, VLC player etc.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-system-software-and-application-software/
✍
Write a Testimonial

Difference between Multiprocessing and Multithreading
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Multiprocessing and Multithreading - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Both Multiprocessing and Multithreading are used to increase the computing power of a system.
Multiprocessing:
Multiprocessing is a system that has more than one or two processors. In Multiprocessing, CPUs are added for increasing computing speed of the system. Because of Multiprocessing, There are many processes are
executed simultaneously. Multiprocessing are classified into two categories:
1. Symmetric Multiprocessing​
2. Asymmetric Multiprocessing

Multithreading:
Multithreading is a system in which multiple threads are created of a process for increasing the computing speed of the system. In multithreading, many threads of a process are executed simultaneously and process
creation in multithreading is done according to economical.

Difference between Multiprocessing and Multithreading:
S.NO
Multiprocessing
Multithreading
1.
In Multiprocessing, CPUs are added for increasing computing power. While In Multithreading, many threads are created of a single process for increasing computing power.
2.
In Multiprocessing, Many processes are executed simultaneously.
While in multithreading, many threads of a process are executed simultaneously.
3.
Multiprocessing are classified into Symmetric and Asymmetric.
While Multithreading is not classified in any categories.
4.
In Multiprocessing, Process creation is a time-consuming process.
While in Multithreading, process creation is according to economical.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-multiprocessing-and-multithreading/
✍
Write a Testimonial

Difference between Virtual memory and Cache memory
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Virtual memory and Cache memory - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Cache Memory:
Cache memory increases the accessing speed of CPU. It is not a technique but a memory unit i.e a storage device. In cache memory, recently used data is copied. Whenever the program is ready to be executed, it is fetched
from main memory and then copied to the cache memory. But, if its copy is already present in the cache memory then the program is directly executed.

Virtual Memory:
Virtual Memory increases the capacity of main memory. Virtual memory is not a storage unit, its a technique. In virtual memory, even such programs which have a larger size than the main memory are allowed to be
executed.

Difference between Virtual memory and Cache memory:
S.NO
Virtual Memory
Cache Memory
1.
Virtual memory increases the capacity of main memory.
While cache memory increase the accessing speed of CPU.
2.
Virtual memory is not a memory unit, its a technique.
Cache memory is exactly a memory unit.
3.
The size of virtual memory is greater than the cache memory.
While the size of cache memory is less than the virtual memory.
4.
Operating System manages the Virtual memory.
On the other hand hardware manages the cache memory.
5.
In virtual memory, The program with size larger than the main memory are executed. While in cache memory, recently used data is copied into.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : shreyashagrawal

Source
https://www.geeksforgeeks.org/difference-between-virtual-memory-and-cache-memory/
✍
Write a Testimonial

Difference between Supercomputer and Mainframe Computer

​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Supercomputer and Mainframe Computer - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Generations of Computer
Supercomputers:
Supercomputers are the largest in size and the most costly computers in the world. Seymour Cray invent the Supercomputer. Super computers are used for large and complex mathematical computations. Supercomputer’s
speed is more than Mainframe computers so they can execute billions of instructions or floating point instructions within a second.
Mainframe Computers:
Mainframe Computers are less costly, small in size and slower in speed than the super computers. are used as a storage for large database and serve as a maximum number of users simultaneously. The first successful
mainframe computer is invented by IBM. Mainframe computer’s speed is comparatively less than Supercomputers. In this millions of instructions are executed simultaneously.
Let’s see the difference between Supercomputer and Mainframe Computer:
S.NO
1.
2.
3.
4.
5.
6.

Supercomputer

Mainframe Computer
While Mainframe computers are used as a storage for large database and serve as a maximum number of users
simultaneously.
Supercomputer’s speed is more than Mainframe computer. It can execute billions of instructions Mainframe computer’s speed is comparatively less than Supercomputers. In this millions of instructions are
within a second.
executed simultaneously.
Supercomputers are the largest computers.
Mainframe computers smaller than supercomputer in size.
Supercomputers are the most costly in the worlds.
Mainframe computers are less costly than supercomputers.
In the present, the supercomputers have Linux and their variants operating systems.
While Mainframe computers can have multiple operating systems simultaneously.
Seymour Cray invent the Supercomputer.
The first successful mainframe computer is invented by IBM.
Super computers are used for large and complex mathematical computations.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : Akanksha_Rai

Source
https://www.geeksforgeeks.org/difference-between-supercomputer-and-mainframe-computer/
✍
Write a Testimonial

Difference between Network OS and Distributed OS
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Network OS and Distributed OS - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Types of Operating Systems
In this topic we shall see the difference between Network Operating System and Distributed Operating System. The main difference between these two operating systems (Network Operating System and Distributed
Operating System) is that in network operating system each node or system can have its own operating system on the other hand in distribute operating system each node or system have same operating system which
is opposite to the network operating system.
The difference Between Network Operating System and Distributed Operating System are given below:

S.NO
Network Operating System
Distributed Operating System
1.
Network Operating System’s main objective is to provide the local services to remote client. Distributed Operating System’s main objective is to manage the hardware resources.
2.
In Network Operating System, Communication takes place on the basis of files.
In Distributed Operating System, Communication takes place on the basis of messages and share memory.
3.
Network Operating System is more scalable than Distributed Operating System.
Distributed Operating System is less scalable than Network Operating System.
4.
In Network Operating System, fault tolerance is less.
While in Distributed Operating System, fault tolerance is high.
5.
Rate of autonomy in Network Operating System is high.
While The rate of autonomy in Distributed Operating System is less.
6.
Ease of implementation in Network Operating System is also high.
While in Distributed Operating System Ease of implementation is less.
7.
In Network Operating System, All nodes can have different operating system.
While in Distributed Operating System, All nodes have same operating system.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-network-os-and-distributed-os/

✍
Write a Testimonial

Raymond’s tree based algorithm
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Raymond's tree based algorithm - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Mutual exclusion in distributed systems
Raymond’s tree based algorithm is lock based algorithm for mutual exclusion in a distributed system in which a site is allowed to enter the critical section if it has the token. In this algorithm, all sites are arranged as a
directed tree such that the edges of the tree are assigned direction towards the site that holds the token. Site which holds the token is also called root of the tree.
Data structure and Notations:
Every site Si keeps a FIFO queue, called request_q
This queue stores the requests of all neighbouring sites that have sent a request for the token to site Si but have not yet been sent token. A non-empty request_q at any site indicates that the site has sent a REQUEST
message to the root node.
Every site Si has a local variable, called holder
This variable points to an immediate neighbour node on a directed path to the root node.
Algorithm:

To enter Critical section:
When a site Si wants to enter the critical section it sends a REQUEST message to the node along the directed path to the root, provided it does not hold the token and its request_q is empty. After sending
REQUEST message it add its request to its request_q.
when a site Sj on the path to the root receives the REQUEST message of site Si, it places the REQUEST in its request_q and sends the REQUEST message along the directed path to the root, if it has not
sent any REQUEST message for previously received REQUEST message.
When the root site Sr( having token) receives the REQUEST message, it sends the token to the requesting site and sets its holder variable to point at that site.
On receiving the token, Site Sj deletes the top entry from its request_q and sends the token to the site indicated by deleted entry. holder variable of Site Sj is set to point at that site.
After deleting the topmost entry of the request_q, if it is still non-empty Site Sj sends a REQUEST message to the site indicated by holder variable in order to get token back.
To execute the critical section:
Site Si executes the critical section if it has received the token and its own entry is at the top of its request_q.
To release the critical section:
After finishing the execution of the critical section, site Si does the following:
If its request_q is non-empty, then it deletes the top msot entry from its <request_q and then it sends the token to that site indicated by deleted entry and also its holder variable is set to point at that site.
After performing above action, if the request_q is still non-empty, then site S i sends a REQUEST message to the site pointed by holder variable in order to get token back
Message Complexity:
In the worst case, the algorithm requires 2 * ( Longest path length of the tree ) message invocation per critical section entry. If all nodes are arranged in a straight line then the longest path length will be N – 1 and thus the
algorithm will require 2 * (N -1) message invocation for critical section entry. However, if all nodes generates equal number of REQUEST messages for the privilege, the algorithm will require approximately 2*N / 3
messages per critical section entry.
Drawbacks of Raymond’s tree based algorithm:
can cause starvation: Raymond’s algorithm uses greedy strategy as a site can executes the critical section on receiving the token even when its request is not on the top of the request queue. This affect the fairness
of the algorithm and thus can cause in starvation.
Performance:
Synchronization delay is (T * log N )/ 2, because the average distance between two sites to successively execute the criticsl section is (Log N)/2. Here T is maximum message transmission time.
In heavy load conditions, the synchronization delay become T beacuse a site executes the critical section every time the token is transferred.
The message complexity of this algorithm is O(log N) as the average distance between any two nodes in a tree with N nodes is log N
Deadlock is impossible

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/raymonds-tree-based-algorithm/
✍
Write a Testimonial

Difference between Long-Term and Short-Term Scheduler
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Long-Term and Short-Term Scheduler - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Process Scheduler
Long-Term Scheduler is also known as Job Scheduler. long-term scheduler regulates the programs which are selected to system for processing. In this the programs are setup in the queue and as per the requirement the
best one job is selected and it takes the processes from job pool. It regulates the Degree of Multi-programming (DOM).
Short-Term Scheduler is also known as CPU Scheduler. Short-Term Scheduler ensures which program is suitable or important for processing. It regulates the less DOM (Degree of Multi-programming).

Difference Between Long-Term and Short-Term Scheduler:

S.NO
Long-Term Scheduler
Short-Term Scheduler
1.
Long-Term Scheduler takes the process from job pool.
Short-Term Scheduler takes the process from ready queue.
2.
Long-Term Scheduler is also known as Job Scheduler.
Short-Term Scheduler is also known as CPU Scheduler.
3.
In Long-Term Scheduler, the programs are setup in the queue and as per the requirement the best one job is selected. In Short-Term Scheduler no such queue is exist.
4.
It regulates the more DOM (Degree of Multi-programming).
It regulates the less DOM (Degree of Multi-programming).
5.
It regulates the programs which are selected to system for processing.
It ensures which program is suitable or important for processing.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-long-term-and-short-term-scheduler/
✍
Write a Testimonial

Message Passing in Java
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Message Passing in Java - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
What is message passing and why it is used?
Message Passing in terms of computers is communication between processes. It is a form of communication used in object-oriented programming as well as parallel programming. Message passing in Java is like sending
an object i.e. message from one thread to another thread. It is used when threads do not have shared memory and are unable to share monitors or semaphores or any other shared variables to communicate. Suppose we
consider an example of producer and consumer, likewise what producer will produce, the consumer will be able to consume that only. We mostly use Queue to implement communication between threads.

In the example explained below, we will be using vector(queue) to store the messages, 7 at a time and after that producer will wait for the consumer until the queue is empty.

In Producer there are two synchronized methods putMessage() which will call form run() method of Producer and add message in Vector whereas getMessage() extracts the message from the queue for the consumer.
Using message passing simplifies the producer-consumer problem as they don’t have to reference each other directly but only communicate via a queue.
Example:
filter_none
edit
close
play_arrow
link
brightness_4
code
import java.util.Vector;
class Producer extends Thread {
// initialization of queue size
static final int MAX = 7;
private Vector messages = new Vector();
@Override
public void run()
{
try {
while (true) {
// producing a message to send to the consumer
putMessage();
// producer goes to sleep when the queue is full
sleep(1000);
}
}
catch (InterruptedException e) {
}
}
private synchronized void putMessage()
throws InterruptedException
{
// checks whether the queue is full or not
while (messages.size() == MAX)
// waits for the queue to get empty
wait();
// then again adds element or messages
messages.addElement(new java.util.Date().toString());
notify();
}
public synchronized String getMessage()
throws InterruptedException
{
notify();
while (messages.size() == 0)
wait();
String message = (String)messages.firstElement();
// extracts the message from the queue
messages.removeElement(message);
return message;
}
}
class Consumer extends Thread {
Producer producer;
Consumer(Producer p)
{
producer = p;
}
@Override
public void run()
{
try {
while (true) {
String message = producer.getMessage();
// sends a reply to producer got a message
System.out.println("Got message: " + message);
sleep(2000);
}
}
catch (InterruptedException e) {
}
}
public static void main(String args[])
{
Producer producer = new Producer();
producer.start();
new Consumer(producer).start();
}
}

chevron_right
filter_none
Output:
​
Got
Got
Got
Got
Got
Got
Got
Got

message:
message:
message:
message:
message:
message:
message:
message:

Thu
Thu
Thu
Thu
Thu
Thu
Thu
Thu

May
May
May
May
May
May
May
May

09
09
09
09
09
09
09
09

06:57:53
06:57:54
06:57:55
06:57:56
06:57:57
06:57:58
06:57:59
06:58:00

UTC
UTC
UTC
UTC
UTC
UTC
UTC
UTC

2019​
2019​
2019​
2019​
2019​
2019​
2019​
2019​

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : SiddharthPandey4

Source
https://www.geeksforgeeks.org/message-passing-in-java/

✍
Write a Testimonial

LRU Approximation (Second Chance Algorithm)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ LRU Approximation (Second Chance Algorithm) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
If you are not familiar with Least Recently Used Algorithm, check Least Recently Used Algorithm(Page Replacement)
This algorithm is a combination of using a queue, similar to FIFO (FIFO (Page Replacement)) alongside using an array to keep track of the bits used to give the queued page a “second chance”.
How does the algorithm work:
1. Set all the values of the bitref as False (Let it be the size of max capacity of queue).
2. Set an empty queue to have a max capacity.
3. Check if the queue is not full:
If the element is in the queue, set its corresponding bitref = 1.
If the element is not in the queue, then push it into the queue.
4. If the queue is full:
Find the first element of the queue that has its bitref = 0 and if any element in the front has bitref = 1, set it to 0. Rotate the queue until you find an element with bitref = 0.
Remove that element from the queue.
Push the current element from the input array into the queue.
Explanation:
The bits are set as usual in this case to one for the indices in the bitref until the queue is full.
Once the queue becomes full, according to FIFO Page Replacement Algorithm, we should get rid of the front of the queue (if the element is a fault/miss). But here we don’t do that.

Instead we first check its reference bit (aka bitref) if its 0 or 1 (False or True). If it is 0 (false), we pop it from the queue and push the waiting element into the queue. But if it is 1 (true), we then set its reference bit (bitref)
to 0 and move it to the back of the queue. We keep on doing this until we come across the front of the queue to have its front value’s reference bit (bitref) as 0 (false).
Then we follow the usual by removing it from the queue and pushing the waiting element into the queue.
What if the waiting element is in the queue already? We just set its reference bit (bitref) to 1 (true).

We now move all the values like 2, 4, 1 to the back until we encounter 3, whose bitref is 0. While moving 2, 4, 1 to the back, we set their bitref values to 0.

So now, the question how is this an approximation of LRU, when it clearly implements FIFO instead of LRU. Well, this works by giving a second chance to the front of the queue (which in FIFO‘s case would have been
popped and replaced). Here, the second chance is based on the fact that if the element is seen “recently” its reference bit (bitref) is set to 1 (true). If it was not seen recently, we would not have set its reference bit (bitref) to
1 (true) and thus removed it. Hence, this is why, it is an approximation and not LRU nor FIFO.
Below is the implementation of the above approach:
filter_none
edit
close
play_arrow
link
brightness_4
code
// C++ implementation of the approach
#include <bits/stdc++.h>
using namespace std;
// Function to find an element in the queue as
// std::find does not work for a queue
bool findQueue(queue<int> q, int x)
{
while (!q.empty()) {
if (x == q.front())
return true;
q.pop();
}
// Element not found
return false;
}
// Function to implement LRU Approximation
void LRU_Approximation(vector<int> t, int capacity)
{
int n = t.size();
queue<int> q;
// Capacity is the size of the queue
// hits is number of times page was
// found in cache and faults is the number
// of times the page was not found in the cache
int hits = 0, faults = 0;
// Array to keep track of bits set when a
// certain value is already in the queue
// Set bit --> 1, if its a hit
// find the index and set bitref[index] = 1
// Set bit --> 0, if its a fault, and the front
// of the queue has bitref[front] = 1, send front
// to back and set bitref[front] = 0
bool bitref[capacity] = { false };
// To find the first element that does not
// have the bitref set to true
int ptr = 0;
// To check if the queue is filled up or not
int count = 0;
for (int i = 0; i < t.size(); i++) {
if (!findQueue(q, t[i])) {
// Queue is not filled up to capacity
if (count < capacity) {
q.push(t[i]);
count++;
}
// Queue is filled up to capacity
else {
ptr = 0;
// Find the first value that has its
// bit set to 0
while (!q.empty()) {
// If the value has bit set to 1
// Set it to 0
if (bitref[ptr % capacity])
bitref[ptr % capacity] = !bitref[ptr % capacity];

// Found the bit value 0
else
break;
ptr++;
}
// If the queue was empty
if (q.empty()) {
q.pop();
q.push(t[i]);
}
// If queue was not empty
else {
int j = 0;
// Rotate the queue and set the front's
// bit value to 0 until the value where
// the bitref = 0
while (j < (ptr % capacity)) {
int t1 = q.front();
q.pop();
q.push(t1);
bool temp = bitref[0];
// Rotate the bitref array
for (int counter = 0; counter < capacity - 1; counter++)
bitref[counter] = bitref[counter + 1];
bitref[capacity - 1] = temp;
j++;
}
// Remove front element
// (the element with the bitref = 0)
q.pop();
// Push the element from the
// page array (next input)
q.push(t[i]);
}
}
faults++;
}
// If the input for the iteration was a hit
else {
queue<int> temp = q;
int counter = 0;
while (!q.empty()) {
if (q.front() == t[i])
bitref[counter] = true;
counter++;
q.pop();
}
q = temp;
hits++;
}
}
cout << "Hits: " << hits << "\nFaults: " << faults << '\n';
}
// Driver code
int main()
{
vector<int> t = { 2, 3, 2, 1, 5, 2, 4, 5, 3, 2, 5, 2 };
int capacity = 4;
LRU_Approximation(t, capacity);
return 0;
}

chevron_right
filter_none
Output:
​
Hits: 6​
Faults: 6​

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/lru-approximation-second-chance-algorithm/
✍
Write a Testimonial

Suzuki–Kasami Algorithm for Mutual Exclusion in Distributed System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Suzuki–Kasami Algorithm for Mutual Exclusion in Distributed System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite: Mutual exclusion in distributed systems
Suzuki–Kasami algorithm is a token-based algorithm for achieving mutual exclusion in distributed systems.This is modification of Ricart–Agrawala algorithm, a permission based (Non-token based) algorithm which
uses REQUEST and REPLY messages to ensure mutual exclusion.
In token-based algorithms, A site is allowed to enter its critical section if it possesses the unique token. Non-token based algorithms uses timestamp to order requests for the critical section where as sequence number is
used in token based algorithms.

Each requests for critical section contains a sequence number. This sequence number is used to distinguish old and current requests.
Data structure and Notations:
An array of integers RN[1…N]
A site Si keeps RNi[1…N], where RNi[j] is the largest sequence number received so far through REQUEST message from site Si.
An array of integer LN[1…N]

This array is used by the token.LN[J] is the sequence number of the request that is recently executed by site S j.
A queue Q
This data structure is used by the token to keep record of ID of sites waiting for the token
Algorithm:
To enter Critical section:
When a site Si wants to enter the critical section and it does not have the token then it increments its sequence number RNi[i] and sends a request message REQUEST(i, sn) to all other sites in order to request
the token.
Here sn is update value of RNi[i]
When a site Sj receives the request message REQUEST(i, sn) from site Si, it sets RNj[i] to maximum of RNj[i] and sn i.e RNj[i] = max(RNj[i], sn).
After updating RNj[i], Site Sj sends the token to site S i if it has token and RNj[i] = LN[i] + 1
To execute the critical section:
Site Si executes the critical section if it has acquired the token.
To release the critical section:
After finishing the execution Site Si exits the critical section and does following:
sets LN[i] = RNi[i] to indicate that its critical section request RNi[i] has been executed
For every site Sj, whose ID is not prsent in the token queue Q, it appends its ID to Q if RNi[j] = LN[j] + 1 to indicate that site Sj has an outstanding request.
After above updation, if the Queue Q is non-empty, it pops a site ID from the Q and sends the token to site indicated by popped ID.
If the queue Q is empty, it keeps the token
Message Complexity:
The algorithm requires 0 message invocation if the site already holds the idle token at the time of critical section request or maximum of N message per critical section execution. This N messages involves
(N – 1) request messages
1 reply message
Drawbacks of Suzuki–Kasami Algorithm:
Non-symmetric Algorithm: A site retains the token even if it does not have requested for critical section. According to definition of symmetric algorithm
“No site possesses the right to access its critical section when it has not been requested.”
Performance:
Synchronization delay is 0 and no message is needed if the site holds the idle token at the time of its request.
In case site does not holds the idle token, the maximum synchronization delay is equal to maximum message transmission time and a maximum of N message is required per critical section invocation.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/suzuki-kasami-algorithm-for-mutual-exclusion-in-distributed-system/
✍
Write a Testimonial

Algorithm for implementing Distributed Shared Memory
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Algorithm for implementing Distributed Shared Memory - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Distributed shared memory(DSM) system is a resource management component of distributed operating system that implements shared memory model in distributed system which have no physically shared memory.
The shared memory model provides a virtual address space which is shared by all nodes in a distributed system.
The central issues in implementing DSM are:
how to keep track of location of remote data.
how to overcome communication overheads and delays involved in execution of communication protocols in system for accessing remote data.
how to make shared data concurrently accessible at several nodes to improve performance.

1. Central Server Algorithm:
In this, a central server maintains all shared data. It services read requests from other nodes by returning the data items to them and write requests by updating the data and returning acknowledgement messages.
Time-out cam be used in case of failed acknowledgement while sequence number can be used to avoid duplicate write requests.
It is simpler to implement but the central server can become bottleneck and to overcome this shared data can be distributed among several servers. This distribution can be by address or by using a mapping function
to locate the appropriate server.

2. Migration Algorithm:
In contrast to central server algo where every data access request is forwarded to location of data while in this data is shipped to location of data access request which allows subsequent access to be performed
locally.
It allows only one node to access a shared data at a time and the whole block containing data item migrates instead of individual item requested.
It is susceptible to thrashing where pages frequently migrate between nodes while servicing only a few requests.
This algo provides an opportunity to integrate DSM with virtual memory provided by operating system at individual nodes.

3. Read Replication Algorithm:
This extends the migration algorithm by replicating data blocks and allowing multiple nodes to have read access or one node to have both read write access.
It improves system performance by allowing multiple nodes to access data concurrently.
The write operation in this is expensive as all copies of a shared block at various nodes will either have to invalidated or updated with the current value to maintain consistency of shared data block.
DSM must keep track of location of all copies of data blocks in this.

4. Full Replication Algorithm:
It is an extension of read replication algorithm which allows multiple nodes to have both read and write access to shared data blocks.
Since many nodes can write shared data concurrently, the access to shared data must be controlled to maintain it’s consistency.
To maintain consistency, it can use a gap free sequences in which all nodes wishing to modify shared data will send the modification to sequencer which will then assign a sequence number and multicast the
modification with sequence number to all nodes that have a copy of shared data item.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/algorithm-for-implementing-distributed-shared-memory/
✍
Write a Testimonial

Chandy–Lamport’s global state recording algorithm
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Chandy–Lamport's global state recording algorithm - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Each distributed system has a number of processes running on a number of different physical servers. These processes communicate with each other via communication channels using text messaging. These processes
neither have a shared memory nor a common physical clock, this makes the process of determining the instantaneous global state difficult.
A process could record it own local state at a given time but the messages that are in transit (on its way to be delivered) would not be included in the recorded state and hence the actual state of the system would be
incorrect after the time in transit message is delivered.
Chandy and Lamport were the first to propose a algorithm to capture consistent global state of a distributed system. The main idea behind proposed algorithm is that if we know that all message that hat have been sent by
one process have been received by another then we can record the global state of the system.

Any process in the distributed system can initiate this global state recording algorithm using a special message called MARKER. This marker traverse the distributed system across all communication channel and cause
each process to record its own state. In the end, the state of entire system (Global state) is recorded. This algorithm does not interfere with normal execution of processes.
Assumptions of the algorithm:
There are finite number of processes in the distributed system and they do not share memory and clocks.
There are finite number of communication channels and they are unidirectional and FIFO ordered.
There exists a communication path between any two processes in the system
On a channel, messages are received in the same order as they are sent.

Algorithm:
Marker sending rule for a process P :
Process p records its own local state
For each outgoing channel C from process P, P sends marker along C before sending any other messages along C.
(Note: Process Q will receive this marker on his incoming channel C1.)
Marker receiving rule for a process Q :
If process Q has not yet recorded its own local state then
Record the state of incoming channel C1 as an empty sequence or null.
After recording the state of incoming channel C1, process Q Follows the marker sending rule
If process Q has already recorded its state
Record the state of incoming channel C1 as the sequence of messages received along channel C1 after the state of Q was recorded and before Q received the marker along C1 from process P.
Need of taking snapshot or recording global state of the system:
Checkpointing: It helps in creating checkpoint. If somehow application fails, this checkpoint can be used re
Garbage collection: It can be used to remove objects that do not have any references.
It can be used in deadlock and termination detection.
It is also helpful in other debugging.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/chandy-lamports-global-state-recording-algorithm/
✍
Write a Testimonial

Huang’s Termination detection algorithm
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Huang's Termination detection algorithm - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Huang’s algorithm is an algorithm for detecting termination in a distributed system. The algorithm was proposed by Shing-Tsaan Huang in 1989 in the Journal of Computers.
In a distributed system, a process is either in an active state or in an idle state at any given point of time. Termination occurs when all of the processes becomes idle and there are no any in transit(on its way to be delivered)
computational message.
Assumptions of the algorithm:

One of the co-operating processes which monitors the computation is called the controlling agent.
The initial weight of controlling agent is 1
All other processes are initially idle and have weight 0.
The computation starts when the controlling agent send a computation message to one of the processes.
The process become active on receiving a computation message.
Computation message can be sent only by controlling agent or an active process.
Control message is sent to controlling agent by an active process when they are becoming idle.
The algorithm assigns a weight W (such that 0 < W < 1 ) to every active process and every in transit message.
Notations used in the algorithm:
B(DW): Computation message with weight DW
C(DW): Control message with weight DW
Algorithm:
Rule to send B(DW) –
Suppose Process P with weight W is sending B(DW) to process Q
Split the weight of the process P into W1 and W2.
Such that
W = W1 + W2

and W1 > 0, W2 > 0

Set weight of the process P as W1 ( i.e W = W1 )
Send B(W2) to process Q, here DW = W2.
Note: Only the Controlling agent or any active process can send Computation message.
On receiving B(DW) by process Q –
Add the weight DW to the weight of process Q i.e for process Q, W = W + DW
If process Q was idle, it will become active on receiving B(DW).
Rule to send C(DW) –
Any active process having weight W can become idle by sending C(W) to controlling agent
Send a control message C(W) to the controlling agent. Here DW = W.
Set weight of the process as 0 i.e W = 0. (After this process will become idle.)
On receiving C(DW) by controlling agent –
Add the weight received through control message to the weight of controlling agent i.e W = W + DW
After adding, if the weight of controlling agent becomes 1 then it can be conclude that the computation has terminated.
Advantages of Huang’s Algorithm:
The algorithm detects every true termination in finite time.
Limitations of Huang’s Algorithm:
The algorithm is unable to detect computation termination if a message is lost in transit.
It also does not work when a process fails while in an active state.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/huangs-termination-detection-algorithm/

✍
Write a Testimonial

Hierarchical Deadlock Detection in Distributed System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Hierarchical Deadlock Detection in Distributed System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite: Deadlock detection in Distributed systems
In hierarchical deadlock detection algorithm, sites are arranged in a hierarchical fashion and a site detects deadlocks involving only its descendant sites. Distributed deadlock algorithms delegate the responsibility of
deadlock detection to individual sites while in hierarchical there are local detectors at each site which communicate their local wait for graphs(WFG) with one another.
Approach:
Deadlocks that are local to a single site are detected at that site using their local WFG. Each site also sends its local WFG to deadlock detector at the next level. Thus, distributed deadlocks involving 2 or more sites would
be detected by a deadlock detector in lowest level that has control over these sites.
In this approach, there are 2 methods to detect:

1. Ho-Ramamoorthy algorithm:
Uses only two levels i.e. Master control nodes and Cluster control nodes.
Cluster control nodes are used for detecting deadlock among their members and reporting dependencies outside their cluster to Master control node.
The master control node is responsible for detecting inter-cluster deadlocks.
2. Menasce-Muntz algorithm:
Leaf controllers are responsible for allocating resources whereas branch controllers find deadlock among the resources that their children span in the tree.
Network congestion can be managed and node failure is less critical than in fully centralized.
Detection can be done ways such as Continuous allocation reporting or periodically allocation reporting.
Advantages:
If the hierarchy coincides with resource access pattern local to cluster of sites, this approach can provide efficient deadlock detection as compared to both centralized and distributed methods.
It reduces the dependence on central sites thus, reducing the communication cost.
Disadvantages:
If deadlocks are span over several clusters, this approach will be inefficient.
It is more complicated to implement and would involve nontrivial modification to lock and transaction manager algorithms.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/hierarchical-deadlock-detection-in-distributed-system/
✍
Write a Testimonial

Purpose of an Interrupt in Computer Organization
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Purpose of an Interrupt in Computer Organization - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Interrupt is the mechanism by which modules like I/O or memory may interrupt the normal processing by CPU. It may be either clicking a mouse, dragging a cursor, printing a document etc the case where interrupt is
getting generated.
Why we require Interrupt?
External devices are comparatively slower than CPU. So if there is no interrupt CPU would waste a lot of time waiting for external devices to match its speed with that of CPU. This decreases the efficiency of CPU. Hence,
interrupt is required to eliminate these limitations.
With Interrupt:

1. Suppose CPU instructs printer to print a certain document.
2. While printer does its task, CPU engaged in executing other tasks.
3. When printer is done with its given work, it tells CPU that it has done with its work.
(The word ‘tells’ here is interrupt which sends one message that printer has done its work successfully.).
Advantages:
It increases the efficiency of CPU.
It decreases the waiting time of CPU.
Stops the wastage of instruction cycle.
Disadvantages:
CPU has to do a lot of work to handle interrupts, resume its previous execution of programs (in short, overhead required to handle the interrupt request.).

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/purpose-of-an-interrupt-in-computer-organization/
✍
Write a Testimonial

Lamport’s Algorithm for Mutual Exclusion in Distributed System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Lamport's Algorithm for Mutual Exclusion in Distributed System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite: Mutual exclusion in distributed systems
Lamport’s Distributed Mutual Exclusion Algorithm is a permission based algorithm proposed by Lamport as an illustration of his synchronization scheme for distributed systems.
In permission based timestamp is used to order critical section requests and to resolve any conflict between requests.
In Lamport’s Algorithm critical section requests are executed in the increasing order of timestamps i.e a request with smaller timestamp will be given permission to execute critical section first than a request with larger
timestamp.
In this algorithm:

Three type of messages ( REQUEST, REPLY and RELEASE) are used and communication channels are assumed to follow FIFO order.
A site send a REQUEST message to all other site to get their permission to enter critical section.
A site send a REPLY message to requesting site to give its permission to enter the critical section.
A site send a RELEASE message to all other site upon exiting the critical section.
Every site Si, keeps a queue to store critical section requests ordered by their timestamps.
request_queuei denotes the queue of site Si
A timestamp is given to each critical section request using Lamport’s logical clock.
Timestamp is used to determine priority of critical section requests. Smaller timestamp gets high priority over larger timestamp. The execution of critical section request is always in the order of their timestamp.
Algorithm:
To enter Critical section:
When a site Si wants to enter the critical section, it sends a request message Request(tsi, i) to all other sites and places the request on request_queuei. Here, Tsi denotes the timestamp of Site S i
When a site Sj receives the request message REQUEST(tsi, i) from site Si, it returns a timestamped REPLY message to site Si and places the request of site Si on request_queuej
.
To execute the critical section:
A site Si can enter the critical section if it has received the message with timestamp larger than (tsi, i) from all other sites and its own request is at the top of request_queuei
To release the critical section:
When a site Si exits the critical section, it removes its own request from the top of its request queue and sends a timestamped RELEASE message to all other sites
When a site Sj receives the timestamped RELEASE message from site Si, it removes the request of Si from its request queue
Message Complexity:
Lamport’s Algorithm requires invocation of 3(N – 1) messages per critical section execution. These 3(N – 1) messages involves
(N – 1) request messages
(N – 1) reply messages
(N – 1) release messages
Drawbacks of Lamport’s Algorithm:
Unreliable approach: failure of any one of the processes will halt the progress of entire system.
High message complexity: Algorithm requires 3(N-1) messages per critical section invocation.
Performance:
Synchronization delay is equal to maximum message transmission time
It requires 3(N – 1) messages per CS execution.
Algorithm can be optimized to 2(N – 1) messages by omitting the REPLY message in some situations.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/lamports-algorithm-for-mutual-exclusion-in-distributed-system/
✍
Write a Testimonial

Maekawa’s Algorithm for Mutual Exclusion in Distributed System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Maekawa’s Algorithm for Mutual Exclusion in Distributed System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Mutual exclusion in distributed systems
Maekawa’s Algorithm is quorum based approach to ensure mutual exclusion in distributed systems. As we know, In permission based algorithms like Lamport’s Algorithm, Ricart-Agrawala Algorithm etc. a site request
permission from every other site but in quorum based approach, A site does not request permission from every other site but from a subset of sites which is called quorum.
In this algorithm:
Three type of messages ( REQUEST, REPLY and RELEASE) are used.
A site send a REQUEST message to all other site in its request set or quorum to get their permission to enter critical section.
A site send a REPLY message to requesting site to give its permission to enter the critical section.
A site send a RELEASE message to all other site in its request set or quorum upon exiting the critical section.
The construction of request set or Quorum:
A request set or Quorum in Maekawa’s algorithm must satisfy the following properties:

1. ∀i ∀j : i ≠ j, 1 ≤ i, j ≤ N :: R i ⋂ Rj ≠ ∅
i.e there is at least one common site between the request sets of any two sites.
2. ∀i : 1 ≤ i ≤ N :: Si ∊ Ri
3. ∀i : 1 ≤ i ≤ N :: |Ri| = K
4. Any site Si is contained in exactly K sets.
5. N = K(K - 1) +1 and |R i| = √N
Algorithm:
To enter Critical section:
When a site Si wants to enter the critical section, it sends a request message REQUEST(i) to all other sites in the request set R i.
When a site Sj receives the request message REQUEST(i) from site Si, it returns a REPLY message to site Si if it has not sent a REPLY message to the site from the time it received the last RELEASE
message. Otherwise, it queues up the request.
.
To execute the critical section:
A site Si can enter the critical section if it has received the REPLY message from all the site in request set R i
To release the critical section:
When a site Si exits the critical section, it sends RELEASE(i) message to all other sites in request set R i
When a site Sj receives the RELEASE(i) message from site Si, it send REPLY message to the next site waiting in the queue and deletes that entry from the queue
In case queue is empty, site Sj update its status to show that it has not sent any REPLY message since the receipt of the last RELEASE message
Message Complexity:
Maekawa’s Algorithm requires invocation of 3√N messages per critical section execution as the size of a request set is √N. These 3√N messages involves.
√N request messages
√N reply messages
√N release messages
Drawbacks of Maekawa’s Algorithm:
This algorithm is deadlock prone because a site is exclusively locked by other sites and requests are not prioritized by their timestamp.
Performance:
Synchronization delay is equal to twice the message propagation delay time
It requires 3√n messages per critical section execution.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/maekawas-algorithm-for-mutual-exclusion-in-distributed-system/
✍
Write a Testimonial

Ricart–Agrawala Algorithm in Mutual Exclusion in Distributed System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Ricart–Agrawala Algorithm in Mutual Exclusion in Distributed System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite: Mutual exclusion in distributed systems
Ricart–Agrawala algorithm is an algorithm to for mutual exclusion in a distributed system proposed by Glenn Ricart and Ashok Agrawala. This algorithm is an extension and optimization of Lamport’s Distributed
Mutual Exclusion Algorithm. Like Lamport’s Algorithm, it also follows permission based approach to ensure mutual exclusion.
In this algorithm:
Two type of messages ( REQUEST and REPLY) are used and communication channels are assumed to follow FIFO order.
A site send a REQUEST message to all other site to get their permission to enter critical section.
A site send a REPLY message to other site to give its permission to enter the critical section.

A timestamp is given to each critical section request using Lamport’s logical clock.
Timestamp is used to determine priority of critical section requests. Smaller timestamp gets high priority over larger timestamp. The execution of critical section request is always in the order of their timestamp.
Algorithm:

To enter Critical section:
When a site Si wants to enter the critical section, it send a timestamped REQUEST message to all other sites.
When a site Sj receives a REQUEST message from site Si, It sends a REPLY message to site Si if and only if
Site Sj is neither requesting nor currently executing the critical section.
In case Site S j is requesting, the timestamp of Site S i‘s request is smaller than its own request.
Otherwise the request is deferred by site S j.
To execute the critical section:
Site Si enters the critical section if it has received the REPLY message from all other sites.
To release the critical section:
Upon exiting site Si sends REPLY message to all the deferred requests.
Message Complexity:
Ricart–Agrawala algorithm requires invocation of 2(N – 1) messages per critical section execution. These 2(N – 1) messages involves
(N – 1) request messages
(N – 1) reply messages
Drawbacks of Ricart–Agrawala algorithm:
Unreliable approach: failure of any one of node in the system can halt the progress of the system. In this situation, the process will starve forever.
The problem of failure of node can be solved by detecting failure after some timeout.
Performance:
Synchronization delay is equal to maximum message transmission time
It requires 2(N – 1) messages per Critical section execution

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/ricart-agrawala-algorithm-in-mutual-exclusion-in-distributed-system/
✍
Write a Testimonial

Recovery from Deadlock in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Recovery from Deadlock in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Deadlock Detection And Recovery
When a Deadlock Detection Algorithm determines that a deadlock has occurred in the system, the system must recover from that deadlock. There are two approaches of breaking a Deadlock:
1. Process Termination:
To eliminate the deadlock, we can simply kill one or more processes. For this, we use two methods:
(a). Abort all the Deadlocked Processes:
Aborting all the processes will certainly break the deadlock, but with a great expenses. The deadlocked processes may have computed for a long time and the result of those partial computations must be discarded
and there is a probability to recalculate them later.
(b). Abort one process at a time untill deadlock is eliminated:
Abort one deadlocked process at a time, untill deadlock cycle is eliminated from the system. Due to this method, there may be considerable overhead, because after aborting each process, we have to run deadlock
detection algorithm to check whether any processes are still deadlocked.
2. Resource Preemption:
To eliminate deadlocks using resource preemption, we preepmt some resources from processes and give those resources to other processes. This method will raise three issues –
(a). Selecting a victim:
We must determine which resources and which processes are to be preempted and also the order to minimize the cost.
(b). Rollback:
We must determine what should be done with the process from which resources are preempted. One simple idea is total rollback. That means abort the process and restart it.
(c). Starvation:
In a system, it may happen that same process is always picked as a victim. As a result, that process will never complete its designated task. This situation is called Starvation and must be avoided. One solution is
that a process must be picked as a victim only a finite number of times.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/recovery-from-deadlock-in-operating-system/
✍
Write a Testimonial

Mutual exclusion in distributed system
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Mutual exclusion in distributed system - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Mutual exclusion is a concurrency control property which is introduced to prevent race conditions. It is the requirement that a process can not enter its critical section while another concurrent process is currently present
or executing in its critical section i.e only one process is allowed to execute the critical section at any given instance of time.
Mutual exclusion in single computer system Vs. distributed system:
In single computer system, memory and other resources are shared between different processes. The status of shared resources and the status of users is easily available in the shared memory so with the help of shared
variable (For example: Semaphores) mutual exclusion problem can be easily solved.
In Distributed systems, we neither have shared memory nor a common physical clock and there for we can not solve mutual exclusion problem using shared variables. To eliminate the mutual exclusion problem in
distributed system approach based on message passing is used.

A site in distributed system do not have complete information of state of the system due to lack of shared memory and a common physical clock.
Requirements of Mutual exclusion Algorithm:
No Deadlock:
Two or more site should not endlessly wait for any message that will never arrive.
No Starvation:
Every site who wants to execute critical section should get an opportunity to execute it in finite time. Any site should not wait indefinitely to execute critical section while other site are repeatedly executing critical
section
Fairness:
Each site should get a fair chance to execute critical section. Any request to execute critical section must be executed in the order they are made i.e Critical section execution requests should be executed in the order
of their arrival in the system.
Fault Tolerance:
In case of failure, it should be able to recognize it by itself in order to continue functioning without any disruption.
Solution to distributed mutual exclusion:
As we know shared variables or a local kernel can not be used to implement mutual exclusion in distributed systems. Message passing is a way to implement mutual exclusion. Below are the three approaches based on
message passing to implement mutual exclusion in distributed systems:
1. Token Based Algorithm:
A unique token is shared among all the sites.
If a site possesses the unique token, it is allowed to enter its critical section
This approach uses sequence number to order requests for the critical section.
Each requests for critical section contains a sequence number. This sequence number is used to distinguish old and current requests.
This approach insures Mutual exclusion as the token is unique
Example: ​
Suzuki-Kasami’s Broadcast Algorithm

2. Non-token based approach:
A site communicates with other sites in order to determine which sites should execute critical section next. This requires exchange of two or more successive round of messages among sites.
This approach use timestamps instead of sequence number to order requests for the critical section.
When ever a site make request for critical section, it gets a timestamp. Timestamp is also used to resolve any conflict between critical section requests.
All algorithm which follows non-token based approach maintains a logical clock. Logical clocks get updated according to Lamport’s scheme
Example: ​
Lamport's algorithm, Ricart–Agrawala algorithm

3. Quorum based approach:
Instead of requesting permission to execute the critical section from all other sites, Each site requests only a subset of sites which is called a quorum.
Any two subsets of sites or Quorum contains a common site.
This common site is responsible to ensure mutual exclusion
Example: ​
Maekawa’s Algorithm

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/mutual-exclusion-in-distributed-system/
✍
Write a Testimonial

Translation Lookaside Buffer (TLB) in Paging
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Translation Lookaside Buffer (TLB) in Paging - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
In Operating System (Memory Management Technique : Paging), for each process page table will be created, which will contain Page Table Entry (PTE). This PTE will contain information like frame number (The address
of main memory where we want to refer), and some other useful bits (e.g., valid/invalid bit, dirty bit, protection bit etc). This page table entry (PTE) will tell where in the main memory the actual page is residing.
Now the question is where to place the page table, such that overall access time (or reference time) will be less.
The problem initially was to fast access the main memory content based on address generated by CPU (i.e logical/virtual address). Initially, some people thought of using registers to store page table, as they are high-speed
memory so access time will be less.

The idea used here is, place the page table entries in registers, for each request generated from CPU (virtual address), it will be matched to the appropriate page number of the page table, which will now tell where in the
main memory that corresponding page resides. Everything seems right here, but the problem is register size is small (in practical, it can accommodate maximum of 0.5k to 1k page table entries) and process size may be big
hence the required page table will also be big (lets say this page table contains 1M entries), so registers may not hold all the PTE’s of Page table. So this is not a practical approach.
To overcome this size issue, the entire page table was kept in main memory. but the problem here is two main memory references are required:
1. To find the frame number
2. To go to the address specified by frame number
To overcome this problem a high-speed cache is set up for page table entries called a Translation Lookaside Buffer (TLB). Translation Lookaside Buffer (TLB) is nothing but a special cache used to keep track of recently
used transactions. TLB contains page table entries that have been most recently used. Given a virtual address, the processor examines the TLB if a page table entry is present (TLB hit), the frame number is retrieved and
the real address is formed. If a page table entry is not found in the TLB (TLB miss), the page number is used to index the process page table. TLB first checks if the page is already in main memory, if not in main memory
a page fault is issued then the TLB is updated to include the new page entry.

Steps in TLB hit:
1. CPU generates virtual address.
2. It is checked in TLB (present).
3. Corresponding frame number is retrieved, which now tells where in the main memory page lies.
Steps in Page miss:
1.
2.
3.
4.
5.

CPU generates virtual address.
It is checked in TLB (not present).
Now the page number is matched to page table residing in main memory (assuming page table contains all PTE).
Corresponding frame number is retrieved, which now tells where in the main memory page lies.
The TLB is updated with new PTE (if space is not there, one of the replacement technique comes into picture i.e either FIFO, LRU or MFU etc).

Effective memory access time(EMAT) : TLB is used to reduce effective memory access time as it is a high speed associative cache.
EMAT = h*(c+m) + (1-h)*(c+2m)
where, h = hit ratio of TLB
m = Memory access time
c = TLB access time

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : VaibhavRai3, shubham_singh

Source
https://www.geeksforgeeks.org/translation-lookaside-buffer-tlb-in-paging/
✍
Write a Testimonial

The Linux Kernel
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ The Linux Kernel - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
The main purpose of a computer is to run a predefined sequence of instructions, known as a program. A program under execution is often referred to as a process. Now, most special purpose computers are meant to run a
single process, but in a sophisticated system such a general purpose computer, are intended to run many processes simulteneously. Any kind of process requires hardware resources such are Memory, Processor time,
Storage space, etc.
In a General Purpose Computer running many processes simulteneously, we need a middle layer to manage the distribution of the hardware resources of the computer efficiently and fairly among all the various processes
running on the computer. This middle layer is referred to as the kernel. Basically the kernel virtualizes the common hardware resources of the computer to provide each process with its own virtual resources. This makes
the process seem as it is the sole process running on the machine. The kernel is also responsible for preventing and mitigating conflicts between different processes.
This schematically represented below:

Figure: Virtual Resources for each Process
The Core Subsystems of the Linux Kernel are as follows:
1.
2.
3.
4.
5.

The Process Scheduler
The Memory Management Unit (MMU)
The Virtual File System (VFS)
The Networking Unit
Inter-Process Communication Unit

Figure: The Linux Kernel
For the purpose of this article we will only be focussing on the 1st three important subsystems of the Linux Kernel.
The basic functioning of each of the 1st three subsystems is elaborated below:
The Process Scheduler:
This kernel subsystem is responsible for fairly distributing the CPU time among all the processes running on the system simulteneously.
The Memory Management Unit:
This kernel sub-unit is responsible for proper distribution of the memory resources among the various processes running on the system. The MMU does more than just simply provide separate virtual address spaces
for each of the processes.
The Virtual File System:
This subsystem is responsible for providing a unified interface to access stored data across different filesystems and physical storage media.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/the-linux-kernel/
✍
Write a Testimonial

Difference between Random Access Memory (RAM) and Hard Disk Drive (HDD)

​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Random Access Memory (RAM) and Hard Disk Drive (HDD) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
A person novice to computers often is in confusion between Random Access Memory (RAM) and Hard Disk Drive (HDD). Here we draw comparisons between the two.
Similarities between RAM and HDD :
Both are used for storage of data.
Both are physical components of the computer machine.
Differences between RAM and HDD :

Parameter

RAM

HDD

Full Form
RAM stands for Random Access Memory.
HDD stands for Hard Disk Drive.
Also known as
RAM is also known as primary memory.
HDD is also known as secondary memory.
Components
RAM does not contains, mechanical parts, only electronical parts like transistors.
HDD contains moving mechanical parts, like the arm.
R/W Time
RAM has shorter R/W time.
HDD has longer R/W time.
Memory Access In RAM each and every element takes same time to be accessed.
In HDD different elements take different time to be accessed.
Size
In a system the RAM is smaller than the HDD.
In a system the HDD is larger than the RAM.
Cost
RAM is costlier per unit storage.
HDD is cheaper per unit storage.
Noise
RAM does not produces noise.
HDD can produce noise due to mechanical movements.
Duration of Data Data stored in RAM is temporary. It remembers as long as it has electricity, i.e. the power is on. Data stored in HDD is permanent, i.e. it retains data even after shutdown.
Speed of Computer Inadequate RAM slows down the speed of the computer.
HDD does not affect the speed of the computer.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : ak21

Source
https://www.geeksforgeeks.org/difference-between-random-access-memory-ram-and-hard-disk-drive-hdd/
✍
Write a Testimonial

Privileged and Non-Privileged Instructions in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Privileged and Non-Privileged Instructions in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
In any Operating System, it is necessary to have Dual Mode Operation to ensure protection and security of the System from unauthorized or errant users . This Dual Mode separates the User Mode from the System Mode
or Kernel Mode.

What are Privileged Instructions?

The Instructions that can run only in Kernel Mode are called Privileged Instructions .
Privileged Instructions possess the following characteristics :

(i) If any attempt is made to execute a Privileged Instruction in User Mode, then it will not be executed and treated as an illegal instruction. The Hardware traps it to the Operating System.
(ii) Before transferring the control to any User Program, it is the responsibility of the Operating System to ensure that the Timer is set to interrupt. Thus, if the timer interrupts then the Operating System regains the
control.
Thus, any instruction which can modify the contents of the Timer is a Privileged Instruction.
(iii) Privileged Instructions are used by the Operating System in order to achieve correct operation.
(iv) Various examples of Privileged Instructions include:
I/O instructions and Halt instructions
Turn off all Interrupts
Set the Timer
Context Switching
Clear the Memory or Remove a process from the Memory
Modify entries in Device-status table
What are Non-Privileged Instructions?
The Instructions that can run only in User Mode are called Non-Privileged Instructions .
Various examples of Non-Privileged Instructions include:
Reading the status of Processor
Reading the System Time
Generate any Trap Instruction
Sending the final prinout of Printer
Also, it is important to note that in order to change the mode from Privileged to Non-Privileged, we require a Non-privileged Instruction that does not generate any interrupt.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : VaibhavRai3

Source
https://www.geeksforgeeks.org/privileged-and-non-privileged-instructions-in-operating-system/
✍
Write a Testimonial

Implementing Directory Management using Shell Script
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Implementing Directory Management using Shell Script - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Directory management constitutes the functions dealing with organization and maintenance of various directories. Directories usually contain files of any type, but this may vary between file systems. The content of a
directory does not affect the directory object itself.
Some of the directory functions are:
Navigation
Absolute/ Relative Pathnames
Listing Directories
Creating Directories
Modifying Directories
You can write your script in an editor like pico etc. Execute your files as mentioned below in the output screenshot. The following shell script implements these functions of directory management, using commands
available in Linux.

filter_none
edit
close
play_arrow
link
brightness_4
code
echo "
"
echo "----Implementing Directory Management----"
echo "
"
ch=0
while [ $ch -lt 6 ]
do
echo "Press the following to :"
echo "1) Create a new directory."
echo "2) Modify a directory."
echo "3) Navigate into directory."

echo "4) Listing directories."
echo "5) Exit."
read ch
case $ch in
1) echo " "
echo "---Creation of Directory---"
echo " "
echo "Enter the name of the directory:"
read name
mkdir $name
;;
2) echo " "
echo "---Modification of Directory---"
echo "
"
echo "Enter the directory to be modified:"
read orgdir
echo "Press the following to :"
echo "
"
echo "1) Rename directory."
echo "2) Copy directory to another."
echo "3) Move directory."
echo "4) Delete directory."
echo "5) Exit from Modify Mode."
read modch
case $modch in
1) echo " "
echo "---Rename a directory---"
echo " "
echo "Enter new name for the directory:"
read newname
mv $orgdir $newname
;;
2) echo " "
echo "---Copying a directory to another---"
echo " "
echo "Enter target directory:"
read target
mkdir $target
cp $orgdir $target
;;
3) echo " "
echo "---Moving a directory---"
echo " "
echo "Enter target directory:"
read target
mkdir $target
mv $orgdir $target
;;
4) echo " "
echo "---Deleting a directory---"
echo " "
rmdir $orgdir
;;
5) echo " "
echo "---Exiting from modify mode---"
echo " "
exit
;;
esac
;;
3)
echo
echo
echo
echo
echo
echo
read

"---Navigation of Directory---"
" "
"Enter your choice for method of navigation :"
"1) Go to Parent Directory. "
"2) Navigate to specific directory."
"3) Exit from Navigate Mode."
navch

case $navch in
1) echo " "
echo "---Parent Directory---"
echo " "
cd ..
pwd
;;
2) echo " "
echo "---Navigation to Specific Directory---"
echo " "
echo "Enter the target Path:"
read path
cd $path
pwd
;;
3) echo " "
echo "---Exiting from Navigate Mode---"
echo " "
exit
;;
esac
;;
4)
echo
echo
echo
echo
echo
echo
read

"--Listing of Directories---"
" "
"Enter your choice for method of listing :"
"1) List of directories. "
"2) List of directories and their details."
"3) Exit from List Mode."
lisch

case $lisch in
1) echo " "
echo "---List of directories---"
echo " "
ls
;;
2) echo " "
echo "---Detailed List of directories---"
echo " "
ls -l
;;
3) echo " "
echo "---Exiting from List Mode---"
echo " "
exit
;;
esac
;;
5)echo " "
echo "---Exiting---"
echo " "
exit
;;
esac
done

chevron_right
filter_none
Output:

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/implementing-directory-management-using-shell-script/
✍
Write a Testimonial

Logical and Physical Address in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Logical and Physical Address in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu

Logical Address is generated by CPU while a program is running. The logical address is virtual address as it does not exist physically, therefore, it is also known as Virtual Address. This address is used as a reference to
access the physical memory location by CPU. The term Logical Address Space is used for the set of all logical addresses generated by a program’s perspective.
The hardware device called Memory-Management Unit is used for mapping logical address to its corresponding physical address.
Physical Address identifies a physical location of required data in a memory. The user never directly deals with the physical address but can access by its corresponding logical address. The user program generates the
logical address and thinks that the program is running in this logical address but the program needs physical memory for its execution, therefore, the logical address must be mapped to the physical address by MMU before
they are used. The term Physical Address Space is used for all physical addresses corresponding to the logical addresses in a Logical address space.

Mapping virtual-address to physical-addresses
Differences Between Logical and Physical Address in Operating System
1.
2.
3.
4.
5.

The basic difference between Logical and physical address is that Logical address is generated by CPU in perspective of a program whereas the physical address is a location that exists in the memory unit.
Logical Address Space is the set of all logical addresses generated by CPU for a program whereas the set of all physical address mapped to corresponding logical addresses is called Physical Address Space.
The logical address does not exist physically in the memory whereas physical address is a location in the memory that can be accessed physically.
Identical logical addresses are generated by Compile-time and Load time address binding methods whereas they differs from each other in run-time address binding method. Please refer this for details.
The logical address is generated by the CPU while the program is running whereas the physical address is computed by the Memory Management Unit (MMU).

Comparison Chart:
Paramenter

LOGICAL ADDRESS

PHYSICAL ADDRESS

Basic
generated by CPU
location in a memory unit
Address Space Logical Address Space is set of all logical addresses generated by CPU in reference to a program. Physical Address is set of all physical addresses mapped to the corresponding logical addresses.
Visibility
User can view the logical address of a program.
User can never view physical address of program.
Generation
generated by the CPU
Computed by MMU
Access
The user can use the logical address to access the physical address.
The user can indirectly access physical address but not directly.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : VaibhavRai3

Source
https://www.geeksforgeeks.org/logical-and-physical-address-in-operating-system/
✍
Write a Testimonial

Memory Interleaving
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Memory Interleaving - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Virtual Memory
Abstraction is one the most important aspect of computing. It is widely implemented Practice in the Computational field.
Memory Interleaving is less or More an Abstraction technique. Though its a bit different from Abstraction. It is a Technique which divides memory into a number of modules such that Successive words in the address
space are placed in the Different module.
Consecutive Word in a Module:

Figure-1: Consecutive Word in a Module
Let us assume 16 Data’s to be Transferred to the Four Module. Where Module 00 be Module 1, Module 01 be Module 2, Module 10 be Module 3 & Module 11 be Module 4. Also 10, 20, 30….130 are the data to be
transferred.
From the figure above in Module 1, 10 [Data] is transferred then 20, 30 & finally, 40 which are the Data. That means the data are added consecutively in the Module till its max capacity.
Most significant bit (MSB) provides the Address of the Module & least significant bit (LSB) provides the address of the data in the module.
For Example, to get 90 (Data) 1000 will be provided by the processor. In this 10 will indicate that the data is in module 10 (module 3) & 00 is the address of 90 in Module 10 (module 3). So,
Module
Module
Module
Module

1
2
3
4

Contains
Contains
Contains
Contains

Data
Data
Data
Data

:
:
:
:

10, 20, 30, 40​
50, 60, 70, 80​
90, 100, 110, 120​
130, 140, 150, 160

Consecutive Word in Consecutive Module:

Figure-2: Consecutive Word in Consecutive Module
Now again we assume 16 Data’s to be transferred to the Four Module. But Now the consecutive Data are added in Consecutive Module. That is, 10 [Data] is added in Module 1, 20 [Data] in Module 2 and So on.
Least Significant Bit (LSB) provides the Address of the Module & Most significant bit (MSB) provides the address of the data in the module.
For Example, to get 90 (Data) 1000 will be provided by the processor. In this 00 will indicate that the data is in module 00 (module 1) & 10 is the address of 90 in Module 00 (module 1). That is,
Module
Module
Module
Module

1
2
3
4

Contains
Contains
Contains
Contains

Data
Data
Data
Data

:
:
:
:

10,
20,
30,
40,

50,
60,
70,
80,

90, 130​
100, 140​
110, 150​
120, 160

Why we use Memory Interleaving? [Advantages]:
Whenever, Processor request Data from the main memory. A block (chunk) of Data is Transferred to the cache and then to Processor. So whenever a cache miss occurs the Data is to be fetched from main memory. But
main memory is relatively slower than the cache. So to improve the access time of the main memory interleaving is used.
We can access all four Module at the same time thus achieving Parallelism. From Figure 2 the data can be acquired from the Module using the Higher bits. This method Uses memory effectively.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/memory-interleaving/
✍
Write a Testimonial

Get/Set process resource limits in C
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Get/Set process resource limits in C - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
The getrlimit() and setrlimit() system calls can be used to get and set the resource limits such as files, CPU, memory etc. associated with a process.
Each resource has an associated soft and hard limit.
soft limit: The soft limit is the actual limit enforced by the kernel for the corresponding resource.
hard limit: The hard limit acts as a ceiling for the soft limit.
The soft limit ranges in between 0 and hard limit.

The two limits are defined by the following structure
filter_none
edit
close
play_arrow
link
brightness_4
code
struct rlimit {
rlim_t rlim_cur;
rlim_t rlim_max;
};

/* Soft limit */
/* Hard limit (ceiling for rlim_cur) */

chevron_right
filter_none
The signatures of the system calls are
filter_none
edit
close
play_arrow
link
brightness_4
code
int getrlimit(int resource, struct rlimit *rlim);
int setrlimit(int resource, const struct rlimit *rlim);

chevron_right
filter_none
resource refers to the resource limits you want to retrieve or modify.
To set both the limits, set the values with the new values to the elements of rlimit structure.
To get both the limits, pass the address of rlim. Successful call to getrlimit(), sets the rlimit elements to the limits.
On success, both return 0. On error, -1 is returned, and errno is set appropriately.
Here, is a program demonstrating the system calls by changing the value one greater than the maximum file descriptor number to 3.

filter_none
edit
close
play_arrow
link
brightness_4
code
// C program to demonstrate working of getrlimit()
// and setlimit()
#include <stdio.h>
#include <sys/resource.h>
#include <string.h>
#include <errno.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
int main() {
struct rlimit old_lim, lim, new_lim;
// Get old limits
if( getrlimit(RLIMIT_NOFILE, &old_lim) == 0)
printf("Old limits -> soft limit= %ld \t"
" hard limit= %ld \n", old_lim.rlim_cur,
old_lim.rlim_max);
else
fprintf(stderr, "%s\n", strerror(errno));
// Set new value
lim.rlim_cur = 3;
lim.rlim_max = 1024;

// Set limits
if(setrlimit(RLIMIT_NOFILE, &lim) == -1)
fprintf(stderr, "%s\n", strerror(errno));
// Get new limits
if( getrlimit(RLIMIT_NOFILE, &new_lim) == 0)
printf("New limits -> soft limit= %ld "
"\t hard limit= %ld \n", new_lim.rlim_cur,
new_lim.rlim_max);
else
fprintf(stderr, "%s\n", strerror(errno));
return 0;
}

chevron_right
filter_none
Output:
​
Old limits -> soft limit= 1048576
New limits -> soft limit= 3

hard limit= 1048576 ​
hard limit= 1024​

The Old limits values may vary depending upon the system.
Now, If you try to open a new file, it will show run time error, because maximum 3 files can be opened and that are already being opened by the system(STDIN, STDOUT, STDERR).

filter_none
edit
close
play_arrow
link
brightness_4
code
// C program to demonstrate error when a
// process tries to access resources beyond
// limit.
#include <stdio.h>
#include <sys/resource.h>
#include <string.h>
#include <errno.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
int main() {
struct rlimit old_lim, lim, new_lim;
// Get old limits
if( getrlimit(RLIMIT_NOFILE, &old_lim) == 0)
printf("Old limits -> soft limit= %ld \t"
" hard limit= %ld \n", old_lim.rlim_cur,
old_lim.rlim_max);
else
fprintf(stderr, "%s\n", strerror(errno));
// Set new value
lim.rlim_cur = 3;
lim.rlim_max = 1024;
// Set limits
if(setrlimit(RLIMIT_NOFILE, &lim) == -1)
fprintf(stderr, "%s\n", strerror(errno));
// Get new limits
if( getrlimit(RLIMIT_NOFILE, &new_lim) == 0)
printf("New limits -> soft limit= %ld \t"
" hard limit= %ld \n", new_lim.rlim_cur,
new_lim.rlim_max);
else
fprintf(stderr, "%s\n", strerror(errno));
// Try to open a new file
if(open("foo.txt", O_WRONLY | O_CREAT, 0) == -1)
fprintf(stderr, "%s\n", strerror(errno));
else
printf("Opened successfully\n");
return 0;
}

chevron_right
filter_none
Output:
Old limits -> soft limit= 1048576
New limits -> soft limit= 3

hard limit= 1048576
hard limit= 1024

Too many open files

There is another system call prlimit() that combines both the system calls.
For more details, check manual by typing
man 2 prlimit

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/get-set-process-resource-limits-in-c/
✍
Write a Testimonial

Memory Hierarchy Design and its Characteristics
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Memory Hierarchy Design and its Characteristics - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​

Suggest a Topic

Select a Category​
menu
In the Computer System Design, Memory Hierarchy is an enhancement to organize the memory such that it can minimize the access time. The Memory Hierarchy was developed based on a program behavior known as
locality of references.The figure below clearly demonstrates the different levels of memory hierarchy :

This Memory Hierarchy Design is divided into 2 main types:

1. External Memory or Secondary Memory –
Comprising of Magnetic Disk, Optical Disk, Magnetic Tape i.e. peripheral storage devices which are accessible by the processor via I/O Module.
2. Internal Memory or Primary Memory –
Comprising of Main Memory, Cache Memory & CPU registers. This is directly accessible by the processor.
We can infer the following characteristics of Memory Hierarchy Design from above figure:
1. Capacity:
It is the global volume of information the memory can store. As we move from top to bottom in the Hierarchy, the capacity increases.
2. Access Time:
It is the time interval between the read/write request and the availability of the data. As we move from top to bottom in the Hierarchy, the access time increases.
3. Performance:
Earlier when the computer system was designed without Memory Hierarchy design, the speed gap increases between the CPU registers and Main Memory due to large difference in access time. This results in lower
performance of the system and thus, enhancement was required. This enhancement was made in the form of Memory Hierarchy Design because of which the performance of the system increases. One of the most
significant ways to increase system performance is minimizing how far down the memory hierarchy one has to go to manipulate data.
4. Cost per bit:
As we move from bottom to top in the Hierarchy, the cost per bit increases i.e. Internal Memory is costlier than External Memory.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/memory-hierarchy-design-and-its-characteristics/
✍
Write a Testimonial

Deadlock, Starvation, and Livelock
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Deadlock, Starvation, and Livelock - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Deadlock and Starvation
Livelock occurs when two or more processes continually repeat the same interaction in response to changes in the other processes without doing any useful work. These processes are not in the waiting state, and they are
running concurrently. This is different from a deadlock because in a deadlock all processes are in the waiting state.

Example:
Imagine a pair of processes using two resources, as shown:

filter_none
edit
close
play_arrow
link
brightness_4
code
void process_A(void)
{
enter_reg(& resource_1);
enter_reg(& resource_2);
use_both_resources();
leave_reg(& resource_2);
leave_reg(& resource_1);
}
void process_B(void)
{
enter_reg(& resource_1);
enter_reg(& resource_2);
use_both_resources();
leave_reg(& resource_2);
leave_reg(& resource_1);
}

chevron_right
filter_none
Each of the two processes needs the two resources and they use the polling primitive enter_reg to try to acquire the locks necessary for them. In case the attempt fails, the process just tries again.
If process A runs first and acquires resource 1 and then process B runs and acquires resource 2, no matter which one runs next, it will make no further progress, but neither of the two processes blocks. What actually
happens is that it uses up its CPU quantum over and over again without any progress being made but also without any sort of blocking. Thus this situation is not that of a deadlock( as no process is being blocked) but we
have something functionally equivalent to deadlock: LIVELOCK.
What leads to Livelocks?
Occurrence of livelocks can occur in the most surprising of ways. The total number of allowed processes in some systems, is determined by the number of entries in the process table. Thus process table slots can be
referred to as Finite Resources. If a fork fails because of the table being full, waiting a random time and trying again would be a reasonable approach for the program doing the fork.
Consider a UNIX system having 100 process slots. Ten programs are running, each of which having to create 12 (sub)processes. After each process has created 9 processes, the 10 original processes and the 90 new
processes have exhausted the table. Each of the 10 original processes now sits in an endless loop forking and failing – which is aptly the situation of a deadlock. The probability of this happening is very little but it could
happen.
Difference between Deadlock, Starvation, and Livelock:
A livelock is similar to a deadlock, except that the states of the processes involved in the livelock constantly change with regard to one another, none progressing. Livelock is a special case of resource starvation; the
general definition only states that a specific process is not progressing.
Livelock:
filter_none
edit
close
play_arrow
link
brightness_4
code
var l1 = .... // lock object like semaphore or mutex etc
var l2 = .... // lock object like semaphore or mutex etc
// Thread1
Thread.Start( ()=> {
while (true) {
if (!l1.Lock(1000)) {
continue;
}
if (!l2.Lock(1000)) {
continue;
}
/// do some work
});
// Thread2
Thread.Start( ()=> {
while (true) {
if (!l2.Lock(1000)) {
continue;
}
if (!l1.Lock(1000)) {
continue;
}
// do some work
});

chevron_right
filter_none
Deadlock:

filter_none
edit
close
play_arrow
link
brightness_4
code
var p = new object();
lock(p)
{
lock(p)
{
// deadlock. Since p is previously locked
// we will never reach here...
}

chevron_right
filter_none
A deadlock is a state in which each member of a group of actions, is waiting for some other member to release a lock. A livelock on the other hand is almost similar to a deadlock, except that the states of the processes
involved in a livelock constantly keep on changing with regard to one another, none progressing. Thus Livelock is a special case of resource starvation, as stated from the general definition, the process is not progressing.
Starvation:
Starvation is a problem which is closely related to both, Livelock and Deadlock. In a dynamic system, requests for resources keep on happening. Thereby, some policy is needed to make a decision about who gets the
resource when. This process, being reasonable, may lead to a some processes never getting serviced even though they are not deadlocked.
filter_none
edit
close
play_arrow
link
brightness_4
code
Queue q = .....
while (q.Count & gt; 0)
{
var c = q.Dequeue();
.........
// Some method in different thread accidentally
// puts c back in queue twice within same time frame
q.Enqueue(c);
q.Enqueue(c);
// leading to growth of queue twice then it
// can consume, thus starving of computing
}

chevron_right
filter_none
Starvation happens when “greedy” threads make shared resources unavailable for long periods. For instance, suppose an object provides a synchronized method that often takes a long time to return. If one thread invokes
this method frequently, other threads that also need frequent synchronized access to the same object will often be blocked.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/deadlock-starvation-and-livelock/
✍
Write a Testimonial

Functions of Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Functions of Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Introduction of Operating System – Set 1
An Operating System acts as a communication bridge (interface) between the user and computer hardware. The purpose of an operating system is to provide a platform on which a user can execute programs in a
convenient and efficient manner.
An operating system is a piece of software that manages the allocation of computer hardware. The coordination of the hardware must be appropriate to ensure the correct working of the computer system and to prevent user
programs from interfering with the proper working of the system.
What is Operating System ?
An operating system is a program on which application programs are executed and acts as an communication bridge (interface) between the user and the computer hardware.

The main task an operating system carries out is the allocation of resources and services, such as allocation of: memory, devices, processors and information. The operating system also includes programs to manage these
resources, such as a traffic controller, a scheduler, memory management module, I/O programs, and a file system.
Important functions of an operating System:
1. Security –
The operating system uses password protection to protect user data and similar other techniques. it also prevents unauthorized access to programs and user data.
2. Control over system performance –
Monitors overall system health to help improve performance. records the response time between service requests and system response to have a complete view of the system health. This can help improve
performance by providing important information needed to troubleshoot problems.
3. Job accounting –

Operating system Keeps track of time and resources used by various tasks and users, this information can be used to track resource usage for a particular user or group of user.
4. Error detecting aids –
Operating system constantly monitors the system to detect errors and avoid the malfunctioning of computer system.
5. Coordination between other software and users –
Operating systems also coordinate and assign interpreters, compilers, assemblers and other software to the various users of the computer systems.
6. Memory Management –
The operating system manages the Primary Memory or Main Memory. Main memory is made up of a large array of bytes or words where each byte or word is assigned a certain address. Main memory is a fast
storage and it can be accessed directly by the CPU. For a program to be executed, it should be first loaded in the main memory. An Operating System performs the following activities for memory management:
It keeps tracks of primary memory, i.e., which bytes of memory are used by which user program. The memory addresses that have already been allocated and the memory addresses of the memory that has not yet
been used. In multi programming, the OS decides the order in which process are granted access to memory, and for how long. It Allocates the memory to a process when the process requests it and deallocates the
memory when the process has terminated or is performing an I/O operation.
7. Processor Management –
In a multi programming environment, the OS decides the order in which processes have access to the processor, and how much processing time each process has. This function of OS is called process scheduling. An
Operating System performs the following activities for processor management.
Keeps tracks of the status of processes. The program which perform this task is known as traffic controller. Allocates the CPU that is processor to a process. De-allocates processor when a process is no more
required.
8. Device Management –
An OS manages device communication via their respective drivers. It performs the following activities for device management. Keeps tracks of all devices connected to system. designates a program responsible for
every device known as the Input/Output controller. Decides which process gets access to a certain device and for how long. Allocates devices in an effective and efficient way. Deallocates devices when they are no
longer required.
9. File Management –
A file system is organized into directories for efficient or easy navigation and usage. These directories may contain other directories and other files. An Operating System carries out the following file management
activities. It keeps track of where information is stored, user access settings and status of every file and more… These facilities are collectively known as the file system.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : KarimKamel

Source
https://www.geeksforgeeks.org/functions-of-operating-system/
✍
Write a Testimonial

Buddy Memory Allocation Program | Set 2 (Deallocation)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Buddy Memory Allocation Program | Set 2 (Deallocation) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Buddy Allocation | Set 1
Question: Write a program to implement the buddy system of memory allocation and deallocation in Operating Systems.
Explanation –
As we already know from Set 1, the allocation is done via the usage of free lists. Now, for deallocation, we will maintain an extra data structure-a Map (unordered_set in C++, HashMap in Java) with the starting address of
segment as key and size of the segment as value and update it whenever an allocation request comes. Now, when deallocation request comes, we will first check the map to see if it is a valid request. If so, we will then add
the block to the free list tracking blocks of its sizes. Then, we will search the free list to see if it’s buddy is free-if so, we will merge the blocks and place them on the free list above them (which tracks blocks of double the
size), else we will not coalesce and simply return after that.
How to know which block is a given block’s buddy?
Let us define two terms-buddyNumber and buddyAddress. The buddyNumber of a block is calculated by the formula:
(base_address-starting_address_of_main_memory)/block_size

We note that this is always an integer, as both numerator and denominator are powers of 2. Now, a block will be another block’s buddy if both of them were formed by the splitting of the same bigger block. For example,
if 4 consecutive allocation requests of 16 bytes come, we will end up with blocks 0-15, 16-31, 32-47, 48-63 where blocks 0-15 and 16-31 are buddies (as they were formed by splitting block 0-32) but 0-15 and 32-47
aren’t. The buddyAddress of a block is the starting index of its buddy block, given by the formula:

block_starting_address+block_size (if buddyNumber is even)​
block_starting_address-block_size (if buddyNumber is odd)

Thus, all we have to do is find this buddyAddress in the free list (by comparing with all the starting addresses in that particular list), and if present, coalescing can be done.
Examples:
Let us see how the algorithm proceeds by tracking a memory block of size 128 KB. Initially, the free list is: {}, {}, {}, {}, {}, {}, {}, { (0, 127) }
Allocation Request: 16 bytes
No such block found, so we traverse up and split the 0-127 block into 0-63, 64-127; we add 64-127 to list tracking 64 byte blocks and pass 0-63 downwards; again it is split into 0-31 and 32-63; we add 32-63 to list
tracking 32 byte blocks, passing 0-31 downwards; one more spli is done and 0-15 is returned to user while 16-31 is added to free list tracking 16 byte blocks.
List is: {}, {}, {}, {}, { (16, 31) }, { (32, 63) }, { (64, 127) }, {}
Allocation Request: 16 bytes

Straight up memory segment 16-31 will be allocated as it already exists.
List is: {}, {}, {}, {}, {}, { (32, 63) }, { (64, 127) }, {}
Allocation Request: 16 bytes
No such block found, so we will traverse up to block 32-63 and split it to blocks 32-47 and 48-63; we will add 48-63 to list tracking 16 byte blocks and return 32-47 to user.
List is: {}, {}, {}, {}, { (48, 63) }, {}, { (64, 127) }, {}
Allocation Request: 16 bytes
Straight up memory segment 48-63 will be allocated as it already exists.
List is: {}, {}, {}, {}, {}, {}, { (64, 127) }, {}
Deallocation Request: StartIndex = 0
Deallocation will be done but no coalesscing is possible as it’s buddyNumber is 0 and buddyAddress is 16 (via the formula), none of which is in the free list.
List is: {}, {}, {}, {}, { (0, 15) }, {}, { (64, 127) }, {}
Deallocation Request: StartIndex = 9
Result: Invalid request, as this segmeent was never allocated.
List is: {}, {}, {}, {}, { (0, 15) }, {}, { (64, 127) }, {}
Deallocation Request: StartIndex = 32
Deallocation will be done but no coalesscing is possible as the buddyNumber of the blocks are 0 and 2 buddyAddress of the blocks are 16 and 48, repectively, none of which is in the free list.
List is: {}, {}, {}, {}, { (0, 15), (32-47) }, {}, { (64, 127) }, {}
Deallocation Request: StartIndex = 16
Deallocation will be done and coealsecing of the blocks 0-15 and 16-31 will also be done as the buddyAddress of block 16-31 is 0, which is present in the free list tracking 16 byte blocks.
List is: {}, {}, {}, {}, { (32-47) }, { (0, 31) }, { (64, 127) }, {}

Figure – Buddy algorithm-allocation and deallocation
Implementation –
Below is the complete program.
Java
filter_none
edit
close
play_arrow
link
brightness_4
code
import java.io.*;
import java.util.*;
class Buddy {
// Inner class to store lower
// and upper bounds of the allocated memory
class Pair {
int lb, ub;
Pair(int a, int b)
{
lb = a;
ub = b;
}
}
// Size of main memory
int size;
// Array to track all
// the free nodes of various sizes
ArrayList<Pair> arr[];
// Hashmap to store the starting
// address and size of allocated segment
// Key is starting address, size is value
HashMap<Integer, Integer> hm;
// Else compiler will give warning
// about generic array creation
@SuppressWarnings("unchecked")
Buddy(int s)
{
size = s;
hm = new HashMap<>();
// Gives us all possible powers of 2
int x = (int)Math.ceil(Math.log(s) / Math.log(2));
// One extra element is added
// to simplify arithmetic calculations
arr = new ArrayList[x + 1];
for (int i = 0; i <= x; i++)
arr[i] = new ArrayList<>();
// Initially, only the largest block is free
// and hence is on the free list
arr[x].add(new Pair(0, size - 1));
}
void allocate(int s)
{
// Calculate which free list to search to get the
// smallest block large enough to fit the request

int x = (int)Math.ceil(Math.log(s) / Math.log(2));
int i;
Pair temp = null;
// We already have such a block
if (arr[x].size() > 0) {
// Remove from free list
// as it will be allocated now
temp = (Pair)arr[x].remove(0);
System.out.println("Memory from " + temp.lb
+ " to " + temp.ub + " allocated");
// Store in HashMap
hm.put(temp.lb, temp.ub - temp.lb + 1);
return;
}
// If not, search for a larger block
for (i = x + 1; i < arr.length; i++) {
if (arr[i].size() == 0)
continue;
// Found a larger block, so break
break;
}
// This would be true if no such block was found
// and array was exhausted
if (i == arr.length) {
System.out.println("Sorry, failed to allocate memory");
return;
}
// Remove the first block
temp = (Pair)arr[i].remove(0);
i--;
// Traverse down the list
for (; i >= x; i--) {
// Divide the block in two halves
// lower index to half-1
Pair newPair = new Pair(temp.lb, temp.lb
+ (temp.ub - temp.lb) / 2);
// half to upper index
Pair newPair2 = new Pair(temp.lb
+ (temp.ub - temp.lb + 1) / 2,
temp.ub);
// Add them to next list
// which is tracking blocks of smaller size
arr[i].add(newPair);
arr[i].add(newPair2);
// Remove a block to continue the downward pass
temp = (Pair)arr[i].remove(0);
}
// Finally inform the user
// of the allocated location in memory
System.out.println("Memory from " + temp.lb
+ " to " + temp.ub + " allocated");
// Store in HashMap
hm.put(temp.lb, temp.ub - temp.lb + 1);
}
void deallocate(int s)
{
// Invalid reference, as this was never allocated
if (!hm.containsKey(s)) {
System.out.println("Sorry, invalid free request");
return;
}
// Get the list which will track free blocks
// of this size
int x = (int)Math.ceil(Math.log(hm.get(s))
/ Math.log(2));
int i, buddyNumber, buddyAddress;
// Add it to the free list
arr[x].add(new Pair(s, s + (int)Math.pow(2, x) - 1));
System.out.println("Memory block from " + s + " to "
+ (s + (int)Math.pow(2, x) - 1) + " freed");
// Calculate it's buddy number and buddyAddress. The
// base address is implicitly 0 in this program, so no
// subtraction is necessary for calculating buddyNumber
buddyNumber = s / hm.get(s);
if (buddyNumber % 2 != 0) {
buddyAddress = s - (int)Math.pow(2, x);
}
else {
buddyAddress = s + (int)Math.pow(2, x);
}
// Search in the free list for buddy
for (i = 0; i < arr[x].size(); i++) {
// This indicates the buddy is also free
if (arr[x].get(i).lb == buddyAddress) {
// Buddy is the block after block
// with this base address
if (buddyNumber % 2 == 0) {
// Add to appropriate free list
arr[x + 1].add(new Pair(s, s
+ 2 * ((int)Math.pow(2, x)) - 1));
System.out.println("Coalescing of blocks starting at "
+ s + " and "
+ buddyAddress + " was done");
}
// Buddy is the block before block
// with this base address
else {
// Add to appropriate free list
arr[x + 1].add(new Pair(buddyAddress,
buddyAddress + 2 * ((int)Math.pow(2, x))
- 1));
System.out.println("Coalescing of blocks starting at "
+ buddyAddress + " and "
+ s + " was done");
}
// Remove the individual segements

// as they have coalesced
arr[x].remove(i);
arr[x].remove(arr[x].size() - 1);
break;
}
}
// Remove entry from HashMap
hm.remove(s);
}
public static void main(String args[]) throws IOException
{
int initialMemory = 0, type = -1, val = 0;
// Uncomment below section for interactive I/O
/*Scanner sc=new Scanner(System.in);
initialMemory = sc.nextInt();
Buddy obj=new Buddy(initialMemory);
while(true)
{
type = sc.nextInt();
if(type==-1)
break;
else if(type==1)
{
val=sc.nextInt();
obj.allocate(val);
}
else
{
val=sc.nextInt();
obj.deallocate(val);
}
}*/
initialMemory = 128;
Buddy obj = new Buddy(initialMemory);
obj.allocate(16);
obj.allocate(16);
obj.allocate(16);
obj.allocate(16);
obj.deallocate(0);
obj.deallocate(9);
obj.deallocate(32);
obj.deallocate(16);
}
}

chevron_right
filter_none
C#
filter_none
edit
close
play_arrow
link
brightness_4
code
using System;
using System.Collections.Generic;
public class Buddy
{
// Inner class to store lower
// and upper bounds of the
// allocated memory
class Pair
{
public int lb, ub;
public Pair(int a, int b)
{
lb = a;
ub = b;
}
}
// Size of main memory
int size;
// Array to track all
// the free nodes of various sizes
List<Pair> []arr;
// Hashmap to store the starting
// address and size of allocated segment
// Key is starting address, size is value
Dictionary<int, int> hm;
// Else compiler will give warning
// about generic array creation
Buddy(int s)
{
size = s;
hm = new Dictionary<int, int>();
// Gives us all possible powers of 2
int x = (int)Math.Ceiling(Math.Log(s) /
Math.Log(2));
// One extra element is added
// to simplify arithmetic calculations
arr = new List<Pair>[x + 1];
for (int i = 0; i <= x; i++)
arr[i] = new List<Pair>();
// Initially, only the largest block is
// free and hence is on the free list
arr[x].Add(new Pair(0, size - 1));
}
void allocate(int s)
{
// Calculate which free list to
// search to get the smallest block
// large enough to fit the request
int x = (int)Math.Ceiling(Math.Log(s) /
Math.Log(2));
int i;
Pair temp = null;
// We already have such a block
if (arr[x].Count > 0)
{

// Remove from free list
// as it will be allocated now
temp = (Pair)arr[x][0];
arr[x].RemoveAt(0);
Console.WriteLine("Memory from " + temp.lb +
" to " + temp.ub + " allocated");
// Store in Dictionary
hm.Add(temp.lb, temp.ub - temp.lb + 1);
return;
}
// If not, search for a larger block
for (i = x + 1; i < arr.Length; i++)
{
if (arr[i].Count == 0)
continue;
// Found a larger block, so break
break;
}
// This would be true if no such block
// was found and array was exhausted
if (i == arr.Length)
{
Console.WriteLine("Sorry, failed to" +
" allocate memory");
return;
}
// Remove the first block
temp = (Pair)arr[i][0];
arr[i].RemoveAt(0);
i--;
// Traverse down the list
for (; i >= x; i--)
{
// Divide the block in two halves
// lower index to half-1
Pair newPair = new Pair(temp.lb, temp.lb +
(temp.ub - temp.lb) / 2);
// half to upper index
Pair newPair2 = new Pair(temp.lb + (temp.ub temp.lb + 1) / 2, temp.ub);
// Add them to next list
// which is tracking blocks of
// smaller size
arr[i].Add(newPair);
arr[i].Add(newPair2);
// Remove a block to continue
// the downward pass
temp = (Pair)arr[i][0];
arr[i].RemoveAt(0);
}
// Finally inform the user
// of the allocated location in memory
Console.WriteLine("Memory from " + temp.lb +
" to " + temp.ub + " allocated");
// Store in Dictionary
hm.Add(temp.lb, temp.ub - temp.lb + 1);
}
void deallocate(int s)
{
// Invalid reference,
// as this was never allocated
if (!hm.ContainsKey(s))
{
Console.WriteLine("Sorry, invalid free request");
return;
}
// Get the list which will track free blocks
// of this size
int x = (int)Math.Ceiling(Math.Log(hm[s]) /
Math.Log(2));
int i, buddyNumber, buddyAddress;
// Add it to the free list
arr[x].Add(new Pair(s, s +
(int)Math.Pow(2, x) - 1));
Console.WriteLine("Memory block from " + s +
" to " + (s +
(int)Math.Pow(2, x) - 1) + " freed");
// Calculate it's buddy number and
// buddyAddress. The base address is
// implicitly 0 in this program,
// so no subtraction is necessary for
// calculating buddyNumber
buddyNumber = s / hm[s];
if (buddyNumber % 2 != 0)
{
buddyAddress = s - (int)Math.Pow(2, x);
}
else
{
buddyAddress = s + (int)Math.Pow(2, x);
}
// Search in the free list for buddy
for (i = 0; i < arr[x].Count; i++)
{
// This indicates the buddy is also free
if (arr[x][i].lb == buddyAddress)
{
// Buddy is the block after block
// with this base address
if (buddyNumber % 2 == 0)
{
// Add to appropriate free list
arr[x + 1].Add(new Pair(s, s + 2 *
((int)Math.Pow(2, x)) - 1));
Console.WriteLine("Coalescing of blocks starting at " +
s + " and " + buddyAddress + " was done");
}
// Buddy is the block before block
// with this base address
else
{
// Add to appropriate free list

arr[x + 1].Add(new Pair(buddyAddress,
buddyAddress +
2 * ((int)Math.Pow(2, x)) - 1));
Console.WriteLine("Coalescing of blocks starting at " +
buddyAddress + " and " + s + " was done");
}
// Remove the individual segements
// as they have coalesced
arr[x].RemoveAt(i);
arr[x].RemoveAt(arr[x].Count - 1);
break;
}
}
// Remove entry from Dictionary
hm.Remove(s);
}
// Driver Code
public static void Main(String []args)
{
int initialMemory = 0;
initialMemory = 128;
Buddy obj = new Buddy(initialMemory);
obj.allocate(16);
obj.allocate(16);
obj.allocate(16);
obj.allocate(16);
obj.deallocate(0);
obj.deallocate(9);
obj.deallocate(32);
obj.deallocate(16);
}
}
// This code is contributed by 29AjayKumar

chevron_right
filter_none
Output:
​
Memory from 0 to 15 allocated​
Memory from 16 to 31 allocated​
Memory from 32 to 47 allocated​
Memory from 48 to 63 allocated​
Memory block from 0 to 15 freed​
Sorry, invalid free request​
Memory block from 32 to 47 freed​
Memory block from 16 to 31 freed​
Coalescing of blocks starting at 0 and 16 was done​

Time Complexity –
As already discussed in set 1, time complexity of allocation is O(log(n)). For deallocation, in the worst case, all the allocated blocks can be of size 1 unit, which will then require O(n) time to scan the list for coalescing.
However, in practice, it is highly unlikely that such an allocation will happen so it is generally much faster than linear time.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : Akanksha_Rai, 29AjayKumar

Source
https://www.geeksforgeeks.org/buddy-memory-allocation-program-set-2-deallocation/
✍
Write a Testimonial

Buddy Memory Allocation Program | Set 1 (Allocation)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Buddy Memory Allocation Program | Set 1 (Allocation) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Buddy System
Question: Write a program to implement the buddy system of memory allocation in Operating Systems.
Explanation –
The buddy system is implemented as follows- A list of free nodes, of all the different possible powers of 2, is maintained at all times (So if total memory size is 1 MB, we’d have 20 free lists to track-one for blocks of size
1 byte, 1 for 2 bytes, next for 4 bytes and so on).
When a request for allocation comes, we look for the smallest block bigger than it. If such a block is found on the free list, the allocation is done (say, the request is of 27 KB and the free list tracking 32 KB blocks has at
least one element in it), else we traverse the free list upwards till we find a big enough block. Then we keep splitting it in two blocks-one for adding to the next free list (of smaller size), one to traverse down the tree till
we reach the target and return the requested memory block to the user. If no such allocation is possible, we simply return null.

Example:
Let us see how the algorithm proceeds by tracking a memory block of size 128 KB. Initially, the free list is: {}, {}, {}, {}, {}, {}, {}, { (0, 127) }
Request: 32 bytes
No such block found, so we traverse up and split the 0-127 block into 0-63, 64-127; we add 64-127 to list tracking 64 byte blocks and pass 0-63 downwards; again it is split into 0-31 and 32-63; since we have found
the required block size, we add 32-63 to list tracking 32 byte blocks and return 0-31 to user.
List is: {}, {}, {}, {}, {}, { (32, 63) }, { (64, 127) }, {}
Request: 7 bytes
No such block found-split block 32-63 into two blocks, namely 32-47 and 48-63; then split 32-47 into 32-39 and 40-47; finally, return 32-39 to user (internal fragmentation of 1 byte occurs)
List is: {}, {}, {}, { (40, 47) }, { (48, 63) }, {}, { (64, 127) }, {}
Request: 64 bytes
Straight up memory segment 64-127 will be allocated as it already exists.
List is: {}, {}, {}, { (40, 47) }, { (48, 63) }, {}, {}, {}
Request: 56 bytes
Result: Not allocated
The result will be as follows:

Figure – Buddy Allocation-128 shows the starting address of next possible block (if main memory size ever increases)
Implementation –
Java
filter_none
edit
close
play_arrow
link
brightness_4
code
import java.io.*;
import java.util.*;
class Buddy {
// Inner class to store lower
// and upper bounds of the allocated memory
class Pair
{
int lb, ub;
Pair(int a, int b)
{
lb = a;
ub = b;
}
}
// Size of main memory
int size;
// Array to track all
// the free nodes of various sizes
ArrayList<Pair> arr[];
// Else compiler will give warning
// about generic array creation
@SuppressWarnings("unchecked")
Buddy(int s)
{
size = s;
// Gives us all possible powers of 2
int x = (int)Math.ceil(Math.log(s) / Math.log(2));
// One extra element is added
// to simplify arithmetic calculations
arr = new ArrayList[x + 1];
for (int i = 0; i <= x; i++)
arr[i] = new ArrayList<>();
// Initially, only the largest block is free
// and hence is on the free list
arr[x].add(new Pair(0, size - 1));
}
void allocate(int s)
{
// Calculate which free list to search to get the
// smallest block large enough to fit the request
int x = (int)Math.ceil(Math.log(s) / Math.log(2));
int i;
Pair temp = null;
// We already have such a block
if (arr[x].size() > 0)
{
// Remove from free list
// as it will be allocated now
temp = (Pair)arr[x].remove(0);
System.out.println("Memory from " + temp.lb
+ " to " + temp.ub + " allocated");
return;
}
// If not, search for a larger block
for (i = x + 1; i < arr.length; i++) {
if (arr[i].size() == 0)
continue;
// Found a larger block, so break
break;
}
// This would be true if no such block was found
// and array was exhausted
if (i == arr.length)
{
System.out.println("Sorry, failed to allocate memory");
return;
}

// Remove the first block
temp = (Pair)arr[i].remove(0);
i--;
// Traverse down the list
for (; i >= x; i--) {
// Divide the block in two halves
// lower index to half-1
Pair newPair = new Pair(temp.lb, temp.lb
+ (temp.ub - temp.lb) / 2);
// half to upper index
Pair newPair2 = new Pair(temp.lb
+ (temp.ub - temp.lb + 1) / 2, temp.ub);
// Add them to next list
// which is tracking blocks of smaller size
arr[i].add(newPair);
arr[i].add(newPair2);
// Remove a block to continue the downward pass
temp = (Pair)arr[i].remove(0);
}
// Finally inform the user
// of the allocated location in memory
System.out.println("Memory from " + temp.lb
+ " to " + temp.ub + " allocated");
}
public static void main(String args[]) throws IOException
{
int initialMemory = 0, val = 0;
// Uncomment the below section for interactive I/O
/*Scanner sc=new Scanner(System.in);
initialMemory = sc.nextInt();
Buddy obj = new Buddy(initialMemory);
while(true)
{
val = sc.nextInt();// Accept the request
if(val <= 0)
break;
obj.allocate(val);// Proceed to allocate
}*/
initialMemory = 128;
// Initialize the object with main memory size
Buddy obj = new Buddy(initialMemory);
obj.allocate(32);
obj.allocate(7);
obj.allocate(64);
obj.allocate(56);
}
}

chevron_right
filter_none
C#
filter_none
edit
close
play_arrow
link
brightness_4
code
using System;
using System.Collections.Generic;
public class Buddy
{
// Inner class to store lower
// and upper bounds of the
// allocated memory
class Pair
{
public int lb, ub;
public Pair(int a, int b)
{
lb = a;
ub = b;
}
}
// Size of main memory
int size;
// Array to track all
// the free nodes of various sizes
List<Pair> []arr;
// Else compiler will give warning
// about generic array creation
Buddy(int s)
{
size = s;
// Gives us all possible powers of 2
int x = (int)Math.Ceiling(Math.Log(s) /
Math.Log(2));
// One extra element is added
// to simplify arithmetic calculations
arr = new List<Pair>[x + 1];
for (int i = 0; i <= x; i++)
arr[i] = new List<Pair>();
// Initially, only the largest block is free
// and hence is on the free list
arr[x].Add(new Pair(0, size - 1));
}
void allocate(int s)
{
// Calculate which free list to search
// to get the smallest block
// large enough to fit the request
int x = (int)Math.Ceiling(Math.Log(s) /
Math.Log(2));
int i;

Pair temp = null;
// We already have such a block
if (arr[x].Count > 0)
{
// Remove from free list
// as it will be allocated now
temp = (Pair)arr[x][0];
arr[x].RemoveAt(0);
Console.WriteLine("Memory from " + temp.lb +
" to " + temp.ub + " allocated");
return;
}
// If not, search for a larger block
for (i = x + 1; i < arr.Length; i++)
{
if (arr[i].Count == 0)
continue;
// Found a larger block, so break
break;
}
// This would be true if no such block
// was found and array was exhausted
if (i == arr.Length)
{
Console.WriteLine("Sorry, failed to" +
" allocate memory");
return;
}
// Remove the first block
temp = (Pair)arr[i][0];
arr[i].RemoveAt(0);
i--;
// Traverse down the list
for (; i >= x; i--)
{
// Divide the block in two halves
// lower index to half-1
Pair newPair = new Pair(temp.lb, temp.lb +
(temp.ub - temp.lb) / 2);
// half to upper index
Pair newPair2 = new Pair(temp.lb + (temp.ub temp.lb + 1) / 2, temp.ub);
// Add them to next list which is
// tracking blocks of smaller size
arr[i].Add(newPair);
arr[i].Add(newPair2);
// Remove a block to continue
// the downward pass
temp = (Pair)arr[i][0];
arr[i].RemoveAt(0);
}
// Finally inform the user
// of the allocated location in memory
Console.WriteLine("Memory from " + temp.lb +
" to " + temp.ub + " allocated");
}
// Driver Code
public static void Main(String []args)
{
int initialMemory = 0;
initialMemory = 128;
// Initialize the object with main memory size
Buddy obj = new Buddy(initialMemory);
obj.allocate(32);
obj.allocate(7);
obj.allocate(64);
obj.allocate(56);
}
}
// This code is contributed by 29AjayKumar

chevron_right
filter_none
Output:
Memory
Memory
Memory
Sorry,

from 0 to 31 allocated​
from 32 to 39 allocated​
from 64 to 127 allocated​
failed to allocate memory

Time Complexity –
If the main memory size is n, we have log(n) number of different powers of 2 and hence log(n) elements in the array (named arr in the code) tracking free lists. To allocate a block, we only need to traverse the array once
upwards and once downwards, hence time complexity is O(2log(n)) or simply O(logn)

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : ak21, 29AjayKumar

Source
https://www.geeksforgeeks.org/buddy-memory-allocation-program-set-1-allocation/
✍
Write a Testimonial

Second Chance (or Clock) Page Replacement Policy
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Second Chance (or Clock) Page Replacement Policy - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​

menu
Prerequisite – Page Replacement Algorithms
Apart from LRU, OPT and FIFO page replacement policies, we also have the second chance/clock page replacement policy. In the Second Chance page replacement policy, the candidate pages for removal are considered
in a round robin matter, and a page that has been accessed between consecutive considerations will not be replaced. The page replaced is the one that, when considered in a round robin matter, has not been accessed since
its last consideration.
It can be implemented by adding a “second chance” bit to each memory frame-every time the frame is considered (due to a reference made to the page inside it), this bit is set to 1, which gives the page a second chance, as
when we consider the candidate page for replacement, we replace the first one with this bit set to 0 (while zeroing out bits of the other pages we see in the process). Thus, a page with the “second chance” bit set to 1 is
never replaced during the first consideration and will only be replaced if all the other pages deserve a second chance too!
Example –
Let’s say the reference string is 0 4 1 4 2 4 3 4 2 4 0 4 1 4 2 4 3 4 and we have 3 frames. Let’s see how the algorithm proceeds by tracking the second chance bit and the pointer.

Initially, all frames are empty so after first 3 passes they will be filled with {0, 4, 1} and the second chance array will be {0, 0, 0} as none has been referenced yet. Also, the pointer will cycle back to 0.
Pass-4: Frame={0, 4, 1}, second_chance = {0, 1, 0} [4 will get a second chance], pointer = 0 (No page needed to be updated so the candidate is still page in frame 0), pf = 3 (No increase in page fault number).
Pass-5: Frame={2, 4, 1}, second_chance= {0, 1, 0} [0 replaced; it’s second chance bit was 0, so it didn’t get a second chance], pointer=1 (updated), pf=4
Pass-6: Frame={2, 4, 1}, second_chance={0, 1, 0}, pointer=1, pf=4 (No change)
Pass-7: Frame={2, 4, 3}, second_chance= {0, 0, 0} [4 survived but it’s second chance bit became 0], pointer=0 (as element at index 2 was finally replaced), pf=5
Pass-8: Frame={2, 4, 3}, second_chance= {0, 1, 0} [4 referenced again], pointer=0, pf=5
Pass-9: Frame={2, 4, 3}, second_chance= {1, 1, 0} [2 referenced again], pointer=0, pf=5
Pass-10: Frame={2, 4, 3}, second_chance= {1, 1, 0}, pointer=0, pf=5 (no change)
Pass-11: Frame={2, 4, 0}, second_chance= {0, 0, 0}, pointer=0, pf=6 (2 and 4 got second chances)
Pass-12: Frame={2, 4, 0}, second_chance= {0, 1, 0}, pointer=0, pf=6 (4 will again get a second chance)
Pass-13: Frame={1, 4, 0}, second_chance= {0, 1, 0}, pointer=1, pf=7 (pointer updated, pf updated)
Page-14: Frame={1, 4, 0}, second_chance= {0, 1, 0}, pointer=1, pf=7 (No change)
Page-15: Frame={1, 4, 2}, second_chance= {0, 0, 0}, pointer=0, pf=8 (4 survived again due to 2nd chance!)
Page-16: Frame={1, 4, 2}, second_chance= {0, 1, 0}, pointer=0, pf=8 (2nd chance updated)
Page-17: Frame={3, 4, 2}, second_chance= {0, 1, 0}, pointer=1, pf=9 (pointer, pf updated)
Page-18: Frame={3, 4, 2}, second_chance= {0, 1, 0}, pointer=1, pf=9 (No change)
In this example, second chance algorithm does as well as the LRU method, which is much more expensive to implement in hardware.
More Examples –
Input: 2 5 10 1 2 2 6 9 1 2 10 2 6 1 2 1 6 9 5 1​
3​
Output: 13​
​
Input: 2 5 10 1 2 2 6 9 1 2 10 2 6 1 2 1 6 9 5 1​
4​
Output: 11

Algorithm –
Create an array frames to track the pages currently in memory and another Boolean array second_chance to track whether that page has been accessed since it’s last replacement (that is if it deserves a second chance or
not) and a variable pointer to track the target for replacement.
1. Start traversing the array arr. If the page already exists, simply set its corresponding element in second_chance to true and return.
2. If the page doesn’t exist, check whether the space pointed to by pointer is empty (indicating cache isn’t full yet) – if so, we will put the element there and return, else we’ll traverse the array arr one by one
(cyclically using the value of pointer), marking all corresponding second_chance elements as false, till we find a one that’s already false. That is the most suitable page for replacement, so we do so and return.
3. Finally, we report the page fault count.

C++
filter_none
edit
close
play_arrow
link
brightness_4
code
// CPP program to find largest in an array
// without conditional/bitwise/ternary/ operators
// and without library functions.
#include<iostream>
#include<cstring>
#include<sstream>
using namespace std;
// If page found, updates the second chance bit to true
static bool findAndUpdate(int x,int arr[],
bool second_chance[],int frames)
{
int i;
for(i = 0; i < frames; i++)
{
if(arr[i] == x)
{
// Mark that the page deserves a second chance
second_chance[i] = true;
// Return 'true', that is there was a hit
// and so there's no need to replace any page
return true;

}
}
// Return 'false' so that a page for replacement is selected
// as he reuested page doesn't exist in memory
return false;
}
// Updates the page in memory and returns the pointer
static int replaceAndUpdate(int x,int arr[],
bool second_chance[],int frames,int pointer)
{
while(true)
{
// We found the page to replace
if(!second_chance[pointer])
{
// Replace with new page
arr[pointer] = x;
// Return updated pointer
return (pointer + 1) % frames;
}
// Mark it 'false' as it got one chance
// and will be replaced next time unless accessed again
second_chance[pointer] = false;
//Pointer is updated in round robin manner
pointer = (pointer + 1) % frames;
}
}
static void printHitsAndFaults(string reference_string,
int frames)
{
int pointer, i, l=0, x, pf;
//initially we consider frame 0 is to be replaced
pointer = 0;
//number of page faults
pf = 0;
// Create a array to hold page numbers
int arr[frames];
// No pages initially in frame,
// which is indicated by -1
memset(arr, -1, sizeof(arr));
// Create second chance array.
// Can also be a byte array for optimizing memory
bool second_chance[frames];
// Split the string into tokens,
// that is page numbers, based on space
string str[100];
string word = "";
for (auto x : reference_string)
{
if (x == ' ')
{
str[l]=word;
word = "";
l++;
}
else
{
word = word + x;
}
}
str[l] = word;
l++;
// l=the length of array
for(i = 0; i < l; i++)
{
x = stoi(str[i]);
// Finds if there exists a need to replace
// any page at all
if(!findAndUpdate(x,arr,second_chance,frames))
{
// Selects and updates a victim page
pointer = replaceAndUpdate(x,arr,
second_chance,frames,pointer);
// Update page faults
pf++;
}
}
cout << "Total page faults were " << pf << "\n";
}
// Driver code
int main()
{
string reference_string = "";
int frames = 0;
// Test 1:
reference_string = "0 4 1 4 2 4 3 4 2 4 0 4 1 4 2 4 3 4";
frames = 3;
// Output is 9
printHitsAndFaults(reference_string,frames);
// Test 2:
reference_string = "2 5 10 1 2 2 6 9 1 2 10 2 6 1 2 1 6 9 5 1";
frames = 4;
// Output is 11
printHitsAndFaults(reference_string,frames);
return 0;
}
// This code is contributed by NikhilRathor

chevron_right
filter_none
Java
filter_none
edit
close
play_arrow
link

brightness_4
code
// Java program to find largest in an array
// without conditional/bitwise/ternary/ operators
// and without library functions.
import java.util.*;
import java.io.*;
class secondChance
{
public static void main(String args[])throws IOException
{
String reference_string = "";
int frames = 0;
//Test 1:
reference_string = "0 4 1 4 2 4 3 4 2 4 0 4 1 4 2 4 3 4";
frames = 3;
//Output is 9
printHitsAndFaults(reference_string,frames);
//Test 2:
reference_string = "2 5 10 1 2 2 6 9 1 2 10 2 6 1 2 1 6 9 5 1";
frames = 4;
//Output is 11
printHitsAndFaults(reference_string,frames);
}
//If page found, updates the second chance bit to true
static boolean findAndUpdate(int x,int arr[],
boolean second_chance[],int frames)
{
int i;
for(i = 0; i < frames; i++)
{
if(arr[i] == x)
{
//Mark that the page deserves a second chance
second_chance[i] = true;
//Return 'true', that is there was a hit
//and so there's no need to replace any page
return true;
}
}
//Return 'false' so that a page for replacement is selected
//as he reuested page doesn't exist in memory
return false;
}
//Updates the page in memory and returns the pointer
static int replaceAndUpdate(int x,int arr[],
boolean second_chance[],int frames,int pointer)
{
while(true)
{
//We found the page to replace
if(!second_chance[pointer])
{
//Replace with new page
arr[pointer] = x;
//Return updated pointer
return (pointer+1)%frames;
}
//Mark it 'false' as it got one chance
// and will be replaced next time unless accessed again
second_chance[pointer] = false;
//Pointer is updated in round robin manner
pointer = (pointer+1)%frames;
}
}
static void printHitsAndFaults(String reference_string,
int frames)
{
int pointer,i,l,x,pf;
//initially we consider frame 0 is to be replaced
pointer = 0;
//number of page faults
pf = 0;
//Create a array to hold page numbers
int arr[] = new int[frames];
//No pages initially in frame,
//which is indicated by -1
Arrays.fill(arr,-1);
//Create second chance array.
//Can also be a byte array for optimizing memory
boolean second_chance[] = new boolean[frames];
//Split the string into tokens,
//that is page numbers, based on space
String str[] = reference_string.split(" ");
//get the length of array
l = str.length;
for(i = 0; i<l; i++)
{
x = Integer.parseInt(str[i]);
//Finds if there exists a need to replace
//any page at all
if(!findAndUpdate(x,arr,second_chance,frames))
{
//Selects and updates a victim page
pointer = replaceAndUpdate(x,arr,
second_chance,frames,pointer);
//Update page faults
pf++;
}
}
System.out.println("Total page faults were "+pf);
}
}

chevron_right

filter_none
C#
filter_none
edit
close
play_arrow
link
brightness_4
code
// C# program to find largest in an array
// without conditional/bitwise/ternary/ operators
// and without library functions.
using System;
public class secondChance
{
public static void Main()
{
String reference_string = "";
int frames = 0;
// Test 1:
reference_string = "0 4 1 4 2 4 3 4 2 4 0 4 1 4 2 4 3 4";
frames = 3;
// Output is 9
printHitsAndFaults(reference_string,frames);
// Test 2:
reference_string = "2 5 10 1 2 2 6 9 1 2 10 2 6 1 2 1 6 9 5 1";
frames = 4;
// Output is 11
printHitsAndFaults(reference_string,frames);
}
// If page found, updates the second chance bit to true
static bool findAndUpdate(int x,int []arr,
bool []second_chance,int frames)
{
int i;
for(i = 0; i < frames; i++)
{
if(arr[i] == x)
{
//Mark that the page deserves a second chance
second_chance[i] = true;
//Return 'true', that is there was a hit
//and so there's no need to replace any page
return true;
}
}
// Return 'false' so that a page
// for replacement is selected
// as he reuested page doesn't
// exist in memory
return false;
}
// Updates the page in memory
// and returns the pointer
static int replaceAndUpdate(int x,int []arr,
bool []second_chance,int frames,int pointer)
{
while(true)
{
//We found the page to replace
if(!second_chance[pointer])
{
//Replace with new page
arr[pointer] = x;
//Return updated pointer
return (pointer+1)%frames;
}
//Mark it 'false' as it got one chance
// and will be replaced next time unless accessed again
second_chance[pointer] = false;
//Pointer is updated in round robin manner
pointer = (pointer + 1) % frames;
}
}
static void printHitsAndFaults(String reference_string,
int frames)
{
int pointer, i, l, x, pf;
// initially we consider
// frame 0 is to be replaced
pointer = 0;
// number of page faults
pf = 0;
// Create a array to hold page numbers
int []arr = new int[frames];
// No pages initially in frame,
// which is indicated by -1
for(int s = 0;s<frames;s++)
arr[s]=-1;
//Create second chance array.
//Can also be a byte array for optimizing memory
bool []second_chance = new bool[frames];
//Split the string into tokens,
//that is page numbers, based on space
String []str = reference_string.Split();
//get the length of array
l = str.Length;
for(i = 0; i < l; i++)
{
x = int.Parse(str[i]);

//Finds if there exists a need to replace
//any page at all
if(!findAndUpdate(x,arr,second_chance,frames))
{
//Selects and updates a victim page
pointer = replaceAndUpdate(x,arr,
second_chance,frames,pointer);
//Update page faults
pf++;
}
}
Console.WriteLine("Total page faults were "+pf);
}
}
// This code has been contributed by 29AjayKumar

chevron_right
filter_none
Output:
​
Total page faults were 9​
Total page faults were 11

Note:
1. The arrays arr and second_chance can be replaced and combined together via a hashmap (with element as key, true/false as value) to speed up search.
2. Time complexity of this method is O(Number_of_frames*reference_string_length) or O(mn) but since number of frames will be a constant in an Operating System (as main memory size is fixed), it is simply
O(n) [Same as hashmap approach, but that will have lower constants]
3. Second chance algorithm may suffer from Belady’s Anomaly.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : 29AjayKumar, NikhilRathor

Source
https://www.geeksforgeeks.org/second-chance-or-clock-page-replacement-policy/
✍
Write a Testimonial

Longest Remaining Time First (LRTF) CPU Scheduling Program
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Longest Remaining Time First (LRTF) CPU Scheduling Program - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – CPU Scheduling | Longest Remaining Time First (LRTF) algorithm
We have given some process with arrival time and Burst Time and we have to find the completion time (CT), Turn Around Time(TAT), Average Turn Around Time (Avg TAT), Waiting Time(WT), Average Waiting
Time (AWT) for the given processes.
Example: Consider the following table of arrival time and burst time for four processes P1, P2, P3 and P4.
Process
P1
P2
P3
p4

Arrival time
1 ms
2 ms
3 ms
4 ms

Burst Time​
2 ms​
4 ms​
6 ms​
8 ms

Gantt chart will be as following below,

Since, complietion time (CT) can be directly determined by Gantt chart, and
Turn Around Time (TAT)​
= (Complition Time) - (Arival Time)​
​
Also, Waiting Time (WT)​
= (Turn Around Time) - (Burst Time)

Therefore,
Output:
Total Turn Around Time = 68 ms​
So, Average Turn Around Time = 68/4 = 17.00 ms​
​
And, Total Waiting Time = 48 ms​
So, Average Waiting Time = 12.00 ms

Algorithm –
Step-1: Create a structure of process containing all necessary fields like AT (Arrival Time), BT(Burst Time), CT(Completion Time), TAT(Turn Around Time), WT(Waiting Time).
Step-2: Sort according to the AT;
Step-3: Find the process having Largest Burst Time and execute for each single unit. Increase the total time by 1 and reduce the Burst Time of that process with 1.
Step-4: When any process have 0 BT left, then update the CT(Completion Time of that process CT will be Total Time at that time).
Step-2: After calculating the CT for each process, find TAT and WT.
(TAT = CT - AT) ​
(WT = TAT - BT)

Implementation of Algorithm –

C++
filter_none
edit
close
play_arrow
link
brightness_4
code
#include <bits/stdc++.h>
using namespace std;
// creating a structure of a process
struct process {
int processno;
int AT;
int BT;
// for backup purpose to print in last
int BTbackup;
int WT;
int TAT;
int CT;
};
// creating a structe of 4 processes
struct process p[4];
// variable to find the total time
int totaltime = 0;
int prefinaltotal = 0;
// comparator function for sort()
bool compare(process p1, process p2)
{
// compare the Arrival time of two processes
return p1.AT < p2.AT;
}
// finding the largest Arrival Time among all the available
// process at that time
int findlargest(int at)
{
int max = 0, i;
for (i = 0; i < 4; i++) {
if (p[i].AT <= at) {
if (p[i].BT > p[max].BT)
max = i;
}
}
// returning the index of the process having the largest BT
return max;
}
// function to find the completion time of each process
int findCT()
{
int index;
int flag = 0;
int i = p[0].AT;
while (1) {
if (i <= 4) {
index = findlargest(i);
}
else
index = findlargest(4);
cout << "Process executing at time " << totaltime
<< " is: P" << index + 1 << "\t";
p[index].BT -= 1;
totaltime += 1;
i++;
if (p[index].BT == 0) {
p[index].CT = totaltime;
cout << " Process P" << p[index].processno
<< " is completed at " << totaltime;
}
cout << endl;
// loop termination condition
if (totaltime == prefinaltotal)
break;
}
}
int main()
{
int i;
// initializing the process number
for (i = 0; i < 4; i++) {
p[i].processno = i + 1;
}
// cout<<"arrival time of 4 processes : ";
for (i = 0; i < 4; i++) // taking AT
{
p[i].AT = i + 1;
}
// cout<<" Burst time of 4 processes : ";
for (i = 0; i < 4; i++) {
// assigning {2, 4, 6, 8} as Burst Time to the processes
// backup for displaying the output in last
// calculating total required time for terminating
// the function().
p[i].BT = 2 * (i + 1);
p[i].BTbackup = p[i].BT;
prefinaltotal += p[i].BT;
}
// displaying the process before executing
cout << "PNo\tAT\tBT\n";
for (i = 0; i < 4; i++) {
cout << p[i].processno << "\t";
cout << p[i].AT << "\t";
cout << p[i].BT << "\t";
cout << endl;
}
cout << endl;
// soritng process according to Arrival Time
sort(p, p + 4, compare);
// calculating initial time when execution starts

totaltime += p[0].AT;
// calculating to terminate loop
prefinaltotal += p[0].AT;
findCT();
int totalWT = 0;
int totalTAT = 0;
for (i = 0; i < 4; i++) {
// since, TAT = CT - AT
p[i].TAT = p[i].CT - p[i].AT;
p[i].WT = p[i].TAT - p[i].BTbackup;
// finding total waiting time
totalWT += p[i].WT;
// finding total turn around time
totalTAT += p[i].TAT;
}
cout << "After execution of all processes ... \n";
// after all process executes
cout << "PNo\tAT\tBT\tCT\tTAT\tWT\n";
for (i =
cout
cout
cout
cout
cout
cout
cout
}

0;
<<
<<
<<
<<
<<
<<
<<

i < 4; i++) {
p[i].processno << "\t";
p[i].AT << "\t";
p[i].BTbackup << "\t";
p[i].CT << "\t";
p[i].TAT << "\t";
p[i].WT << "\t";
endl;

cout << endl;
cout << "Total TAT = " << totalTAT << endl;
cout << "Average TAT = " << totalTAT / 4.0 << endl;
cout << "Total WT = " << totalWT << endl;
cout << "Average WT = " << totalWT / 4.0 << endl;
return 0;
}

chevron_right
filter_none
Python3
filter_none
edit
close
play_arrow
link
brightness_4
code
# Python3 program to implement
# Longest Remaining Time First
# creating a structure of 4 processes
p = []
for i in range(4):
p.append([0, 0, 0, 0, 0, 0, 0])
# variable to find the total time
totaltime = 0
prefinaltotal = 0
# finding the largest Arrival Time
# among all the available process
# at that time
def findlargest(at):
max = 0
for i in range(4):
if (p[i][1] <= at):
if (p[i][2] > p[max][2]) :
max = i
# returning the index of the
# process having the largest BT
return max
# function to find the completion
# time of each process
def findCT(totaltime):
index = 0
flag = 0
i = p[0][1]
while (1):
if (i <= 4):
index = findlargest(i)
else:
index = findlargest(4)
print("Process execute at time ",
totaltime, end = " ")
print(" is: P", index + 1,
sep = "", end = " ")
p[index][2] -= 1
totaltime += 1
i += 1
if (p[index][2] == 0):
p[index][6] = totaltime
print("Process P", p[index][0],
sep = "", end = " ")
print(" is completed at ",
totaltime, end = " ")
print()
# loop termination condition
if (totaltime == prefinaltotal):
break
# Driver code
if __name__ =="__main__":
# initializing the process number
for i in range(4):
p[i][0] = i + 1
for i in range(4): # taking AT
p[i][1] = i + 1
for i in range(4):
# assigning 2, 4, 6, 8 as Burst Time
# to the processes backup for displaying
# the output in last calculating total
# required time for terminating the function().
p[i][2] = 2 * (i + 1)
p[i][3] = p[i][2]
prefinaltotal += p[i][2]
# displaying the process before executing

print("PNo\tAT\tBT")
for i in range(4):
print(p[i][0], "\t",
p[i][1], "\t", p[i][2])
print()
# soritng process according to Arrival Time
p = sorted(p, key = lambda p:p[1])
# calculating initial time when
# execution starts
totaltime += p[0][1]
# calculating to terminate loop
prefinaltotal += p[0][1]
findCT(totaltime)
totalWT = 0
totalTAT = 0
for i in range(4):
# since, TAT = CT - AT
p[i][5] = p[i][6]- p[i][1]
p[i][4] = p[i][5] - p[i][3]
# finding total waiting time
totalWT += p[i][4]
# finding total turn around time
totalTAT += p[i][5]
print("\nAfter execution of all processes ... ")
# after all process executes
print("PNo\tAT\tBT\tCT\tTAT\tWT" )
for i in range(4):
print(p[i][0], "\t", p[i][1], "\t",
p[i][3], "\t", end = " ")
print(p[i][6], "\t",
p[i][5], "\t", p[i][4])
print()
print("Total TAT = ", totalTAT)
print("Average TAT = ", totalTAT / 4.0)
print("Total WT = ", totalWT)
print("Average WT = ", totalWT / 4.0)
# This code is contributed by
# Shubham Singh(SHUBHAMSINGH10)

chevron_right
filter_none
Output:
PNo
AT
BT​
1
1
2
​
2
2
4
​
3
3
6
​
4
4
8
​
​
Process executing at time 1 is: P1
​
Process executing at time 2 is: P2
​
Process executing at time 3 is: P3
​
Process executing at time 4 is: P4
​
Process executing at time 5 is: P4
​
Process executing at time 6 is: P4
​
Process executing at time 7 is: P3
​
Process executing at time 8 is: P4
​
Process executing at time 9 is: P3
​
Process executing at time 10 is: P4
​
Process executing at time 11 is: P2
​
Process executing at time 12 is: P3
​
Process executing at time 13 is: P4
​
Process executing at time 14 is: P2
​
Process executing at time 15 is: P3
​
Process executing at time 16 is: P4
​
Process executing at time 17 is: P1
Process
Process executing at time 18 is: P2
Process
Process executing at time 19 is: P3
Process
Process executing at time 20 is: P4
Process
After execution of all processes ... ​
PNo
AT
BT
CT
TAT
WT​
1
1
2
18
17
15
​
2
2
4
19
17
13
​
3
3
6
20
17
11
​
4
4
8
21
17
9
​
​
Total TAT = 68​
Average TAT = 17​
Total WT = 48​
Average WT = 12

P1
P2
P3
P4

is
is
is
is

completed
completed
completed
completed

at
at
at
at

18​
19​
20​
21​

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : SHUBHAMSINGH10

Source
https://www.geeksforgeeks.org/longest-remaining-time-first-lrtf-cpu-scheduling-program/
✍
Write a Testimonial

Longest Remaining Time First (LRTF) CPU Scheduling Algorithm
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Longest Remaining Time First (LRTF) CPU Scheduling Algorithm - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Process Management | CPU Scheduling
This is a pre-emptive version of Longest Job First (LJF) scheduling algorithm. In this scheduling algorithm, we find the process with the maximum remaining time and then process it. We check for the maximum
remaining time after some interval of time(say 1 unit each) to check if another process having more Burst Time arrived up to that time.
Procedure:
Step-1: First, sort the processes in increasing order of their Arrival Time.
Step-2: Choose the process having least arrival time but with most Burst Time. Then process it for 1 unit. Check if any other process arrives upto that time of execution or not.
Step-3: Repeat the above both steps until execute all the processes.
Example-1: Consider the following table of arrival time and burst time for four processes P1, P2, P3 and P4.

Process
P1
P2
P3
P4

Arrival time
1 ms
2 ms
3 ms
4 ms

Burst Time​
2 ms​
4 ms​
6 ms​
8 ms

Working: (for input 1):
1.
2.
3.
4.

At t = 1, Available Process : P1. So, select P1 and execute 1 ms.
At t = 2, Available Process : P1, P2. So, select P2 and execute 1 ms (since BT(P1)=1 which is less than BT(P2) = 4)
At t = 3, Available Process : P1, P2, P3. So, select P3 and execute 1 ms (since, BT(P1) = 1 , BT(P2) = 3 , BT(P3) = 6).
Repeat the above steps until the execution of all processes.

Note that CPU will be idle for 0 to 1 unit time since there is no process available in the given interval.
Gantt chart will be as following below,

Since, complietion time (CT) can be directly determined by Gantt chart, and
Turn Around Time (TAT)​
= (Complition Time) - (Arival Time)​
​
Also, Waiting Time (WT)​
= (Turn Around Time) - (Burst Time)

Therefore, final table look like,

Output:
Total Turn Around Time = 68 ms​
So, Average Turn Around Time = 68/4 = 17.00 ms​
​
And, Total Waiting Time = 48 ms​
So Average Waiting Time = 48/4 = 12.00 ms

Example-2: Consider the following table of arrival time and burst time for four processes P1, P2, P3,P4 and P5.
Process
P1
P2
P3
P4
P5

Arrival time
0 ms
0 ms
2 ms
3 ms
4 ms

Burst Time​
2 ms​
3 ms​
2 ms​
5 ms ​
4 ms

Similarly example-1, Gantt chart for this example,

Since, complietion time (CT) can be directly determined by Gantt chart, and
Turn Around Time (TAT)​
= (Complition Time) - (Arival Time)​
​
Also, Waiting Time (WT)​
= (Turn Around Time) - (Burst Time)

Therefore, final table look like,

Output:
Total Turn Around Time = 61 ms​

So, Average Turn Around Time = 61/5 = 12.20 ms​
​
And, Total Waiting Time = 45 ms​
So, Average Waiting Time = 45/5 = 9.00 ms

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/longest-remaining-time-first-lrtf-cpu-scheduling-algorithm/
✍
Write a Testimonial

Boot Block in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Boot Block in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Basically for a computer to start running to get an instance when it is powered up or rebooted it need to have an initial program to run. And this initial program which is known as bootstrap need to be simple. It must
initialize all aspects of the system, from CPU registers to device controllers and the contents of the main memory and then starts the operating system.
To do this job the bootstrap program basically finds the operating system kernel on disk and then loads the kernel into memory and after this, it jumps to the initial address to begin the operating-system execution.
Why ROM:
For most of today’s computer bootstrap is stored in Read Only Memory (ROM).

1. This location is good for storage because this place doesn’t require initialization and moreover location here it is fixed so that processor can start executing when powered up or reset.
2. ROM is basically read-only memory and hence it cannot be affected by the computer virus.
The problem is that changing the bootstrap code basically requires changes in the ROM hardware chips.Because of this reason, most system nowadays has the tiny bootstrap loader program in the boot whose only job is to
bring the full bootstrap program from the disk. Through this now we are able to change the full bootstrap program easily and the new version can be easily written onto the disk.
Full bootstrap program is stored in the boot blocks at a fixed location on the disk. A disk which has a boot partition is called a boot disk. The code in the boot ROM basically instructs the read controller to read the boot
blocks into the memory and then starts the execution of code. The full bootstrap program is more complex than the bootstrap loader in the boot ROM, It is basically able to load the complete OS from a non-fixed location
on disk to start the operating system running. Even though the complete bootstrap program is very small.
Example:
Let us try to understand this using an example of the boot process in Windows 2000.
The Windows 2000 basically stores its boot code in the first sector on the hard disk. Moreover, Windows 2000 allows the hard disk to be divided into one or more partition. In this one partition is basically identified as the
boot partition which basically contains the operating system and the device drivers.
Booting in Windows 2000 starts by running the code that is placed in the system’s ROM memory. This code basically directs the system to read code directly from MBR. In addition to this boot code also contain the table
which lists the partition for the hard disk and also a flag which basically indicates which partition is to be boot from the system. Once the system identifies the boot partition it reads the first sector from the memory which
is basically known as boot sector and continue the process with the remainder of the boot process which basically includes loading of various system services.
The following figure shows the Booting from disk in Windows 2000.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/boot-block-in-operating-system/
✍
Write a Testimonial

Preemptive and Non-Preemptive Scheduling
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Preemptive and Non-Preemptive Scheduling - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​

x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – CPU Scheduling
1. Preemptive Scheduling:
Preemptive scheduling is used when a process switches from running state to ready state or from waiting state to ready state. The resources (mainly CPU cycles) are allocated to the process for the limited amount of time
and then is taken away, and the process is again placed back in the ready queue if that process still has CPU burst time remaining. That process stays in ready queue till it gets next chance to execute.
Algorithms based on preemptive scheduling are: Round Robin (RR),Shortest Remaining Time First (SRTF), Priority (preemptive version), etc.

2. Non-Preemptive Scheduling:
Non-preemptive Scheduling is used when a process terminates, or a process switches from running to waiting state. In this scheduling, once the resources (CPU cycles) is allocated to a process, the process holds the CPU
till it gets terminated or it reaches a waiting state. In case of non-preemptive scheduling does not interrupt a process running CPU in middle of the execution. Instead, it waits till the process complete its CPU burst time and
then it can allocate the CPU to another process.
Algorithms based on non-preemptive scheduling are: Shortest Job First (SJF basically non preemptive) and Priority (non preemptive version), etc.

Key Differences Between Preemptive and Non-Preemptive Scheduling:
1. In preemptive scheduling the CPU is allocated to the processes for the limited time whereas in Non-preemptive scheduling, the CPU is allocated to the process till it terminates or switches to waiting state.
2. The executing process in preemptive scheduling is interrupted in the middle of execution when higher priority one comes whereas, the executing process in non-preemptive scheduling is not interrupted in the middle
of execution and wait till its execution.
3. In Preemptive Scheduling, there is the overhead of switching the process from ready state to running state, vise-verse, and maintaining the ready queue. Whereas in case of non-preemptive scheduling has no
overhead of switching the process from running state to ready state.
4. In preemptive scheduling, if a high priority process frequently arrives in the ready queue then the process with low priority has to wait for a long, and it may have to starve. On the other hands, in the non-preemptive
scheduling, if CPU is allocated to the process having larger burst time then the processes with small burst time may have to starve.
5. Preemptive scheduling attain flexible by allowing the critical processes to access CPU as they arrive into the ready queue, no matter what process is executing currently. Non-preemptive scheduling is called rigid as
even if a critical process enters the ready queue the process running CPU is not disturbed.
6. The Preemptive Scheduling has to maintain the integrity of shared data that’s why it is cost associative as it which is not the case with Non-preemptive Scheduling.
Comparison Chart:
Paramenter

PREEMPTIVE SCHEDULING

Basic

In this resources(CPU Cycle) are allocated to a process for a limited time.

Interrupt

Process can be interrupted in between.
If a process having high priority frequently arrives in the ready queue, low priority
process may starve.
It has overheads of scheduling the processes.
flexible
cost associated

Starvation
Overhead
Flexibility
Cost

NON-PREEMPTIVE SCHEDULING
Once resources(CPU Cycle) are allocated to a process, the process holds it till it completes its burst time or
switches to waiting state.
Process can not be interrupted untill it terminates itself or its time is up.
If a process with long burst time is running CPU, then later coming process with less CPU burst time may starve.
It does not have overheads.
rigid
no cost associated

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : niharika3

Source
https://www.geeksforgeeks.org/preemptive-and-non-preemptive-scheduling/
✍
Write a Testimonial

Kernel I/O Subsystem in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Kernel I/O Subsystem in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Microkernel
The kernel provides many services related to I/O. Several services such as scheduling, caching, spooling, device reservation, and error handling – are provided by the kernel, s I/O subsystem built on the hardware and
device-driver infrastructure. The I/O subsystem is also responsible for protecting itself from the errant processes and malicious users.
1. I/O Scheduling –
To schedule a set of I/O request means to determine a good order in which to execute them. The order in which application issues the system call are the best choice. Scheduling can improve the overall performance
of the system, can share device access permission fairly to all the processes, reduce the average waiting time, response time, turnaround time for I/O to complete.
OS developers implement scheduling by maintaining a wait queue of the request for each device. When an application issue a blocking I/O system call, The request is placed in the queue for that device. The I/O
scheduler rearrange the order to improve the efficiency of the system.
2. Buffering –
A buffer is a memory area that stores data being transferred between two devices or between a device and an application. Buffering is done for three reasons.
1. First is to cope with a speed mismatch between producer and consumer of a data stream.
2. The second use of buffering is to provide adaptation for that have different data-transfer size.
3. Third use of buffering is to support copy semantics for the application I/O. “copy semantic ” means, suppose that an application wants to write data on disk that is stored in its buffer. it calls the write() system
s=call, providing a pointer to the buffer and the integer specify the number of bytes to write.
After the system call returns, what happens if the application of the buffer changes the content of the buffer? With copy semantic, the version of the data written to the disk is guaranteed to be the version at the time
of the application system call.
3. Caching –
A cache is a region of fast memory that holds copy of data. Access to the cached copy is much easier than the original file. For instance, the instruction of the currently running process is stored on the disk, cached in
physical memory, and copies again in the CPU’s secondary and primary cache.
The main difference between a buffer and a cache is that a buffer may hold only the existing copy of data item, while cache, by definition, holds a copy on faster storage of an item that resides elsewhere.
4. Spooling and Device Reservation –
A spool is a buffer that holds the output of a device, such as a printer that cannot accept interleaved data stream. Although a printer can serve only one job at a time, several applications may wish to print their output
concurrently, without having their output mixes together.
The OS solves this problem by preventing all output continuing to the printer. The output of all application is spooled in a separate disk file. When an application finishes printing then the spooling system queues the
corresponding spool file for output to the printer.
5. Error Handling –
An Os that uses protected memory can guard against many kinds of hardware and application errors, so that a complete system failure is not the usual result of each minor mechanical glitch, Devices, and I/O
transfers can fail in many ways, either for transient reasons, as when a network becomes overloaded or for permanent reasons, as when a disk controller becomes defective.
6. I/O Protection –
Errors and the issue of protection are closely related. A user process may attempt to issue illegal I/O instruction to disrupt the normal function of a system. we can use the various mechanisms to ensure that such
disruption cannot take place in the system.
To prevent illegal I/O access, we define all I/O instruction to be privileged instructions. The user cannot issue I/O instruction directly.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : shreyashagrawal

Source
https://www.geeksforgeeks.org/kernel-i-o-subsystem-in-operating-system/
✍
Write a Testimonial

File Access Methods in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ File Access Methods in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​

x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – File Systems
When a file is used, information is read and accessed into computer memory and there are several ways to access this information of the file. Some systems provide only one access method for files. Other systems, such as
those of IBM, support many access methods, and choosing the right one for a particular application is a major design problem.
There are three ways to access a file into a computer system: Sequential-Access, Direct Access, Index sequential Method.
1. Sequential Access –
It is the simplest access method. Information in the file is processed in order, one record after the other. This mode of access is by far the most common; for example, editor and compiler usually access the file in this
fashion.
Read and write make up the bulk of the operation on a file. A read operation -read next- read the next position of the file and automatically advance a file pointer, which keeps track I/O location. Similarly, for the
writewrite next append to the end of the file and advance to the newly written material.
Key points:
Data is accessed one record right after another record in an order.
When we use read command, it move ahead pointer by one
When we use write command, it will allocate memory and move the pointer to the end of the file
Such a method is reasonable for tape.
2. Direct Access –
Another method is direct access method also known as relative access method. A filed-length logical record that allows the program to read and write record rapidly. in no particular order. The direct access is based
on the disk model of a file since disk allows random access to any file block. For direct access, the file is viewed as a numbered sequence of block or record. Thus, we may read block 14 then block 59 and then we
can write block 17. There is no restriction on the order of reading and writing for a direct access file.
A block number provided by the user to the operating system is normally a relative block number, the first relative block of the file is 0 and then 1 and so on.
3. Index sequential method –
It is the other method of accessing a file which is built on the top of the direct access method. These methods construct an index for the file. The index, like an index in the back of a book, contains the pointer to the
various blocks. To find a record in the file, we first search the index and then by the help of pointer we access the file directly.
Key points:
It is built on top of Sequential access.
It control the pointer by using index.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : Gokohan

Source
https://www.geeksforgeeks.org/file-access-methods-in-operating-system/
✍
Write a Testimonial

Structures of Directory in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Structures of Directory in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
A directory is a container that is used to contain folders and file. It organizes files and folders into a hierarchical manner.

There are several logical structures of a directory, these are given below.

1. Single-level directory –
Single level directory is simplest directory structure.In it all files are contained in same directory which make it easy to support and understand.
A single level directory has a significant limitation, however, when the number of files increases or when the system has more than one user. Since all the files are in the same directory, they must have the unique
name . if two users call their dataset test, then the unique name rule violated.

Advantages:
Since it is a single directory, so its implementation is very easy.
If the files are smaller in size, searching will become faster.
The operations like file creation, searching, deletion, updating are very easy in such a directory structure.
Disadvantages:
There may chance of name collision because two files can not have the same name.
Searching will become time taking if the directory is large.
In this can not group the same type of files together.
2. Two-level directory –
As we have seen, a single level directory often leads to confusion of files names among different users. the solution to this problem is to create a separate directory for each user.
In the two-level directory structure, each user has there own user files directory (UFD) . The UFDs has similar structures, but each lists only the files of a single user. system’s master file directory (MFD) is searches
whenever a new user id=s logged in. The MFD is indexed by username or account number, and each entry points to the UFD for that user.

Advantages:
We can give full path like /User-name/directory-name/.
Different users can have same directory as well as file name.
Searching of files become more easy due to path name and user-grouping.
Disadvantages:
A user is not allowed to share files with other users.
Still it not very scalable, two files of the same type cannot be grouped together in the same user.
3. Tree-structured directory –
Once we have seen a two-level directory as a tree of height 2, the natural generalization is to extend the directory structure to a tree of arbitrary height.
This generalization allows the user to create there own subdirectories and to organize on their files accordingly.

A tree structure is the most common directory structure. The tree has a root directory, and every file in the system have a unique path.
Advantages:
Very generalize, since full path name can be given.
Very scalable, the probability of name collision is less.
Searching becomes very easy, we can use both absolute path as well as relative.
Disadvantages:
Every file does not fit into the hierarchical model, files may be saved into multiple directories.
We can not share files.
It is inefficient, because accessing a file may go under multiple directories.
4. Acyclic graph directory –
An acyclic graph is a graph with no cycle and allows to share subdirectories and files. The same file or subdirectories may be in two different directories. It is a natural generalization of the tree-structured directory.
It is used in the situation like when two programmers are working on a joint project and they need to access files. The associated files are stored in a subdirectory, separating them from other projects and files of
other programmers, since they are working on a joint project so they want the subdirectories to be into their own directories. The common subdirectories should be shared. So here we use Acyclic directories.
It is the point to note that shared file is not the same as copy file . If any programmer makes some changes in the subdirectory it will reflect in both subdirectories.

Advantages:
We can share files.
Searching is easy due to different-different paths.
Disadvantages:
We share the files via linking, in case of deleting it may create the problem,
If the link is softlink then after deleting the file we left with a dangling pointer.
In case of hardlink, to delete a file we have to delete all the reference associated with it.
5. General graph directory structure –
In general graph directory structure, cycles are allowed within a directory structure where multiple directories can be derived from more than one parent directory.
The main problem with this kind of directory structure is to calculate total size or space that has been taken by the files and directories.

Advantages:
It allows cycles.

It is more flexible than other directories structure.
Disadvantages:
It is more costly than others.
It needs garbage collection.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : shubham_singh, nehasharma93517, MiranJunaidi

Source
https://www.geeksforgeeks.org/structures-of-directory-in-operating-system/
✍
Write a Testimonial

Non-Contiguous Allocation in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Non-Contiguous Allocation in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Variable Partitioning, Fixed Partitioning
Paging and Segmentation are the two ways which allow a process’s physical address space to be non-contiguous. It has advantage of reducing memory wastage but it increases the overheads due to address translation. It
slows the execution of the memory because time is consumed in address translation.
In non-contiguous allocation, Operating system needs to maintain the table which is called Page Table for each process which contains the base address of the each block which is acquired by the process in memory space.
In non-contiguous memory allocation, different parts of a process is allocated different places in Main Memory. Spanning is allowed which is not possible in other techniques like Dynamic or Static Contiguous memory
allocation. That’s why paging is needed to ensure effective memory allocation. Paging is done to remove External Fragmentation.
Working:
Here a process can be spanned across different spaces in main memory in non-consecutive manner. Suppose process P of size 4KB. Consider main memory have two empty slots each of size 2KB. Hence total free space is,
2*2= 4 KB. In contiguous memory allocation, process P cannot be accommodated as spanning is not allowed.

In contiguous allocation, space in memory should be allocated to whole process. If not, then that space remains unallocated. But in Non-Contiguous allocation, process can be divided into different parts and hence filling
the space in main memory. In this example, process P can be divided into two parts of equal size – 2KB. Hence one part of process P can be allocated to first 2KB space of main memory and other part of processP can be
allocated to second 2KB space of main memory. Below diagram will explain in better way:

But, in what manner we divide a process to allocate them into main memory is very important to understand. Process is divided after analysing the number of empty spaces and their size in main memory. Then only we
divide our process. It is very time consuming process. Their number as well as their sizes changing every time due to execution of already present processes in main memory.
In order to avoid this time consuming process, we divide our process in secondary memory in advance before reaching the memory memory for its execution. Every process is divided into various parts of equal size called
Pages. We also divide our main memory into different parts of equal size called Frames. It is important to understand that:
Size of page in process ​
= Size of frame in memory

Although their numbers can be different. Below diagram will make you understand in better way: consider empty main memory having size of each frame is 2 KB, and two processes P1 and P2 are 2 KB each.

Resolvent main memory,

In conclusion we can say that, Paging allows memory address space of a process to be non-contiguous. Paging is more flexible as only pages of a process are moved. It allows more processes to reside in main memory than
Contiguous memory allocation.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/non-contiguous-allocation-in-operating-system/
✍
Write a Testimonial

Variable (or dynamic) Partitioning in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Variable (or dynamic) Partitioning in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
In operating systems, Memory Management is the function responsible for allocating and managing computer’s main memory. Memory Management function keeps track of the status of each memory location, either
allocated or free to ensure effective and efficient use of Primary Memory.
There are two Memory Management Techniques: Contiguous, and Non-Contiguous. In Contiguous Technique, executing process must be loaded entirely in main-memory. Contiguous Technique can be divided into:
1. Fixed (or static) partitioning
2. Variable (or dynamic) partitioning
Variable Partitioning –
It is a part of Contiguous allocation technique. It is used to alleviate the problem faced by Fixed Partitioning. In contrast with fixed partitioning, partitions are not made before the execution or during system configure.
Various features associated with variable Partitioning-

1. Initially RAM is empty and partitions are made during the run-time according to process’s need instead of partitioning during system configure.
2. The size of partition will be equal to incoming process.

3. The partition size varies according to the need of the process so that the internal fragmentation can be avoided to ensure efficient utilisation of RAM.
4. Number of partitions in RAM is not fixed and depends on the number of incoming process and Main Memory’s size.

There are some advantages and disadvantages of variable partitioning over fixed partitioning as given below.
Advantages of Variable Partitioning –
1. No Internal Fragmentation:
In variable Partitioning, space in main memory is allocated strictly according to the need of process, hence there is no case of internal fragmentation. There will be no unused space left in the partition.
2. No restriction on Degree of Multiprogramming:
More number of processes can be accommodated due to absence of internal fragmentation. A process can be loaded until the memory is not empty.
3. No Limitation on the size of the process:
In Fixed partitioning, the process with the size greater than the size of the largest partition could not be loaded and process can not be divided as it is invalid in contiguous allocation technique. Here, In variable
partitioning, the process size can’t be restricted since the partition size is decided according to the process size.
Disadvantages of Variable Partitioning –
1. Difficult Implementation:
Implementing variable Partitioning is difficult as compared to Fixed Partitioning as it involves allocation of memory during run-time rather than during system configure.
2. External Fragmentation:
There will be external fragmentation inspite of absence of internal fragmentation.
For example, suppose in above example- process P1(2MB) and process P3(1MB) completed their execution. Hence two spaces are left i.e. 2MB and 1MB. Let’s suppose process P5 of size 3MB comes. The empty
space in memory cannot be allocated as no spanning is allowed in contiguous allocation. The rule says that process must be contiguously present in main memory to get executed. Hence it results in External
Fragmentation.

Now P5 of size 3 MB cannot be accommodated in spite of required available space because in contiguous no spanning is allowed.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/variable-or-dynamic-partitioning-in-operating-system/
✍
Write a Testimonial

Fixed (or static) Partitioning in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Fixed (or static) Partitioning in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x

×​
Suggest a Topic

Select a Category​
menu
In operating systems, Memory Management is the function responsible for allocating and managing computer’s main memory. Memory Management function keeps track of the status of each memory location, either
allocated or free to ensure effective and efficient use of Primary Memory.
There are two Memory Management Techniques: Contiguous, and Non-Contiguous. In Contiguous Technique, executing process must be loaded entirely in main-memory. Contiguous Technique can be divided into:
1. Fixed (or static) partitioning
2. Variable (or dynamic) partitioning
Fixed Partitioning:
This is the oldest and simplest technique used to put more than one processes in the main memory. In this partitioning, number of partitions (non-overlapping) in RAM are fixed but size of each partition may or may not
be same. As it is contiguous allocation, hence no spanning is allowed. Here partition are made before execution or during system configure.

As illustrated in above figure, first process is only consuming 1MB out of 4MB in the main memory.
Hence, Internal Fragmentation in first block is (4-1) = 3MB.
Sum of Internal Fragmentation in every block = (4-1)+(8-7)+(8-7)+(16-14)= 3+1+1+2 = 7MB.
Suppose process P5 of size 7MB comes. But this process cannot be accommodated inspite of available free space because of contiguous allocation (as spanning is not allowed). Hence, 7MB becomes part of External
Fragmentation.
There are some advantages and disadvantages of fixed partitioning.
Advantages of Fixed Partitioning –
1. Easy to implement:
Algorithms needed to implement Fixed Partitioning are easy to implement. It simply requires putting a process into certain partition without focussing on the emergence of Internal and External Fragmentation.
2. Little OS overhead:
Processing of Fixed Partitioning require lesser excess and indirect computational power.
Disadvantages of Fixed Partitioning –
1. Internal Fragmentation:
Main memory use is inefficient. Any program, no matter how small, occupies an entire partition. This can cause internal fragmentation.
2. External Fragmentation:
The total unused space (as stated above) of various partitions cannot be used to load the processes even though there is space available but not in the contiguous form (as spanning is not allowed).
3. Limit process size:
Process of size greater than size of partition in Main Memory cannot be accommodated. Partition size cannot be varied according to the size of incoming process’s size. Hence, process size of 32MB in above stated
example is invalid.
4. Limitation on Degree of Multiprogramming:
Partition in Main Memory are made before execution or during system configure. Main Memory is divided into fixed number of partition. Suppose if there are
processes, then
condition must be fulfilled. Number of processes greater than number of partitions in RAM is invalid in Fixed Partitioning.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : ayushnigam9424

Source
https://www.geeksforgeeks.org/fixed-or-static-partitioning-in-operating-system/
✍
Write a Testimonial

Message based Communication in IPC (inter process communication)

partitions in RAM and

are the number of

​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Message based Communication in IPC (inter process communication) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisites – Cloud computing, Load balancing in Cloud Computing, Inter-process Communication
In the development of models and technologies, message abstraction is a necessary aspect that enables distributed computing. Distributed system is defined as a system in which components reside at networked
communication and synchronise its functions only by movement of messages. In this, message recognizes any discrete data that is moved from one entity to another. It includes any kind of data representation having
restriction of size and time, whereas it invokes a remote procedure or a sequence of object instance or a common message. This is the reason that “message-based communication model” can be beneficial to refer various
model for inter-process communication, which is based on the data streaming abstraction.
Various distributed programming model use this type of communication despite of the abstraction which is shown to developers for programming the co-ordination of shared components. Below are some major distributed
programming models that uses “message-based communication model”.
Message Passing –
In this model, the concept of message as the major abstraction of model is introduced. The units which inter-change the data and information that is explicitly encode, in the form of message. According to then
model, the schema and content of message changes or varies. Message Passing Interface and OpenMP are major example of this type of model.
Remote Procedure Call –
This model explores the keys of procedure call beyond the restrictions of a single process, thus pointing the execution of program in remote processes. In this, primary client-server is implied. A remote process
maintains a server component, thus enabling client processes to invoke the approaches and returns the output of the execution. Messages, created by the Remote Procedure Call (RPC) implementation, retrieve the
information of the procedure itself and that procedure is to execute having necessary arguments and also returns the values. The use of messages regarding this referred as marshal-ling of the arguments and return
values.
Distributed Objects –
It is an implementation of Remote Procedure Call (RPC) model for the Object-oriented model, and contextualizes this for the remote invocation of methods extended by objects. Every process assigns a set of
interfaces which are accessible remotely. Client process can demand a reference to these interfaces and invoke the methods available through them. The basic runtime infrastructure is in transformation the local
method calls into a request to a remote process and collecting the result of the execution. The interaction within the caller and the remote process is done trough messages. This model is stateless by design,
distributed object models introduce the complexity of object state management and lifetime. Common Object Request Broker Architecture (CORBA), Component Object Model (COM, DCOM and COM+), Java
Remote Method Invocation (RMI), and .NET Remoting are some major examples which falls under Distributed object infrastructure.
Active objects –
Programming models based on active objects comprise by definition the presence of instances, whether they are agent of objects, despite the availability of requests. It means, that objects having particular control
thread, which enables them to convey their activity. These models sometime make manual use of messages to encounter the execution of functions and a more complex and a more complex semantics is attached to
the messages.
Web Services –
Web service technology delivers an approach of the RPC concept over the HTTP, thus enabling the communication of components that are evolved with numerous technologies. A web service is revealed as a
remote object maintained on a Web server, and method invocations are transformed in HTTP requests wrapped with the help of specific protocol. It is necessary to observe that the concept of message is a basic
abstraction of inter-process communication and it is utilized either implicitly or explicitly.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/message-based-communication-in-ipc-inter-process-communication/
✍
Write a Testimonial

Program for Deadlock free condition in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Program for Deadlock free condition in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Given: A system has R identical resources, P processes competing for them and N is the maximum need of each process. The task is to find the minimum number of Resources required So that deadlock will never occur.
Formula:
R >= P * (N - 1) + 1

Examples:

Input : P = 3, N = 4​
Output : R >= 10​
​
Input : P = 7, N = 2​
Output : R >= 8

Approach:
Consider, 3 process A, B and C.
Let, Need of each process is 4
Therefore, The maximum resources require will be 3 * 4 = 12 i.e, Give 4 resources to each Process.
And, The minimum resources required will be 3 * (4 – 1) + 1 = 10.
i.e, Give 3 Resources to each of the Process, and we are left out with 1 Resource.
That 1 resource will be given to any of the Process A, B or C.
So that after using that resource by any one of the Process, It left the resources and that resources will be used by any other Process and thus Deadlock will Never Occur.
C++
filter_none
edit
close
play_arrow
link
brightness_4
code
// C++ implementation of above program.
#include <bits/stdc++.h>
using namespace std;

// function that calculates
// the minimum no. of resources
int Resources(int process, int need)
{
int minResources = 0;
// Condition so that deadlock
// will not occuur
minResources = process * (need - 1) + 1;
return minResources;
}
// Driver code
int main()
{
int process = 3, need = 4;
cout << "R >= " << Resources(process, need);
return 0;
}

chevron_right
filter_none
Java
filter_none
edit
close
play_arrow
link
brightness_4
code
// Java implementation of above program
class GFG
{
// function that calculates
// the minimum no. of resources
static int Resources(int process, int need)
{
int minResources = 0;
// Condition so that deadlock
// will not occuur
minResources = process * (need - 1) + 1;
return minResources;
}
// Driver Code
public static void main(String args[])
{
int process = 3, need = 4;
System.out.print("R >= ");
System.out.print(Resources(process, need));
}
}

chevron_right
filter_none
Python3
filter_none
edit
close
play_arrow
link
brightness_4
code
# Python 3 implementation of
# above program
# function that calculates
# the minimum no. of resources
def Resources(process, need):
minResources = 0
# Condition so that deadlock
# will not occuur
minResources = process * (need - 1) + 1
return minResources
# Driver Code
if __name__ == "__main__" :
process, need = 3, 4
print("R >=", Resources(process, need))
# This Code is Contributed
# by Naman_Garg

chevron_right
filter_none
C#
filter_none
edit
close
play_arrow
link
brightness_4
code
// C# implementation of above program
using System;
class GFG

{
// function that calculates
// the minimum no. of resources
static int Resources(int process, int need)
{
int minResources = 0;
// Condition so that deadlock
// will not occuur
minResources = process * (need - 1) + 1;
return minResources;
}
// Driver Code
public static void Main()
{
int process = 3, need = 4;
Console.Write("R >= ");
Console.Write(Resources(process, need));
}
}
// This code is contributed
// by Sanjit_Prasad

chevron_right
filter_none
Output:
R >= 10

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : Naman_Garg, Sanjit_Prasad, ankita_saini

Source
https://www.geeksforgeeks.org/program-for-deadlock-free-condition-in-operating-system/
✍
Write a Testimonial

Sandvine Interview Experience for FTE 2018
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Sandvine Interview Experience for FTE 2018 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Sandvine visited our campus for FTE.
Total 3 round
1. Aptitude (20 MCQ) 20 min
2. Technical (30 MCQ) 40 min ( Computer Network, Operating System, C/Java/python code output )
3. 3 coding question + 1 question to write about 3 bugs in the given code. 40 min
Coding Question were :
1. Given a binary tree and depth n, we have to print node at the depth n and function should not traverse tree
beyond the depth n.
2. given a binary array consisting of {0, 1} we have to return the index at which one operation can be performed ( convert 0 to 1 ) such continuous 1 is maximized. linear space and time complexity were expected.
3. given a linked list such we have to group nodes at even place and odd place together after that we have to append odd place node to the even place node in the reverse order.
ex : 1->2->3->4->5->6->7 ;
output : 2->4->6->7->5->3->1 :
we have to write only pseudo code in their text editor.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/sandvine-interview-experience-for-fte-2018/
✍
Write a Testimonial

Difference between RAM and ROM
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between RAM and ROM - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Types of computer memory (RAM and ROM)
Random Access Memory (RAM) is used to store the programs and data being used by the CPU in real time. The data on the random access memory can be read, written, and erased any number of times. RAM is the
hardware element where the data being currently used is stored. It is a volatile memory. Types of RAM:
1. Static RAM, or (SRAM) which stores a bit of data using the state of a six transistor memory cell.

2. Dynamic RAM, or (DRAM) which stores a bit data using a pair of transistor and capacitor which constitute a DRAM memory cell.
Read Only Memory (ROM) is a type of memory where the data has been prerecorded. Data stored in ROM is retained even after the computer is turned off ie, non-volatile. Types of ROM:
1.
2.
3.
4.

Programmable ROM, where the data is written after the memory chip has been created. It is non-volatile.
Erasable Programmable ROM, where the data on this non-volatile memory chip can be erased by exposing it to high-intensity UV light.
Electrically Erasable Programmable ROM, where the data on this non-volatile memory chip can be electrically erased using field electron emission.
Mask ROM, in which the data is written during the manufacturing of the memory chip.

The following table differentiates ROM and RAM:

Difference
Data retention
Working type
Use
Speed
CPU Interaction
Size and Capacity
Used as/in
Accessibility
Cost

RAM
RAM is a volatile memory which could store the data as long as the power is supplied.
Data stored in RAM can be retrieved and altered.
Used to store the data that has to be currently processed by CPU temporarily.
It is a high-speed memory.
The CPU can access the data stored on it.
Large size with higher capacity.
CPU Cache, Primary memory.
The data stored is easily accessible
Costlier

ROM
ROM is a non-volatile memory which could retain the data even when power is turned off.
Data stored in ROM can only be read.
It stores the instructions required during bootstrap of the computer.
It is much slower than the RAM.
The CPU can not access the data stored on it unless the data is stored in RAM.
Small size with less capacity.
Firmware, Micro-controllers
The data stored is not as easily accessible as in RAM
cheaper than RAM.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/difference-between-ram-and-rom/
✍
Write a Testimonial

Difference between Spooling and Buffering
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Spooling and Buffering - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
There are two ways by which Input/output subsystems can improve the performance and efficiency of the computer by using a memory space in the main memory or on the disk and these two are spooling and buffering.
Spooling –
Spooling stands for Simultaneous peripheral operation online. A spool is similar to buffer as it holds the jobs for a device until the device is ready to accept the job. It considers disk as a huge buffer that can store as many
jobs for the device till the output devices are ready to accept them.
Buffering –
The main memory has an area called buffer that is used to store or hold the data temporarily that is being transmitted either between two devices or between a device or an application. Buffering is an act of storing data
temporarily in the buffer. It helps in matching the speed of the data stream between the sender and the receiver. If the speed of the sender’s transmission is slower than the receiver, then a buffer is created in the main
memory of the receiver, and it accumulates the bytes received from the sender and vice versa.

The basic difference between Spooling and Buffering is that Spooling overlaps the input/output of one job with the execution of another job while the buffering overlaps the input/output of one job with the execution of the
same job.
Differences between Spooling and Buffering –
The key difference between spooling and buffering is that Spooling can handle the input/output of one job along with the computation of another job at the same time while buffering handles input/output of one job
along with its computation.
Spooling stands for Simultaneous Peripheral Operation online. Whereas buffering is not an acronym.
Spooling is more efficient than buffering, as spooling can overlap processing two jobs at a time.
Buffering uses limited area in main memory while Spooling uses the disk as a huge buffer.
Comparison chart –
SPOOLING
Basic Difference
Full form (stands for)
Efficiency
Consider Size

It overlap the input/output of one job with the execution of another job.
Simultaneous peripheral operation online
Spooling is more efficient than buffering.
It considers disk as a huge spool or buffer.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : AmanSachan

BUFFERING
It overlaps the input/output of one job with the execution of the same job.
No full form
Buffering is less efficient than spooling.
Buffer is a limited area in main memory.

Source
https://www.geeksforgeeks.org/difference-between-spooling-and-buffering/
✍
Write a Testimonial

Program for SSTF disk scheduling algorithm
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Program for SSTF disk scheduling algorithm - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Disk scheduling algorithms
Given an array of disk track numbers and initial head position, our task is to find the total number of seek operations done to access all the requested tracks if Shortest Seek Time First (SSTF) is a disk scheduling
algorithm is used.
Shortest Seek Time First (SSTF) –
Basic idea is the tracks which are closer to current disk head position should be serviced first in order to minimise the seek operations.
Algorithm –

1.
2.
3.
4.
5.
6.

Let Request array represents an array storing indexes of tracks that have been requested. ‘head’ is the position of disk head.
Find the positive distance of all tracks in the request array from head.
Find a track from requested array which has not been accessed/serviced yet and has minimum distance from head.
Increment the total seek count with this distance.
Currently serviced track position now becomes the new head position.
Go to step 2 until all tracks in request array have not been serviced.

Example –
Request sequence = {176, 79, 34, 60, 92, 11, 41, 114}
Initial head position = 50
The following chart shows the sequence in which requested tracks are serviced using SSTF.

Therefore, total seek count is calculates as:
= (50-41)+(41-34)+(34-11)+(60-11)+(79-60)+(92-79)+(114-92)+(176-114)​
= 204

Implementation –
Implementation of SSTF is given below. Note that we have made a node class having 2 members. ‘distance’ is used to store the distance between head and the track position. ‘accessed’ is a boolean variable which tells
whether the track has been accessed/serviced before by disk head or not.
Java
filter_none
edit
close
play_arrow
link
brightness_4
code
class node {
// represent difference between
// head position and track number
int distance = 0;
// true if track has been accessed
boolean accessed = false;
}
public class SSTF {
// Calculates difference of each
// track number with the head position
public static void calculateDifference(int queue[],
int head, node diff[])
{
for (int i = 0; i < diff.length; i++)
diff[i].distance = Math.abs(queue[i] - head);
}
// find unaccessed track
// which is at minimum distance from head
public static int findMin(node diff[])

{
int index = -1, minimum = Integer.MAX_VALUE;
for (int i = 0; i < diff.length; i++) {
if (!diff[i].accessed && minimum > diff[i].distance) {
minimum = diff[i].distance;
index = i;
}
}
return index;
}
public static void shortestSeekTimeFirst(int request[],
int head)
{
if (request.length == 0)
return;
// create array of objects of class node
node diff[] = new node[request.length];
// initialize array
for (int i = 0; i < diff.length; i++)
diff[i] = new node();
// count total number of seek operation
int seek_count = 0;
// stores sequence in which disk access is done
int[] seek_sequence = new int[request.length + 1];
for (int i = 0; i < request.length; i++) {
seek_sequence[i] = head;
calculateDifference(request, head, diff);
int index = findMin(diff);
diff[index].accessed = true;
// increase the total count
seek_count += diff[index].distance;
// accessed track is now new head
head = request[index];
}
// for last accessed track
seek_sequence[seek_sequence.length - 1] = head;
System.out.println("Total number of seek operations = "
+ seek_count);
System.out.println("Seek Sequence is");
// print the sequence
for (int i = 0; i < seek_sequence.length; i++)
System.out.println(seek_sequence[i]);
}
public static void main(String[] args)
{
// request array
int arr[] = { 176, 79, 34, 60, 92, 11, 41, 114 };
shortestSeekTimeFirst(arr, 50);
}
}

chevron_right
filter_none
Python3
filter_none
edit
close
play_arrow
link
brightness_4
code
# Python3 program for implementation of
# SSTF disk scheduling
# Calculates difference of each
# track number with the head position
def calculateDifference(queue, head, diff):
for i in range(len(diff)):
diff[i][0] = abs(queue[i] - head)
# find unaccessed track which is
# at minimum distance from head
def findMin(diff):
index = -1
minimum = 999999999
for i in range(len(diff)):
if (not diff[i][1] and
minimum > diff[i][0]):
minimum = diff[i][0]
index = i
return index
def shortestSeekTimeFirst(request, head):
if (len(request) == 0):
return
l = len(request)
diff = [0] * l
# initialize array
for i in range(l):
diff[i] = [0, 0]
# count total number of seek operation
seek_count = 0
# stores sequence in which disk
# access is done
seek_sequence = [0] * (l + 1)
for i in range(l):
seek_sequence[i] = head
calculateDifference(request, head, diff)
index = findMin(diff)
diff[index][1] = True

# increase the total count
seek_count += diff[index][0]
# accessed track is now new head
head = request[index]
# for last accessed track
seek_sequence[len(seek_sequence) - 1] = head
print("Total number of seek operations =",
seek_count)
print("Seek Sequence is")
# print the sequence
for i in range(l + 1):
print(seek_sequence[i])
# Driver code
if __name__ =="__main__":
# request array
proc = [176, 79, 34, 60,
92, 11, 41, 114]
shortestSeekTimeFirst(proc, 50)
# This code is contributed by
# Shubham Singh(SHUBHAMSINGH10)

chevron_right
filter_none
C#
filter_none
edit
close
play_arrow
link
brightness_4
code
// C# program for implementation of FCFS
// scheduling
using System;
public class node
{
// represent difference between
// head position and track number
public int distance = 0;
// true if track has been accessed
public Boolean accessed = false;
}
public class SSTF
{
// Calculates difference of each
// track number with the head position
public static void calculateDifference(int []queue,
int head, node []diff)
{
for (int i = 0; i < diff.Length; i++)
diff[i].distance = Math.Abs(queue[i] - head);
}
// find unaccessed track
// which is at minimum distance from head
public static int findMin(node []diff)
{
int index = -1, minimum = int.MaxValue;
for (int i = 0; i < diff.Length; i++)
{
if (!diff[i].accessed && minimum > diff[i].distance)
{
minimum = diff[i].distance;
index = i;
}
}
return index;
}
public static void shortestSeekTimeFirst(int []request,
int head)
{
if (request.Length == 0)
return;
// create array of objects of class node
node []diff = new node[request.Length];
// initialize array
for (int i = 0; i < diff.Length; i++)
diff[i] = new node();
// count total number of seek operation
int seek_count = 0;
// stores sequence in which disk access is done
int[] seek_sequence = new int[request.Length + 1];
for (int i = 0; i < request.Length; i++)
{
seek_sequence[i] = head;
calculateDifference(request, head, diff);
int index = findMin(diff);
diff[index].accessed = true;
// increase the total count
seek_count += diff[index].distance;
// accessed track is now new head
head = request[index];
}
// for last accessed track
seek_sequence[seek_sequence.Length - 1] = head;
Console.WriteLine("Total number of seek operations = "
+ seek_count);
Console.WriteLine("Seek Sequence is");

// print the sequence
for (int i = 0; i < seek_sequence.Length; i++)
Console.WriteLine(seek_sequence[i]);
}
// Driver code
public static void Main(String[] args)
{
// request array
int []arr = { 176, 79, 34, 60, 92, 11, 41, 114 };
shortestSeekTimeFirst(arr, 50);
}
}
// This code contributed by Rajput-Ji

chevron_right
filter_none
Output:
Total number of seek operations = 204​
Seek Sequence is​
50​
41​
34​
11​
60​
79​
92​
114​
176

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : sonuyadavaffriya, SHUBHAMSINGH10, Rajput-Ji

Source
https://www.geeksforgeeks.org/program-for-sstf-disk-scheduling-algorithm/
✍
Write a Testimonial

Process states and Transitions in a UNIX Process
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Process states and Transitions in a UNIX Process - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
A process is an instance of a program in execution. A set of processes combined together make a complete program.
There are two categories of processes in Unix, namely
User processes: They are operated in user mode.
Kernel processes: They are operated in kernel mode.
The states that a Process enters in working from start till end are known as Process states. These are listed below as:

Created-Process is newly created by system call, is not ready to run
User running-Process is running in user mode which means it is a user process.
Kernel Running-Indicates process is a kernel process running in kernel mode.
Zombie- Process does not exist/ is terminated.
Preempted- When process runs from kernel to user mode, it is said to be preempted.
Ready to run in memory- It indicated that process has reached a state where it is ready to run in memory and is waiting for kernel to schedule it.
Ready to run, swapped– Process is ready to run but no empty main memory is present
Sleep, swapped- Process has been swapped to secondary storage and is at a blocked state.
Asleep in memory- Process is in memory(not swapped to secondary storage) but is in blocked state.

The numbers indicate the steps that are followed.
The working of Process is explained in following steps:
1.
2.
3.
4.
5.
6.
7.
8.
9.

User-running: Process is in user-running.
Kernel-running: Process is allocated to kernel and hence, is in kernel mode.
Ready to run in memory: Further, after processing in main memory process is rescheduled to the Kernel.i.e.The process is not executing but is ready to run as soon as the kernel schedules it.
Asleep in memory: Process is sleeping but resides in main memory. It is waiting for the task to begin.
Ready to run, swapped: Process is ready to run and be swapped by the processor into main memory, thereby allowing kernel to schedule it for execution.
Sleep, Swapped: Process is in sleep state in secondary memory, making space for execution of other processes in main memory. It may resume once the task is fulfilled.
Pre-empted: Kernel preempts an on-going process for allocation of another process, while the first process is moving from kernel to user mode.
Created: Process is newly created but not running. This is the start state for all processes.
Zombie: Process has been executed thoroughly and exit call has been enabled.
The process, thereby, no longer exists. But, it stores a statistical record for the process.
This is the final state of all processes.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/process-states-and-transitions-in-a-unix-process/
✍
Write a Testimonial

Paytm Interview experience for FTE (On-Campus)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Paytm Interview experience for FTE (On-Campus) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Online Coding Round : Platform used was cocubes.com
Time : 75 min Question : 3 Coding Question
1st Question : Simple linked list implementation. Given two strings in the form of linked lists you were to find the lexicographical greatest string.
2nd Question : You were given an array and you were supposed to find the number of elements which is/are largest among all elements present in the right side of that particular element in the array.
3rd Question : It was a question related to BP+Bitmask. I don’t remember the exact question.
Around 27 students were selected from the coding round and were called for further interview rounds.

Round 1(Face-To-Face)
The interviewer was very friendly.He asked me to introduce my self
Then he asked me some question :
1) Find the maximum cost in traversing a 2-D matrix from one given cell to another. It is same as Min Cost Path DP problem.
He asked me to write a code for this.
2) Rod Cutting DP
I gave him an approach and he was satisfied with this. Then he asked me to write the code.
3) Left view of Binary Tree
I gave him an approach and he was happy with approach. After this I was asked to write the code.
After these questions he went on for some theoretical questions related to OOPS, DBMS, OS etc.

He asked basic questions like polymorphism and its types. Then, about Normalization and its types with example. He also asked questions related to semaphores.

Round 2(Face To Face 2)
The interviewer was very friendly.He asked me to introduce my self and about my hobby.
He then moved on to the theoretical concepts of subjects. He asked me questions related to DBMS(transactions and its properties-ACID, about normalization with examples, indexing), OS(paging, segmentation, difference
between process and thread etc).
After this he moved to my project, he asked me to give brief idea about my project. He asked questions related to my project(SQL vs No SQL, Node-Js concepts etc.).
Then he asked me a simple coding question with certain constraints and I solved it. He was very happy with my solution.
At the last he gave me a puzzle to which I couldn’t solve.
Round 3(Technical+Managerial)
The interviewer was very friendly. He went through my resume and asked about my interest and extra co-curricular activities.
After this he asked about my aims, and other HR’s like questions.
He asked few project related questions and this was all for this round.

Round 4(HR)
The interviewer was very friendly. She asked me to introduce myself.
She asked about my hobby. After this few questions like Why Paytm? etc. This was non-tech round. She wanted to check your nature, attitude and team work skills. Then I was asked to wait for the results.
Verdict – Selected
A big thank you to GeeksforGeeks for providing such invaluable material for preparation.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/paytm-interview-experience-for-fte-on-campus/
✍
Write a Testimonial

Introduction to memory and memory units
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Introduction to memory and memory units - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Memories are made up of registers. Each register in the memory is one storage location. Storage location is also called as memory location. Memory locations are identified using Address. The total number of bit a
memory can store is its capacity.
A storage element is called a Cell. Each register is made up of storage element in which one bit of data is stored. The data in a memory are stored and retrieved by the process called writing and reading respectively.

A word is a group of bits where a memory unit stores binary information. A word with group of 8 bits is called a byte.
A memory unit consists of data lines, address selection lines, and control lines that specify the direction of transfer. The block diagram of a memory unit is shown below:

Data lines provide the information to be stored in memory. The control inputs specify the direction transfer. The k-address lines specify the word chosen.
When there are k address lines, 2k memory word can be accessed.
Refer for RAM and ROM, different types of RAM, cache memory, and secondary memory

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/introduction-to-memory-and-memory-units/
✍
Write a Testimonial

Round Robin Scheduling with different arrival times
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Round Robin Scheduling with different arrival times - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite: Round Robin Scheduling with arrival time as 0
Round robin scheduling algorithm is used to schedule process fairly each job a time slot or quantum and the interrupting the job if it is not completed by then the job come after the other job which are arrived in the
quantum time that make these scheduling fairly
Note:
Round robin is cyclic in nature so starvation doesn’t occur
Round robin is variant of first come, first served scheduling
No priority, special importance given to any process or task
RR scheduling is also known as Time slicing scheduling
Advantages:

Each process is served by CPU for a fixed time so priority is same for each one
Starvation does not occur because of its cyclic nature.
Disadvantages:
Throughput depends on quantum time.
If we want to give some process priority, we cannot.
Process
P1
P2
P3
P4

Arrival Time
0
1
2
3

Burst Time
10
4
5
3

Quantum time is 3 this means each process is only executing for 3 units of time at a time.
How to compute these process requests:1. Take the process which occurs first and start executing the process.(for quantum time only)
2. Check if any other process request has arrived. If a process request arrives during the quantum time in which another process is executing, then add the new process to the Ready queue
3. After quantum time has passed, check for any processes in Ready queue. If ready queue is empty continue current process. If queue not empty and current process is not complete, then add current process to the end
of the ready queue.
4. Take the first process from the Ready queue and start executing it (same rules)
5. Repeat all steps above from 2-5
6. If process is complete and the ready queue is empty then task is complete

After all these we get the three times which are:
1. Completion Time: the time taken for a process to complete.
2. Turn Around Time: total time the process exists in system.(completion time – arrival time).
3. Waiting Time: total time the waiting for there complete execution.(turn around time – burst time ).
How to implement in programming language
​
1. Create two arrays of burst time res_b[] and of ​
arrival time res_a[] and copy the value of ​
the b[] and a[] array for calculate the ​
remaining time.(b[] is burst time, a[] arrival time).​
2. Create an another array for wt[] to store waiting time.​
3. Initialize Time : t=0;​
4. Keep traversing the all process while all ​
process are not done.​
Do following for i'th process if it is not done yet.​
a- if res_a[i]<= q
(quantum time :- q)​
1. if res_b[i]>q ​
a. t=t+q​
b. res_b[i]-=q;​
c. a[i]+=q;​
2. else res_b[i]<=q(for last to execute)​
a. t=t+b[i];​
b. wt[i]=t-b[i]-a[i];​
c.res_b[i]=0;​
b- else res_a[i]<q​
1. Initialize j=0 to number of process​
if a[j]<a[i] (compare is there any ​
other process come before these process)​
1. if res_b[j]>q ​
a. t=t+q​
b. res_b[j]-=q;​
c. a[j]+=q;​
2. else res_b[j]<=q​
a. t=t+b[j];​
b. wt[j]=t-b[j]-a[j];​
c.res_b[j]=0; ​
2. now we executing the i'th process ​
1. if res_b[i]>q ​
a. t=t+q​
b. res_b[i]-=q;​
c. a[i]+=q;​
2. else res_b[i]<=q​
a. t=t+b[i];​
b. wt[i]=t-b[i]-a[i];​
c.res_b[i]=0;​

Below is the implementation of the above approach:
Java
filter_none
edit
close
play_arrow
link
brightness_4
code
// Java program to implement Round Robin
// Scheduling with different arrival time
class roundrobin {
public static void roundRobin(String p[], int a[],
int b[], int n)
{
// result of average times
int res = 0;
int resc = 0;
// for sequence storage
String seq = new String();
// copy the burst array and arrival array
// for not effecting the actual array
int res_b[] = new int[b.length];
int res_a[] = new int[a.length];
for (int i = 0; i < res_b.length; i++) {
res_b[i] = b[i];
res_a[i] = a[i];
}
// critical time of system
int t = 0;
// for store the waiting time
int w[] = new int[p.length];
// for store the Completion time
int comp[] = new int[p.length];
while (true) {
boolean flag = true;
for (int i = 0; i < p.length; i++) {
// these condition for if
// arrival is not on zero
// check that if there come before qtime
if (res_a[i] <= t) {
if (res_a[i] <= n) {
if (res_b[i] > 0) {
flag = false;
if (res_b[i] > n) {
// make decrease the b time
t = t + n;
res_b[i] = res_b[i] - n;
res_a[i] = res_a[i] + n;
seq += "->" + p[i];
}
else {
// for last time
t = t + res_b[i];
// store comp time
comp[i] = t - a[i];
// store wait time
w[i] = t - b[i] - a[i];
res_b[i] = 0;
// add sequence
seq += "->" + p[i];
}
}
}
else if (res_a[i] > n) {

// is any have less arrival time
// the coming process then execute them
for (int j = 0; j < p.length; j++) {
// compare
if (res_a[j] < res_a[i]) {
if (res_b[j] > 0) {
flag = false;
if (res_b[j] > n) {
t = t + n;
res_b[j] = res_b[j] - n;
res_a[j] = res_a[j] + n;
seq += "->" + p[j];
}
else {
t = t + res_b[j];
comp[j] = t - a[j];
w[j] = t - b[j] - a[j];
res_b[j] = 0;
seq += "->" + p[j];
}
}
}
}
// now the previous porcess according to
// ith is process
if (res_b[i] > 0) {
flag = false;
// Check for greaters
if (res_b[i] > n) {
t = t + n;
res_b[i] = res_b[i] - n;
res_a[i] = res_a[i] + n;
seq += "->" + p[i];
}
else {
t = t + res_b[i];
comp[i] = t - a[i];
w[i] = t - b[i] - a[i];
res_b[i] = 0;
seq += "->" + p[i];
}
}
}
}
// if no process is come on thse critical
else if (res_a[i] > t) {
t++;
i--;
}
}
// for exit the while loop
if (flag) {
break;
}
}
System.out.println("name ctime wtime");
for (int i = 0; i < p.length; i++) {
System.out.println(" " + p[i] + "
" + comp[i]
+ "
" + w[i]);
res = res + w[i];
resc = resc + comp[i];
}
System.out.println("Average waiting time is "
+ (float)res / p.length);
System.out.println("Average compilation time is "
+ (float)resc / p.length);
System.out.println("Sequence is like that " + seq);
}
// Driver Code
public static void main(String args[])
{
// name of the process
String name[] = { "p1", "p2", "p3", "p4" };
// arrival for every process
int arrivaltime[] = { 0, 1, 2, 3 };
// burst time for every process
int bursttime[] = { 10, 4, 5, 3 };
// quantum time of each process
int q = 3;
// cal the function for output
roundRobin(name, arrivaltime, bursttime, q);
}
}

chevron_right
filter_none
C#
filter_none
edit
close
play_arrow
link
brightness_4
code
// C# program to implement Round Robin
// Scheduling with different arrival time
using System;
class GFG
{
public static void roundRobin(String []p, int []a,
int []b, int n)
{
// result of average times
int res = 0;
int resc = 0;
// for sequence storage
String seq = "";
// copy the burst array and arrival array
// for not effecting the actual array
int []res_b = new int[b.Length];
int []res_a = new int[a.Length];
for (int i = 0; i < res_b.Length; i++)
{

res_b[i] = b[i];
res_a[i] = a[i];
}
// critical time of system
int t = 0;
// for store the waiting time
int []w = new int[p.Length];
// for store the Completion time
int []comp = new int[p.Length];
while (true)
{
Boolean flag = true;
for (int i = 0; i < p.Length; i++)
{
// these condition for if
// arrival is not on zero
// check that if there come before qtime
if (res_a[i] <= t)
{
if (res_a[i] <= n)
{
if (res_b[i] > 0)
{
flag = false;
if (res_b[i] > n)
{
// make decrease the b time
t = t + n;
res_b[i] = res_b[i] - n;
res_a[i] = res_a[i] + n;
seq += "->" + p[i];
}
else
{
// for last time
t = t + res_b[i];
// store comp time
comp[i] = t - a[i];
// store wait time
w[i] = t - b[i] - a[i];
res_b[i] = 0;
// add sequence
seq += "->" + p[i];
}
}
}
else if (res_a[i] > n)
{
// is any have less arrival time
// the coming process then execute them
for (int j = 0; j < p.Length; j++)
{
// compare
if (res_a[j] < res_a[i])
{
if (res_b[j] > 0)
{
flag = false;
if (res_b[j] > n)
{
t = t + n;
res_b[j] = res_b[j] - n;
res_a[j] = res_a[j] + n;
seq += "->" + p[j];
}
else
{
t = t + res_b[j];
comp[j] = t - a[j];
w[j] = t - b[j] - a[j];
res_b[j] = 0;
seq += "->" + p[j];
}
}
}
}
// now the previous porcess according to
// ith is process
if (res_b[i] > 0)
{
flag = false;
// Check for greaters
if (res_b[i] > n)
{
t = t + n;
res_b[i] = res_b[i] - n;
res_a[i] = res_a[i] + n;
seq += "->" + p[i];
}
else
{
t = t + res_b[i];
comp[i] = t - a[i];
w[i] = t - b[i] - a[i];
res_b[i] = 0;
seq += "->" + p[i];
}
}
}
}
// if no process is come on thse critical
else if (res_a[i] > t)
{
t++;
i--;
}
}
// for exit the while loop
if (flag)
{
break;
}
}
Console.WriteLine("name
ctime
wtime");
for (int i = 0; i < p.Length; i++)
{
Console.WriteLine(" " + p[i] + "\t" +
comp[i] + "\t" + w[i]);
res = res + w[i];
resc = resc + comp[i];

}
Console.WriteLine("Average waiting time is " +
(float)res / p.Length);
Console.WriteLine("Average compilation time is " +
(float)resc / p.Length);
Console.WriteLine("Sequence is like that " + seq);
}
// Driver Code
public static void Main(String []args)
{
// name of the process
String []name = { "p1", "p2", "p3", "p4" };
// arrival for every process
int []arrivaltime = { 0, 1, 2, 3 };
// burst time for every process
int []bursttime = { 10, 4, 5, 3 };
// quantum time of each process
int q = 3;
// cal the function for output
roundRobin(name, arrivaltime, bursttime, q);
}
}
// This code is contributed by Rajput-Ji

chevron_right
filter_none
Output:
​
name ctime wtime​
p1
22
12​
p2
15
11​
p3
16
11​
p4
9
6​
Average waiting time is 10.0​
Average compilation time is 15.5​
Sequence is like that ->p1->p2->p3->p4->p1->p2->p3->p1->p1​

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : Petia Davidova, Rajput-Ji

Source
https://www.geeksforgeeks.org/round-robin-scheduling-with-different-arrival-times/
✍
Write a Testimonial

Deadlock Detection in Distributed Systems
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Deadlock Detection in Distributed Systems - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Deadlock Introduction, deadlock detection
In the centralized approach of deadlock detection, two techniques are used namely: Completely centralized algorithm and Ho Ramamurthy algorithm (One phase and Two-phase).
Completely Centralized Algorithm –
In a network of n sites, one site is chosen as a control site. This site is responsible for deadlock detection. It has control over all resources of the system. If a site requires a resource it requests the control site, the
control site allocates and de-allocates resources and maintains a wait for graph. And at regular interval of time, it checks the wait for graph to detect a cycle. If cycle exits then it will declare system as deadlock
otherwise the system will continue working. The major drawbacks of this technique are as follows:
1. A site has to send request even for using its own resource.
2. There is a possibility of phantom deadlock.
HO Ramamurthy (Two-Phase Algorithm) –
In this technique a resource status table is maintained by the central or control site, if a cycle is detected then the system is not declared deadlock at first, the cycle is checked again as the system is distributed some or
the other resource is vacant or freed by sites at every instant of time. Now, after checking if a cycle is detected again then, the system is declared as deadlock. This technique reduces the possibility of phantom
deadlock but on the other hand time consumption is more.
HO Ramamurthy (One Phase Algorithm) –
In this technique a resource status table and a process table is maintained by the central or control site if the cycle is detected in both processes and resource tables then, the system is declared as deadlock. This
technique reduces time consumption but space complexity increases.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : RakshithSathish

Source
https://www.geeksforgeeks.org/deadlock-detection-in-distributed-systems-2/
✍
Write a Testimonial

Cvent Interview Experience (On campus for Internship and Full Time)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Cvent Interview Experience (On campus for Internship and Full Time) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​

Suggest a Topic

Select a Category​
menu

Technical attitude test(Round 1)
This round consisted of 30 aptitude questions based on general aptitude and computer science fundamental subjects like operating system, dbms, c/c++, sql, computer networks.

Around 300 students sat for this test and 41 students were shortlisted for the next coding round.
Coding round(Round 2)
This round consisted of 1 programming question which had to be solved in a timespan of 90 minutes.
The test was conducted on codility platform.
The coding question was that we are given a rectangle which is of size 26×26 and we have been given some battle ships and their is another person who does not knows what is the position of these battle ships and he
randomly hits those battle ships and the positions that the person hits are given.
The battle shop can be of maximum 4 size and we were given the top left and bottom right corner of the battle ship.
We had to return what is the number of battle ships that are hit and are sunk.
The battle ship is said to be sunk of all the coordinates of the battle ships in the matrix have been hit and if some coordinates are hit then the battle ship is only hit but not sunk.
For example
Ship – {1A 2B, 3A 3A}
Hits – {1A 3A 5C 1B}
In this example there are 2 battle ships and the second person has made 4 hits.
The position of the ships are from index (1, 1) to (2, 2) and the position of second ship is from (3, 1) to (3, 1).
The battle ship 1 has been hit and battle ship 2 has been sunk.
11 students were shortlisted for further rounds of interview.
Technical interview (Round 3)
This was a technical interview
1. You hace been given a array which is sorted and rotated and you have to search a element from that array.
2. There is a orchid of apple trees and all the trees are standing in a line and they have apples on them.
There are 2 people and they have a number k and l that means they can pluck apples from k and l consecutive trees and we had to maximize the number of apples that they can pluck from the trees provided that they
cannot overlap their interval of trees.
3. Implement three stacks in a array.I was able to implement a basic solution and then he extended the problem to implement k stacks in a array.
4. What all you have learnt in your college apart from computer programming.
5. He asked what is the advantage of a RDBMS software over a file based databases management software.

Out of 11 students only 3 students were shortlisted for further rounds of interview.
Aptitude and personally assessment test (Round 4)
It was a 50 minutes short test which had 50 general aptitude and English questions that had to be solved in 22 minutes.
Then I had to solve a good number of psychometric based questions that judge a person’s personality.
Only 2 students reached round 4 that was technical interview round.
Technical cum HR Interview (Round 4)
It was a 1 hour round with the director of technology, Cvent.
1. He asked me about my introduction.
2. He asked what I liked and disliked about the company ppt?
3. In what all companies have you applied till date?
4. What is your dream company?
5. Why do you want to join Cvent?
6. Are you happy joining Thapar university and what was your dream college before joining Thapar?
7. He asked a technical question that you have been given a 10×10×10 cube consisting of 1000 cubes of size 1. He asked what all types of cubes you can see from all the sides of the cubes. He asked me to drive a formula
for a n×n×n cube.
8. He asked me about my favorite subjects and then i told him that i am comfortable in operating system, data structures and algorithms and database management.
9. He asked questions about databases and concepts such as inner join, outer join etc.
10. He asked whether I had any questions for him?
I asked what is the role of machine learning and artificial intelligence in the Cvent company?
Finally I was the only one to be selected for 6 months internship and full time offer at Cvent.
I would like to thank geeksforgeeks for helping me out in preparing for the company as well as providing an excellent resource for placement preparing.
Being their campus ambassador helped me a lot in acquiring great knowledge and sharing it with my juniors.
This article has been contributed by Ashish Madaan.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/cvent-interview-experienceon-campus-for-internship-and-full-time/
✍
Write a Testimonial

Pass the value from child process to parent process
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Pass the value from child process to parent process - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite: Pipe() and Fork() Basic
Write a C program in which the child process takes an input array and send it to the parent process using pipe() and fork() and then print it in the parent process.

Examples: Suppose we have an array a[]={1, 2, 3, 4, 5} in child process, then output should be 1 2 3 4 5.
​
Input: 1 2 3 4 5​
Output: 1 2 3 4 5​

filter_none
edit
close
play_arrow
link
brightness_4
code
// C program for passing value from
// child process to parent process
#include <pthread.h>
#include <stdio.h>
#include <sys/types.h>
#include <unistd.h>
#include <stdlib.h>
#include <sys/wait.h>
#define MAX 10
int main()
{
int fd[2], i = 0;
pipe(fd);
pid_t pid = fork();
if(pid > 0) {
wait(NULL);
// closing the standard input
close(0);
// no need to use the write end of pipe here so close it
close(fd[1]);
// duplicating fd[0] with standard input 0
dup(fd[0]);
int arr[MAX];
// n stores the total bytes read successfully
int n = read(fd[0], arr, sizeof(arr));
for ( i = 0;i < n/4; i++)
// printing the array received from child process
printf("%d ", arr[i]);
}
else if( pid == 0 ) {
int arr[] = {1, 2, 3, 4, 5};
// no need to use the read end of pipe here so close it
close(fd[0]);
// closing the standard output
close(1);
// duplicating fd[0] with standard output 1
dup(fd[1]);
write(1, arr, sizeof(arr));
}
else {
perror("error\n"); //fork()
}
}

chevron_right
filter_none
Steps for executing above code:
To compile, write gcc program_name.c
To run, write ./a.out

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : nidhi_biet

Source
https://www.geeksforgeeks.org/pass-the-value-from-child-process-to-parent-process/
✍
Write a Testimonial

Deadlock detection in Distributed systems
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Deadlock detection in Distributed systems - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
In a distributed system deadlock can neither be prevented nor avoided as the system is so vast that it is impossible to do so. Therefore, only deadlock detection can be implemented. The techniques of deadlock detection in
the distributed system require the following:
Progress – The method should be able to detect all the deadlocks in the system.
Safety – The method should not detect false or phantom deadlocks.
There are three approaches to detect deadlocks in distributed systems. They are as follows:
1. Centralized approach –
In the centralized approach, there is only one responsible resource to detect deadlock. The advantage of this approach is that it is simple and easy to implement, while the drawbacks include excessive workload at
one node, single-point failure (that is the whole system is dependent on one node if that node fails the whole system crashes) which in turns makes the system less reliable.

2. Distributed approach –
In the distributed approach different nodes work together to detect deadlocks. No single point failure ( that is the whole system is dependent on one node if that node fails the whole system crashes) as the workload is
equally divided among all nodes. The speed of deadlock detection also increases.
3. Hierarchical approach –
This approach is the most advantageous. It is the combination of both centralized and distributed approaches of deadlock detection in a distributed system. In this approach, some selected nodes or cluster of nodes
are responsible for deadlock detection and these selected nodes are controlled by a single node.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : Vidit_Gupta

Source
https://www.geeksforgeeks.org/deadlock-detection-in-distributed-systems/
✍
Write a Testimonial

Methods in Interprocess Communication
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Methods in Interprocess Communication - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Inter Process Communication,
Inter-process communication (IPC) is set of interfaces, which is usually programmed in order for the programs to communicate between series of processes. This allows running programs concurrently in an Operating
System. These are the methods in IPC:
1. Pipes (Same Process) –
This allows flow of data in one direction only. Analogous to simplex systems (Keyboard). Data from the output is usually buffered until input process receives it which must have a common origin.
2. Names Pipes (Different Processes) –
This is a pipe with a specific name it can be used in processes that don’t have a shared common process origin. E.g. is FIFO where the details written to a pipe is first named.
3. Message Queuing –
This allows messages to be passed between processes using either a single queue or several message queue. This is managed by system kernel these messages are coordinated using an API.
4. Semaphores –
This is used in solving problems associated with synchronization and to avoid race condition. These are integer values which are greater than or equal to 0.
5. Shared memory –
This allows the interchange of data through a defined area of memory. Semaphore values have to be obtained before data can get access to shared memory.
6. Sockets –
This method is mostly used to communicate over a network between a client and a server. It allows for a standard connection which is computer and OS independent.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : shreyashagrawal

Source
https://www.geeksforgeeks.org/methods-in-interprocess-communication/
✍
Write a Testimonial

Introduction of Secondary Memory
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Introduction of Secondary Memory - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Primary memory has limited storage capacity and is volatile. Secondary memory overcome this limitation by providing permanent storage of data and in bulk quantity. Secondary memory is also termed as external memory
and refers to the various storage media on which a computer can store data and programs. The Secondary storage media can be fixed or removable. Fixed Storage media is an internal storage medium like hard disk that is
fixed inside the computer. Storage medium that are portable and can be taken outside the computer are termed as removable storage media.
Difference between Primary Memory and Secondary Memory:

Primary Memory

Secondary Memory
Secondary memory is not accessed directly by the Central Processing Unit(CPU). Instead, data accessed from a
secondary memory is first loaded into Random Access Memory(RAM) and is then sent to the Processing Unit.

Primary memory is directly accessed by the Central Processing Unit(CPU).
RAM provides much faster accessing speed to data than secondary memory. By loading software
programs and required files into primary memory(RAM), computer can process data much more
quickly.
Primary memory, i.e. Random Access Memory(RAM) is volatile and gets completely erased when a
computer is shut down.

Secondary Memory is slower in data accessing. Typically primary memory is six times faster than the secondary
memory.
Secondary memory provides a feature of being non-volatile, which means it can hold on to its data with or without
electrical power supply.

Uses of Secondary Media:
Permanent Storage: Primary Memory (RAM) is volatile, i.e. it loses all information when the electricity is turned off, so in order to secure the data permanently in the device, Secondary storage devices are needed.
Portability: Storage medium, like the CDs, flash drives can be used to transfer the data from one devive to another.
Fixed and Removable Storage
Fixed StorageA Fixed storage is an internal media device that is used by a computer system to store data, and usually these are referred to as the Fixed Disks drives or the Hard Drives.
Fixed storage devices are literally not fixed, obviously these can be removed from the system for repairing work, maintenance purpose, and also for upgrade etc. But in general, this can’t be done without a proper toolkit to
open up the computer system to provide physical access, and that needs to be done by an engineer.
Technically, almost all of the data i.e. being processed on a computer system is stored on some type of a built-in fixed storage device.
Types of fixed storage:
Internal flash memory (rare)
SSD (solid-state disk) units
Hard disk drives (HDD)
Removable StorageA Removable storage is an external media device that is used by a computer system to store data, and usually these are referred to as the Removable Disks drives or the External Drives.
Removable storage is any type of storage device that can be removed/ejected from a computer system while the system is running. Examples of external devices include CDs, DVDs and Blu-Ray disk drives, as well as
diskettes and USB drives. Removable storage makes it easier for a user to transfer data from one computer system to another.
In a storage factors, the main benefit of removable disks is that they can provide the fast data transfer rates associated with storage area networks (SANs)
Types of Removable Storage:
Optical discs (CDs, DVDs, Blu-ray discs)
Memory cards
Floppy disks
Magnetic tapes
Disk packs
Paper storage (punched tapes , punched cards)
Secondary Storage Media
There are the following main types of storage media:
1. Magnetic storage media:
Magnetic media is coated with a magnetic layer which is magnetized in clockwise or anticlockwise directions. When the disk moves, the head interprets the data stored at a specific location in binary 1s and 0s at reading.
Examples: hard disks, floppy disks and magnetic tapes.
Floppy Disk: A floppy disk is a flexible disk with a magnetic coating on it. It is packaged inside a protective plastic envelope. These are one of the oldest type of portable storage devices that could store up to 1.44
MB of data but now they are not used due to very less memory storage.
Hard disk: A hard disk consists of one or more circular disks called platters which are mounted on a common spindle. Each surface of a platter is coated with a magnetic material. Both surfaces of each disk are
capable of storing data except the top and bottom disk where only the inner surface is used. The information is recorded on the surface of the rotating disk by magnetic read/write heads. These heads are joined to a
common arm known as access arm.
Hard disk drive components:
Most of the basic types of hard drives contains a number of disk platters that are placed around a spindle which is placed inside a sealed chamber. The chamber also includes read/write head and motors. Data is
stored on each of these disks in the arrangement of concentric circles called tracks which are divided further into sectors. Though internal Hard drives are not very portable and used internally in a computer system,
external hard disks can be used as a substitute for portable storage. Hard disks can store data upto several terabytes.

2. Optical storage media
In optical storage media information is stored and read using a laser beam. The data is stored as a spiral pattern of pits and ridges denoting binary 0 and binary 1.
Examples: CDs and DVDs

Compact Disk: A Compact Disc drive(CDD) is a device that a computer uses to read data that is encoded digitally on a compact disc(CD). A CD drive can be installed inside a computer’s compartment, provided
with an opening for easier disc tray access or it can be used by a peripheral device connected to one of the ports provided in the computer system.A compact disk or CD can store approximately 650 to 700 megabytes
of data. A computer should possess a CD Drive to read the CDs. There are three types of CDs:
CD- ROM
It stands for Compact Disk – Read Only Memory
Data is written on these disks at the time of manufacture. This data cannot be changed, once is it written by
the manufacturer, but can only be read. CD- ROMs are used for text, audio and video distribution like games,
encyclopedias and application software.

CD-R
It stands for Compact Disk- Recordable.
Data can be recorded on these disks but only
once. Once the data is written in a CD-R, it
cannot be erased/modified.

CD-RW
It stands for Compact Disk-Rewritable.
It can be read or written multiple times but a CDRW drive needs to be installed on your computer
before editing a CD-RW.

DVD:
It stands for Digital Versatile Disk or Digital Video Disk. It looks just like a CD and use a similar technology as that of the CDs but allows tracks to be spaced closely enough to store data that is more than six times
the CD’s capacity. It is a significant advancement in portable storage technology. A DVD holds 4.7 GB to 17 GB of data.
Blue Ray Disk:
This is the latest optical storage media to store high definition audio and video. It is similar to a CD or DVD but can store up to 27 GB of data on a single layer disk and up to 54 GB of data on a dual layer disk.
While CDs or DVDs use red laser beam, the blue ray disk uses a blue laser to read/write data on a disk.
3. Solid State Memories
Solid-state storage devices are based on electronic circuits with no moving parts like the reels of tape, spinning discs etc. Solid-state storage devices use special memories called flash memory to store data. Solid state drive
(or flash memory) is used mainly in digital cameras, pen drives or USB flash drives.
Pen Drives:
Pen Drives or Thumb drives or Flash drives are the recently emerged portable storage media. It is an EEPROM based flash memory which can be repeatedly erased and written using electric signals. This memory is

accompanied with a USB connector which enables the pendrive to connect to the computer. They have a capacity smaller than a hard disk but greater than a CD. Pendrive has following advantages:
Transfer Files:
A pen drive being plugged into a USB port of the system can be used as a device to transfer files, documents and photos to a PC and also vice versa. Similarly, selected files can be transferred between a pen drive
and any type of workstation.
Portability:
The lightweight nature and smaller size of a pen drive make it possible to carry it from place to place which makes data transportation an easier task.
Backup Storage:
Most of the pen drives now come with a feature of having password encryption, important information related to family, medical records and photos can be stored on them as a backup.
Transport Data:
Professionals/Students can now easily transport large data files and video/audio lectures on a pen drive and gain access to them from anywhere. Independent PC technicians can store work-related utility tools,
various programs and files on a high-speed 64 GB pen drive and move from one site to another.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/introduction-of-secondary-memory/
✍
Write a Testimonial

Process Synchronization | Set 2
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Process Synchronization | Set 2 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Process Synchronization | Introduction, Critical Section, Semaphores
Process Synchronization is a technique which is used to coordinate the process that use shared Data. There are two types of Processes in an Operating Systems:1. Independent Process –
The process that does not affect or is affected by the other process while its execution then the process is called Independent Process. Example The process that does not share any shared variable, database, files, etc.
2. Cooperating Process –
The process that affect or is affected by the other process while execution, is called a Cooperating Process. Example The process that share file, variable, database, etc are the Cooperating Process.
Process Synchronization is mainly used for Cooperating Process that shares the resources.Let us consider an example of
//racing condition image

It is the condition where several processes tries to access the resources and modify the shared data concurrently and outcome of the process depends on the particular order of execution that leads to data inconsistency, this
condition is called Race Condition.This condition can be avoided using the technique called Synchronization or Process Synchronization, in which we allow only one process to enter and manipulates the shared data in
Critical Section.
//diagram of the view of CS

This setup can be defined in various regions like:
Entry Section –
It is part of the process which decide the entry of a particular process in the Critical Section, out of many other processes.
Critical Section –
It is the part in which only one process is allowed to enter and modify the shared variable.This part of the process ensures that only no other process can access the resource of shared data.
Exit Section –
This process allows the other process that are waiting in the Entry Section, to enter into the Critical Sections. It checks that a process that after a process has finished execution in Critical Section can be removed
through this Exit Section.
Remainder Section –
The other parts of the Code other than Entry Section, Critical Section and Exit Section are known as Remainder Section.
Critical Section problems must satisfy these three requirements:
1. Mutual Exclusion –
It states that no other process is allowed to execute in the critical section if a process is executing in critical section.
2. Progress –
When no process is in the critical section, then any process from outside that request for execution can enter in the critical section without any delay. Only those process can enter that have requested and have finite
time to enter the process.
3. Bounded Waiting –
An upper bound must exist on the number of times a process enters so that other processes are allowed to enter their critical sections after a process has made a request to enter its critical section and before that
request is granted.
Process Synchronization are handled by two approaches:
1. Software Approach –
In Software Approach, Some specific Algorithm approach is used to maintain synchronization of the data. Like in Approach One or Approach Two, for a number of two process, a temporary variable like (turn) or
boolean variable (flag) value is used to store the data. When the condition is True then the process in waiting State, known as Busy Waiting State. This does not satisfy all the Critical Section requirements.
Another Software approach known as Peterson’s Solution is best for Synchronization. It uses two variables in the Entry Section so as to maintain consistency, like Flag (boolean variable) and Turn variable(storing
the process states). It satisfy all the three Critical Section requirements.
//Image of Peterson’s Algorithm

2. Hardware Approach –
The Hardware Approach of synchronization can be done through Lock & Unlock technique.Locking part is done in the Entry Section, so that only one process is allowed to enter into the Critical Section, after it
complete its execution, the process is moved to the Exit Section, where Unlock Operation is done so that another process in the Lock Section can repeat this process of Execution.This process is designed in such a
way that all the three conditions of the Critical Sections are satisfied.
//Image of Lock

Using Interrupts –
These are easy to implement.When Interrupt are disabled then no other process is allowed to perform Context Switch operation that would allow only one process to enter into the Critical State.
//Image of Interrupts

Test_and_Set Operation –
This allows boolean value (True/False) as a hardware Synchronization, which is atomic in nature i.e no other interrupt is allowed to access.This is mainly used in Mutual Exclusion Application. Similar type operation can

be achieved through Compare and Swap function. In this process, a variable is allowed to accessed in Critical Section while its lock operation is ON.Till then, the other process is in Busy Waiting State. Hence Critical
Section Requirements are achieved.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/process-synchronization-set-2/
✍
Write a Testimonial

Need and Functions of Operating Systems
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Need and Functions of Operating Systems - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Goal of an Operating System:
The fundamental goal of a Computer System is to execute user programs and to make tasks easier. Various application programs along with hardware system are used to perform this work. Operating System is a software
which manages and control the entire set of resources and effectively utilize every part of a computer.
The figure shows how OS acts as a medium between hardware unit and application programs.

Need of Operating System:

OS as a platform for Application programs: Operating system provides a platform, on top of which, other programs, called application programs can run. These application programs help the users to perform a
specific task easily. It acts as an interface between the computer and the user. It is designed in such a manner that it operates, controls and executes various applications on the computer.
Managing Input-Output unit: Operating System also allows the computer to manage its own resources such as memory, monitor, keyboard, printer etc. Management of these resources is required for an effective
utilization.The operating system controls the various system input-output resources and allocates them to the users or programs as per their requirement.
Consistent user interface: Operating System provides the user an easy-to-work user interface, so the user doesn’t have to learn a different UI every time and can focus on the content and be productive as quickly as
possible. Operating System provides templates, UI components to make the working of a computer, really easy for the user.
Multitasking: Operating System manages memory and allow multiple programs to run in their own space and even communicate with each other through shared memory. Multitasking gives users a good experience
as they can perform several tasks on a computer at a time.
Functions of an Operating System
An operating system has variety of functions to perform. Some of the prominent functions of an operating system can be broadly outlined as:
Processor Management: This deals with management of the Central Processing Unit (CPU). The operating system takes care of the allotment of CPU time to different processes. When a process finishes its CPU
processing after executing for the allotted time period, this is called scheduling. There are various type of scheduling techniques that are used by the operating systems:
1. Shortest Job First(SJF): Process which need the shortest CPU time are scheduled first.
2. Round Robin Scheduling: Each process is assigned a fixed CPU execution time in cyclic way.
3. Priority Based scheduling (Non Preemptive): In this scheduling, processes are scheduled according to their priorities, i.e., highest priority process is schedule first. If priorities of two processes match, then
schedule according to arrival time
Device Management:
The Operating System communicates with hardware and the attached devices and maintains a balance between them and the CPU. This is all the more important because the CPU processing speed is much higher
than that
of I/O devices. In order to optimize the CPU time, the operating system employstwo techniques – Buffering and Spooling.
Buffering:
In this technique, input and output data is temporarily stored in Input Buffer and Output Buffer. Once the signal for input or output is sent to or from the CPU respectively, the operating system through the device
controller moves the data from the input device to the input buffer and for the output device to the output buffer. In case of input, if the buffer is full, the operating system sends a signal to the program which
processes the data stored in the buffer. When the buffer becomes empty, the program informs the operating system which reloads the buffer and the input operation continues.
Spooling (Simultaneous Peripheral Operation on Line):
This is a device management technique used for processing of different tasks on the same input/output device. When there are various users on a network sharing the same resource then it can be a possibility that
more than one user might give it a command at the same point of time. So, the operating system temporarily stores the data of every user on the hard disk of the computer to which the resource is attached. The
individual user need not wait for the execution process to be completed. Instead the operating system sends the data from the hard disk to the resource one by one.
Example: printer
Memory management:
In a computer, both the CPU and the I/O devices interact with the memory. When a program needs to be executed it is loaded onto the main memory till the execution is completed. Thereafter that memory space is
freed and is available for other programs. The common memory management techniques used by the operating system are Partitioning and Virtual Memory.
Partitioning:

The total memory is divided into various partitions of same size or different sizes. This helps to accommodate number of programs in the memory. The partition can be fixed i.e. remains same for all the programs in
the memory or variable i.e. memory is allocated when a program is loaded on to the memory. The later approach causes less wastage of memory but in due course of time, it may become fragmented.
Virtual Memory:
This is a technique used by the operating systems which allow the user can load the programs which are larger than the main memory of the computer. In this technique the program is executed even if the complete
program can not be loaded inside the main memory leading to efficient memory utilization.
File Management:
The operating System manages the files, folders and directory systems on a computer. Any data on a computer is stored in the form of files and the operating system keeps information about all of them using File
Allocation Table (FAT). The FAT stores general information about files like filename, type (text or binary), size, starting address and access mode (sequential/indexed sequential/direct/relative). The file manager of
the operating system helps to create, edit, copy, allocate memory to the files and also updates the FAT. The operating system also takes care that files are opened with proper access rights to read or edit them.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/need-and-functions-of-operating-systems/
✍
Write a Testimonial

Free space management in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Free space management in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
The system keeps tracks of the free disk blocks for allocating space to files when they are created. Also, to reuse the space released from deleting the files, free space management becomes crucial. The system maintains a
free space list which keeps track of the disk blocks that are not allocated to some file or directory. The free space list can be implemented mainly as:
1. Bitmap or Bit vector –
A Bitmap or Bit Vector is series or collection of bits where each bit corresponds to a disk block. The bit can take two values: 0 and 1: 0 indicates that the block is allocated and 1 indicates a free block.
The given instance of disk blocks on the disk in Figure 1 (where green blocks are allocated) can be represented by a bitmap of 16 bits as: 0000111000000110.

Advantages –
Simple to understand.
Finding the first free block is efficient. It requires scanning the words (a group of 8 bits) in a bitmap for a non-zero word. (A 0-valued word has all bits 0). The first free block is then found by scanning for the
first 1 bit in the non-zero word.
The block number can be calculated as:
(number of bits per word) *(number of 0-values words) + offset of bit first bit 1 in the non-zero word .
For the Figure-1, we scan the bitmap sequentially for the first non-zero word.
The first group of 8 bits (00001110) constitute a non-zero word since all bits are not 0. After the non-0 word is found, we look for the first 1 bit. This is the 5th bit of the non-zero word. So, offset = 5.
Therefore, the first free block number = 8*0+5 = 5.
2. Linked List –
In this approach, the free disk blocks are linked together i.e. a free block contains a pointer to the next free block. The block number of the very first disk block is stored at a separate location on disk and is also
cached in memory.

In Figure-2, the free space list head points to Block 5 which points to Block 6, the next free block and so on. The last free block would contain a null pointer indicating the end of free list.
A drawback of this method is the I/O required for free space list traversal.
3. Grouping –
This approach stores the address of the free blocks in the first free block. The first free block stores the address of some, say n free blocks. Out of these n blocks, the first n-1 blocks are actually free and the last
block contains the address of next free n blocks.

An advantage of this approach is that the addresses of a group of free disk blocks can be found easily.
4. Counting –
This approach stores the address of the first free disk block and a number n of free contiguous disk blocks that follow the first block.
Every entry in the list would contain:
1. Address of first free disk block
2. A number n
For example, in Figure-1, the first entry of the free space list would be: ([Address of Block 5], 2), because 2 contiguous free blocks follow block 5.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/free-space-management-in-operating-system/
✍
Write a Testimonial

Nokia Interview Experience | Set 5
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Nokia Interview Experience | Set 5 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Nokia interview
First round: Based on AMCAT
After cleared first round, we got a mail, even I got short listed for the second round.first round was held in college through webcam.

For second round we went to nokia networks in Bangalore .
Round 2: Started with GD, they took many students from GD Round.
Round 3: Technical round they asked me avoab networks python (which I mentioned? in the resume), and about project.many students rejected in technical round.
Round4: HR round (I didn’t go through this round)
it was a Nice experience, friendly company
Thanks to geeksfogeeks, I read most of the programming concept from friendly study site#geeksforgeeks
All the best for everyone, who are aiming for Nokia.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/nokia-interview-experience-set-5/
✍
Write a Testimonial

Tasks in Real Time systems
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Tasks in Real Time systems - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
The system is subjected to real time, i.e. response should be guaranteed within a specified timing constraint or system should meet the specified deadline. For example flight control system, real-time monitors etc.
There are two types of tasks in real-time systems:
1. Periodic tasks
2. Dynamic tasks
Periodic Tasks: In periodic task, jobs are released at regular intervals. A periodic task is one which repeats itself after a fixed time interval. A periodic task is denoted by five tuples: Ti = < Φi, Pi, ei, D i >
Where,
Φi – is the phase of the task. Phase is release time of the first job in the task. If the phase is not mentioned then release time of first job is assumed to be zero.
Pi – is the period of the task i.e. the time interval between the release times of two consecutive jobs.
ei – is the execution time of the task.
D i – is the relative deadline of the task.
For example: Consider the task T i with period = 5 and execution time = 3
Phase is not given so, assume the release time of the first job as zero. So the job of this task is first released at t = 0 then it executes for 3s and then next job is released at t = 5 which executes for 3s and then next job
is released at t = 10. So jobs are released at t = 5k where k = 0, 1, . . ., n

Hyper period of a set of periodic tasks is the least common multiple of periods of all the tasks in that set. For example, two tasks T1 and T2 having period 4 and 5 respectively will have a hyper period, H = lcm(p1,
p2) = lcm(4, 5) = 20. The hyper period is the time after which pattern of job release times starts to repeat.
Dynamic Tasks: It is a sequential program that is invoked by the occurrence of an event. An event may be generated by the processes external to the system or by processes internal to the system. Dynamically
arriving tasks can be categorized on their criticality and knowledge about their occurrence times.
1. Aperiodic Tasks: In this type of task, jobs are released at arbitrary time intervals i.e. randomly. Aperiodic tasks have soft deadlines or no deadlines.
2. Sporadic Tasks: They are similar to aperiodic tasks i.e. they repeat at random instances. The only difference is that sporadic tasks have hard deadlines. A speriodic task is denoted by three tuples: Ti =(ei, gi,
D i)
Where
ei – the execution time of the task.
gi – the minimum separation between the occurrence of two consecutive instances of the task.
D i – the relative deadline of the task.
Jitter: Sometimes actual release time of a job is not known. Only know that ri is in a range [ ri-, ri+ ]. This range is known as release time jitter. Here ri– is how early a job can be released and ri+ is how late a job can be
released. Only range [ ei-, ei+ ] of the execution time of a job is known. Here ei– is the minimum amount of time required by a job to complete its execution and ei+ the maximum amount of time required by a job to
complete its execution.
Precedence Constraint of Jobs: Jobs in a task are independent if they can be executed in any order. If there is a specific order in which jobs in a task have to be executed then jobs are said to have precedence constraints.
For representing precedence constraints of jobs a partial order relation < is used. This is called precedence relation. A job Ji is a predecessor of job Jj if Ji < Jj i.e. Jj cannot begin its execution until Ji completes. Ji is an
immediate predecessor of Jj if Ji < Jj and there is no other job Jk such that Ji < Jk < Jj. Ji and Jj are independent if neither Ji < Jj nor Jj < Ji is true.
An efficient way to represent precedence constraints is by using a directed graph G = (J, <) where J is the set of jobs. This graph is known as the precedence graph. Jobs are represented by vertices of graph and precedence
constraints are represented using directed edges. If there is a directed edge from Ji to Jj then it means that J i is immediate predecessor of Jj. For example: Consider a task T having 5 jobs J 1 , J2 , J3 , J4 and J5 such that J2 and
J5 cannot begin their execution until J1 completes and there are no other constraints.
The precedence constraints for this example are:
J1 < J2 and J1 < J5

Set representation of precedence graph:
1.
2.
3.
4.
5.

< (1) = { }
< (2) = {1}
< (3) = { }
< (4) = { }
< (5) = {1}

Consider another example where precedence graph is given and you have to find precedence constraints

From the above graph, we derive the following precedence constraints:
1.
2.
3.
4.

J1 < J2
J2 < J3
J2 < J4
J3 < J4

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : dcbleo

Source
https://www.geeksforgeeks.org/tasks-in-real-time-systems/
✍
Write a Testimonial

Communication between two process using signals in C
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Communication between two process using signals in C - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​

menu
Prerequisite : C signal handling
In this post, the communication between child and parent processes is done using kill() and signal(), fork() system call.
fork() creates the child process from the parent. The pid can be checked to decide whether it is the child (if pid == 0) or the parent (pid = child process id).
The parent can then send messages to child using the pid and kill().
The child picks up these signals with signal() and calls appropriate functions.
Example of how 2 processes can talk to each other using kill() and signal():

filter_none
edit
close
play_arrow
link
brightness_4
code
// C program to implement sighup(), sigint()
// and sigquit() signal functions
#include <signal.h>
#include <stdio.h>
#include <stdlib.h>
#include <sys/types.h>
#include <unistd.h>
// function declaration
void sighup();
void sigint();
void sigquit();
// driver code
void main()
{
int pid;
/* get child process */
if ((pid = fork()) < 0) {
perror("fork");
exit(1);
}
if (pid == 0) { /* child */
signal(SIGHUP, sighup);
signal(SIGINT, sigint);
signal(SIGQUIT, sigquit);
for (;;)
; /* loop for ever */
}
else /* parent */
{ /* pid hold id of child */
printf("\nPARENT: sending SIGHUP\n\n");
kill(pid, SIGHUP);
sleep(3); /* pause for 3 secs */
printf("\nPARENT: sending SIGINT\n\n");
kill(pid, SIGINT);
sleep(3); /* pause for 3 secs */
printf("\nPARENT: sending SIGQUIT\n\n");
kill(pid, SIGQUIT);
sleep(3);
}
}
// sighup() function definition
void sighup()
{
signal(SIGHUP, sighup); /* reset signal */
printf("CHILD: I have received a SIGHUP\n");
}
// sigint() function definition
void sigint()
{
signal(SIGINT, sigint); /* reset signal */
printf("CHILD: I have received a SIGINT\n");
}
// sigquit() function definition
void sigquit()
{
printf("My DADDY has Killed me!!!\n");
exit(0);
}

chevron_right
filter_none
Output:

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/signals-c-set-2/
✍
Write a Testimonial

Sleeping Barber problem in Process Synchronization
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Sleeping Barber problem in Process Synchronization - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Inter Process Communication
Problem : The analogy is based upon a hypothetical barber shop with one barber. There is a barber shop which has one barber, one barber chair, and n chairs for waiting for customers if there are any to sit on the chair.
If there is no customer, then the barber sleeps in his own chair.
When a customer arrives, he has to wake up the barber.
If there are many customers and the barber is cutting a customer’s hair, then the remaining customers either wait if there are empty chairs in the waiting room or they leave if no chairs are empty.

Solution : The solution to this problem includes three semaphores.First is for the customer which counts the number of customers present in the waiting room (customer in the barber chair is not included because he is not
waiting). Second, the barber 0 or 1 is used to tell whether the barber is idle or is working, And the third mutex is used to provide the mutual exclusion which is required for the process to execute. In the solution, the
customer has the record of the number of customers waiting in the waiting room if the number of customers is equal to the number of chairs in the waiting room then the upcoming customer leaves the barbershop.

When the barber shows up in the morning, he executes the procedure barber, causing him to block on the semaphore customers because it is initially 0. Then the barber goes to sleep until the first customer comes up.
When a customer arrives, he executes customer procedure the customer acquires the mutex for entering the critical region, if another customer enters thereafter, the second one will not be able to anything until the first one
has released the mutex. The customer then checks the chairs in the waiting room if waiting customers are less then the number of chairs then he sits otherwise he leaves and releases the mutex.
If the chair is available then customer sits in the waiting room and increments the variable waiting value and also increases the customer’s semaphore this wakes up the barber if he is sleeping.
At this point, customer and barber are both awake and the barber is ready to give that person a haircut. When the haircut is over, the customer exits the procedure and if there are no customers in waiting room barber
sleeps.

Algorithm for Sleeping Barber problem:
filter_none
edit

close
play_arrow
link
brightness_4
code
Semaphore Customers = 0;
Semaphore Barber = 0;
Mutex Seats = 1;
int FreeSeats = N;
Barber {
while(true) {
/* waits for a customer (sleeps). */
down(Customers);
/* mutex to protect the number of available seats.*/
down(Seats);
/* a chair gets free.*/
FreeSeats++;
/* bring customer for haircut.*/
up(Barber);
/* release the mutex on the chair.*/
up(Seats);
/* barber is cutting hair.*/
}
}
Customer {
while(true) {
/* protects seats so only 1 customer tries to sit
in a chair if that's the case.*/
down(Seats); //This line should not be here.
if(FreeSeats > 0) {
/* sitting down.*/
FreeSeats--;
/* notify the barber. */
up(Customers);
/* release the lock */
up(Seats);
/* wait in the waiting room if barber is busy. */
down(Barber);
// customer is having hair cut
} else {
/* release the lock */
up(Seats);
// customer leaves
}
}
}

chevron_right
filter_none

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : Kumar Shubham 10

Source
https://www.geeksforgeeks.org/sleeping-barber-problem-in-process-synchronization/
✍
Write a Testimonial

Belady’s Anomaly in Page Replacement Algorithms
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Belady's Anomaly in Page Replacement Algorithms - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Page Replacement Algorithms
In Operating System, process data is loaded in fixed sized chunks and each chunk is referred to as a page. The processor loads these pages in the fixed sized chunks of memory called frames. Typically the size of each page
is always equal to the frame size.
A page fault occurs when a page is not found in the memory, and needs to be loaded from the disk. If a page fault occurs and all memory frames have been already allocated, then replacement of a page in memory is
required on the request of a new page. This is referred to as demand-paging. The choice of which page to replace is specified by a page replacement algorithms. The commonly used page replacement algorithms are FIFO,
LRU, optimal page replacement algorithms etc.
Generally, on increasing the number of frames to a process’ virtual memory, its execution becomes faster as less number of page faults occur. Sometimes the reverse happens, i.e. more number of page faults occur when
more frames are allocated to a process. This most unexpected result is termed as Belady’s Anomaly.

Bélády’s anomaly is the name given to the phenomenon where increasing the number of page frames results in an increase in the number of page faults for a given memory access pattern.
This phenomenon is commonly experienced in following page replacement algorithms:
1. First in first out (FIFO)
2. Second chance algorithm
3. Random page replacement algorithm
Reason of Belady’s Anomaly –
The other two commonly used page replacement algorithms are Optimal and LRU, but Belady’s Anamoly can never occur in these algorithms for any reference string as they belong to a class of stack based page
replacement algorithms.
A stack based algorithm is one for which it can be shown that the set of pages in memory for N frames is always a subset of the set of pages that would be in memory with N + 1 frames. For LRU replacement, the set of
pages in memory would be the n most recently referenced pages. If the number of frames increases then these n pages will still be the most recently referenced and so, will still be in the memory. While in FIFO, if a page
named b came into physical memory before a page – a then priority of replacement of b is greater than that of a, but this is not independent of the number of page frames and hence, FIFO does not follow a stack page
replacement policy and therefore suffers Belady’s Anomaly.
Example: Consider the following diagram to understand the behaviour of a stack-based page replacement algorithm

The diagram illustrates that given the set of pages i.e. {0, 1, 2} in 3 frames of memory is not a subset of the pages in memory – {0, 1, 4, 5} with 4 frames and it is a violation in the property of stack based algorithms. This
situation can be frequently seen in FIFO algorithm.
Belady’s Anomaly in FIFO –
Assuming a system that has no pages loaded in the memory and uses the FIFO Page replacement algorithm. Consider the following reference string:
1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5

Case-1: If the system has 3 frames, the given reference string on using FIFO page replacement algorithm yields a total of 9 page faults. The diagram below illustrates the pattern of the page faults occurring in the example.

Case-2: If the system has 4 frames, the given reference string on using FIFO page replacement algorithm yields a total of 10 page faults. The diagram below illustrates the pattern of the page faults occurring in the
example.

It can be seen from the above example that on increasing the number of frames while using the FIFO page replacement algorithm, the number of page faults increased from 9 to 10.
Note – It is not necessary that every string reference pattern cause Belady anomaly in FIFO but there are certain kind of string references that worsen the FIFO performance on increasing the number of frames.
Why Stack based algorithms do not suffer Anomaly –
All the stack based algorithms never suffer Belady Anomaly because these type of algorithms assigns a priority to a page (for replacement) that is independent of the number of page frames. Examples of such policies are
Optimal, LRU and LFU. Additionally these algorithms also have a good property for simulation, i.e. the miss (or hit) ratio can be computed for any number of page frames with a single pass through the reference string.
In LRU algorithm every time a page is referenced it is moved at the top of the stack, so, the top n pages of the stack are the n most recently used pages.Even if the number of frames are incremented to n+1, top of the stack
will have n+1 most recently used pages.
Similar example can be used to calculate the number of page faults in LRU algorithm. Assuming a system that has no pages loaded in the memory and uses the LRU Page replacement algorithm. Consider the following
reference string:
1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5

Case-1: If the system has 3 frames, the given reference string on using LRU page replacement algorithm yields a total of 10 page faults. The diagram below illustrates the pattern of the page faults occurring in the
example.

Case-2: If the system has 4 frames, the given reference string on using LRU page replacement algorithm, then total 8 page faults occur. The diagram shows the pattern of the page faults in the example.

Conclusion –
Various factors substantially affect the number of page faults, such as reference string length and the number of free page frames available. Anomalies also occurs due to the small cache size as well as the reckless rate of
change of the contents of cache. Also, the situation of fixed number of page faults even after increasing the number of frames can also be seen as an anomaly. Often algorithms like Random page replacement algorithm
are also susceptible to Belady’s Anomaly, because it may behave like first in first out (FIFO) page replacement algorithm. But Stack based algorithms are generally immune to all such situations as they are guaranteed to
give better page hits when the frames are incremented.
GATE CS Corner questions –
Practicing the following questions will help you test your knowledge. All questions have been asked in GATE in previous years or in GATE Mock Tests. It is highly recommended that you practice them.
1.
2.
3.
4.
5.
6.

GATE-CS-2001 | Question 21
GATE-CS-2009 | Question 8
ISRO CS 2011 | Question 73
GATE-CS-2016 (Set 2) | Question 30
ISRO CS 2016 | Question 48
GATE CS Mock 2018 | Question 63

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/beladys-anomaly-in-page-replacement-algorithms/
✍
Write a Testimonial

Least Frequently Used (LFU) Cache Implementation
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Least Frequently Used (LFU) Cache Implementation - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Least Frequently Used (LFU) is a caching algorithm in which the least frequently used cache block is removed whenever the cache is overflowed. In LFU we check the old page as well as the frequency of that page and
if the frequency of the page is larger than the old page we cannot remove it and if all the old pages are having same frequency then take last i.e FIFO method for that and remove that page.
Min-heap data structure is a good option to implement this algorithm, as it handles insertion, deletion, and update in logarithmic time complexity. A tie can be resolved by removing the least recently used cache block.
The following two containers have been used to solve the problem:
A vector of integer pairs has been used to represent the cache, where each pair consists of the block number and the number of times it has been used. The vector is ordered in the form of a min-heap, which allows us
to access the least frequently used block in constant time.
A hashmap has been used to store the indices of the cache blocks which allows searching in constant time.
Below is the implementation of the above approach:

filter_none
edit
close
play_arrow
link
brightness_4
code
// C++ program for LFU cache implementation
#include <bits/stdc++.h>
using namespace std;
// Generic function to swap two pairs
void swap(pair<int, int>& a, pair<int, int>& b)
{
pair<int, int> temp = a;
a = b;
b = temp;
}
// Returns the index of the parent node
inline int parent(int i)
{
return (i - 1) / 2;
}
// Returns the index of the left child node
inline int left(int i)
{
return 2 * i + 1;
}
// Returns the index of the right child node
inline int right(int i)
{
return 2 * i + 2;
}
// Self made heap tp Rearranges
// the nodes in order to maintain the heap property
void heapify(vector<pair<int, int> >& v,

unordered_map<int, int>& m, int i, int n)
{
int l = left(i), r = right(i), minim;
if (l < n)
minim = ((v[i].second < v[l].second) ? i : l);
else
minim = i;
if (r < n)
minim = ((v[minim].second < v[r].second) ? minim : r);
if (minim != i) {
m[v[minim].first] = i;
m[v[i].first] = minim;
swap(v[minim], v[i]);
heapify(v, m, minim, n);
}
}
// Function to Increment the frequency
// of a node and rearranges the heap
void increment(vector<pair<int, int> >& v,
unordered_map<int, int>& m, int i, int n)
{
++v[i].second;
heapify(v, m, i, n);
}
// Function to Insert a new node in the heap
void insert(vector<pair<int, int> >& v,
unordered_map<int, int>& m, int value, int& n)
{
if (n == v.size()) {
m.erase(v[0].first);
cout << "Cache block " << v[0].first
<< " removed.\n";
v[0] = v[--n];
heapify(v, m, 0, n);
}
v[n++] = make_pair(value, 1);
m.insert(make_pair(value, n - 1));
int i = n - 1;
// Insert a node in the heap by swapping elements
while (i && v[parent(i)].second > v[i].second) {
m[v[i].first] = parent(i);
m[v[parent(i)].first] = i;
swap(v[i], v[parent(i)]);
i = parent(i);
}
cout << "Cache block " << value << " inserted.\n";
}
// Function to refer to the block value in the cache
void refer(vector<pair<int, int> >& cache, unordered_map<int,
int>& indices, int value, int& cache_size)
{
if (indices.find(value) == indices.end())
insert(cache, indices, value, cache_size);
else
increment(cache, indices, indices[value], cache_size);
}
// Driver Code
int main()
{
int cache_max_size = 4, cache_size = 0;
vector<pair<int, int> > cache(cache_max_size);
unordered_map<int, int> indices;
refer(cache, indices, 1, cache_size);
refer(cache, indices, 2, cache_size);
refer(cache, indices, 1, cache_size);
refer(cache, indices, 3, cache_size);
refer(cache, indices, 2, cache_size);
refer(cache, indices, 4, cache_size);
refer(cache, indices, 5, cache_size);
return 0;
}

chevron_right
filter_none
Output:
​
Cache
Cache
Cache
Cache
Cache
Cache

block
block
block
block
block
block

1
2
3
4
3
5

inserted.​
inserted.​
inserted.​
inserted.​
removed.​
inserted.​

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : vaibhav29498

Source
https://www.geeksforgeeks.org/least-frequently-used-lfu-cache-implementation/
✍
Write a Testimonial

Producer Consumer Problem using Semaphores | Set 1
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Producer Consumer Problem using Semaphores | Set 1 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Semaphores in operating system, Inter Process Communication
Producer consumer problem is a classical synchronization problem. We can solve this problem by using semaphores.
A semaphore S is an integer variable that can be accessed only through two standard operations : wait() and signal().
The wait() operation reduces the value of semaphore by 1 and the signal() operation increases its value by 1.
​

wait(S){​
while(S<=0);
S--;​
}​
​
signal(S){​
S++;​
}​

// busy waiting​

Semaphores are of two types:

1. Binary Semaphore – This is similar to mutex lock but not the same thing. It can have only two values – 0 and 1. Its value is initialized to 1. It is used to implement the solution of critical section problem with
multiple processes.
2. Counting Semaphore – Its value can range over an unrestricted domain. It is used to control access to a resource that has multiple instances.
Problem Statement – We have a buffer of fixed size. A producer can produce an item and can place in the buffer. A consumer can pick items and can consume them. We need to ensure that when a producer is placing an
item in the buffer, then at the same time consumer should not consume any item. In this problem, buffer is the critical section.
To solve this problem, we need two counting semaphores – Full and Empty. “Full” keeps track of number of items in the buffer at any given time and “Empty” keeps track of number of unoccupied slots.
Initialization of semaphores –
mutex = 1
Full = 0 // Initially, all slots are empty. Thus full slots are 0
Empty = n // All slots are empty initially
Solution for Producer –
​
do{​
​
//produce an item​
​
wait(empty);​
wait(mutex);​
​
//place in buffer​
​
signal(mutex);​
signal(full);​
​
}while(true)​

When producer produces an item then the value of “empty” is reduced by 1 because one slot will be filled now. The value of mutex is also reduced to prevent consumer to access the buffer. Now, the producer has placed
the item and thus the value of “full” is increased by 1. The value of mutex is also increased by 1 beacuse the task of producer has been completed and consumer can access the buffer.
Solution for Consumer –
​
do{​
​
wait(full);​
wait(mutex);​
​
// remove item from buffer​
​
signal(mutex);​
signal(empty);​
​
// consumes item​
​
}while(true)​

As the consumer is removing an item from buffer, therefore the value of “full” is reduced by 1 and the value is mutex is also reduced so that the producer cannot access the buffer at this moment. Now, the consumer has
consumed the item, thus increasing the value of “empty” by 1. The value of mutex is also increased so that producer can access the buffer now.
See for implementation – Producer-Consumer solution using Semaphores in Java | Set 2

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : ShikhinDahikar

Source
https://www.geeksforgeeks.org/producer-consumer-problem-using-semaphores-set-1/
✍
Write a Testimonial

Requirements of Memory Management System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Requirements of Memory Management System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Memory management keeps track of the status of each memory location, whether it is allocated or free. It allocates the memory dynamically to the programs at their request and frees it for reuse when it is no longer
needed. Memory management meant to satisfy some requirements that we should keep in mind.

These Requirements of memory management are:
1. Relocation – The available memory is generally shared among a number of processes in a multiprogramming system, so it is not possible to know in advance which other programs will be resident in main memory
at the time of execution of his program. Swapping the active processes in and out of the main memory enables the operating system to have a larger pool of ready-to-execute process.
When a program gets swapped out to a disk memory, then it is not always possible that when it is swapped back into main memory then it occupies the previous memory location, since the location may still be
occupied by another process. We may need to relocate the process to a different area of memory. Thus there is a possibility that program may be moved in main memory due to swapping.

The figure depicts a process image. The process image is occupying a continuous region of main memory. The operating system will need to know many things including the location of process control information,
the execution stack, and the code entry. Within a program, there are memory references in various instructions and these are called logical addresses.
After loading of the program into main memory, the processor and the operating system must be able to translate logical addresses into physical addresses. Branch instructions contain the address of the next
instruction to be executed. Data reference instructions contain the address of byte or word of data referenced.
2. Protection – There is always a danger when we have multiple programs at the same time as one program may write to the address space of another program. So every process must be protected against unwanted
interference when other process tries to write in a process whether accidental or incidental. Between relocation and protection requirement a trade-off occurs as the satisfaction of relocation requirement increases the
difficulty of satisfying the protection requirement.
Prediction of the location of a program in main memory is not possible, that’s why it is impossible to check the absolute address at compile time to assure protection. Most of the programming language allows the
dynamic calculation of address at run time. The memory protection requirement must be satisfied by the processor rather than the operating system because the operating system can hardly control a process when it
occupies the processor. Thus it is possible to check the validity of memory references.
3. Sharing – A protection mechanism must have to allow several processes to access the same portion of main memory. Allowing each processes access to the same copy of the program rather than have their own
separate copy has an advantage.
For example, multiple processes may use the same system file and it is natural to load one copy of the file in main memory and let it shared by those processes. It is the task of Memory management to allow
controlled access to the shared areas of memory without compromising the protection. Mechanisms are used to support relocation supported sharing capabilities.
4. Logical organization – Main memory is organized as linear or it can be a one-dimensional address space which consists of a sequence of bytes or words. Most of the programs can be organized into modules, some
of those are unmodifiable (read-only, execute only) and some of those contain data that can be modified. To effectively deal with a user program, the operating system and computer hardware must support a basic
module to provide the required protection and sharing. It has the following advantages:
Modules are written and compiled independently and all the references from one module to another module are resolved by `the system at run time.
Different modules are provided with different degrees of protection.
There are mechanisms by which modules can be shared among processes. Sharing can be provided on a module level that lets the user specify the sharing that is desired.
5. Physical organization – The structure of computer memory has two levels referred to as main memory and secondary memory. Main memory is relatively very fast and costly as compared to the secondary memory.
Main memory is volatile. Thus secondary memory is provided for storage of data on a long-term basis while the main memory holds currently used programs. The major system concern between main memory and
secondary memory is the flow of information and it is impractical for programmers to understand this for two reasons:
The programmer may engage in a practice known as overlaying when the main memory available for a program and its data may be insufficient. It allows different modules to be assigned to the same region of
memory. One disadvantage is that it is time-consuming for the programmer.
In a multiprogramming environment, the programmer does not know how much space will be available at the time of coding and where that space will be located inside the memory.
Reference: Internals and design Principles,7th edition by William Stallings

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : VaibhavRai3

Source
https://www.geeksforgeeks.org/requirements-of-memory-management-system/
✍
Write a Testimonial

Resource Allocation Techniques for Processes
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Resource Allocation Techniques for Processes - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
The Operating System allocates resources when a program need them. When the program terminates, the resources are de-allocated, and allocated to other programs that need them. Now the question is, what strategy does
the operating system use to allocate these resources to user programs?
There are two Resource allocation techniques:
1. Resource partitioning approach –
In this approach, the operating system decides beforehand, that what resources should be allocated to which user program. It divides the resources in the system to many resource partitions, where each partition may
include various resources – for example, 1 MB memory, disk blocks, and a printer.
Then, it allocates one resource partition to each user program before the program’s initiation. A resource table records the resource partition and its current allocation status (Allocated or Free).
Advantages:
Easy to Implement
Less Overhead
Disadvantages:
Lacks flexibility – if a resource partition contains more resources than what a particular process requires, the additional resources are wasted.
If a program needs more resources than a single resource partition, it cannot execute (Though free resources are present in other partitions).
An example resource table may look like:

2. Pool based approach –
In this approach, there is a common pool of resources. The operating System checks the allocation status in the resource table whenever a program makes a request for a resource. If the resource is free, it allocates
the resource to the program.
Advantages:
Allocated resources are not wasted.
Any resource requirement can be fulfilled if the resource is free (unlike Partitioning approach)
Disadvantages:
Overhead of allocating and de-allocating the resources on every request and release.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : Harshit kalal

Source
https://www.geeksforgeeks.org/resource-allocation-techniques-for-processes/
✍
Write a Testimonial

Multithreading in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Multithreading in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
A thread is a path which is followed during a program’s execution. Majority of programs written now a days run as a single thread.Lets say, for example a program is not capable of reading keystrokes while making
drawings. These tasks cannot be executed by the program at the same time. This problem can be solved through multitasking so that two or more tasks can be executed simultaneously.
Multitasking is of two types: Processor based and thread based. Processor based multitasking is totally managed by the OS, however multitasking through multithreading can be controlled by the programmer to some
extent.
The concept of multi-threading needs proper understanding of these two terms – a process and a thread. A process is a program being executed. A process can be further divided into independent units known as
threads.

A thread is like a small light-weight process within a process. Or we can say a collection of threads is what is known as a process.

Applications –
Threading is used widely in almost every field. Most widely it is seen over the internet now days where we are using transaction processing of every type like recharges, online transfer, banking etc. Threading is a
segment which divide the code into small parts that are of very light weight and has less burden on CPU memory so that it can be easily worked out and can achieve goal in desired field. The concept of threading is
designed due to the problem of fast and regular changes in technology and less the work in different areas due to less application. Then as says “need is the generation of creation or innovation” hence by following this
approach human mind develop the concept of thread to enhance the capability of programming.
Refer for Multi threading models, benefits of Multithreading, and difference between multitasking, multithreading and multiprocessing.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/multithreading-in-operating-system/
✍
Write a Testimonial

Benefits of Multithreading in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Benefits of Multithreading in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Operating-System-Thread
The benefits of multi threaded programming can be broken down into four major categories:
1. Responsiveness –
Multithreading in an interactive application may allow a program to continue running even if a part of it is blocked or is performing a lengthy operation, thereby increasing responsiveness to the user.
In a non multi threaded environment, a server listens to the port for some request and when the request comes, it processes the request and then resume listening to another request. The time taken while processing of
request makes other users wait unnecessarily. Instead a better approach would be to pass the request to a worker thread and continue listening to port.

For example, a multi threaded web browser allow user interaction in one thread while an video is being loaded in another thread. So instead of waiting for the whole web-page to load the user can continue viewing
some portion of the web-page.
2. Resource Sharing –
Processes may share resources only through techniques such asMessage Passing
Shared Memory
Such techniques must be explicitly organized by programmer. However, threads share the memory and the resources of the process to which they belong by default.
The benefit of sharing code and data is that it allows an application to have several threads of activity within same address space.
3. Economy –
Allocating memory and resources for process creation is a costly job in terms of time and space.
Since, threads share memory with the process it belongs, it is more economical to create and context switch threads. Generally much more time is consumed in creating and managing processes than in threads.
In Solaris, for example, creating process is 30 times slower than creating threads and context switching is 5 times slower.
4. Scalability –
The benefits of multi-programming greatly increase in case of multiprocessor architecture, where threads may be running parallel on multiple processors. If there is only one thread then it is not possible to divide the
processes into smaller tasks that different processors can perform.
Single threaded process can run only on one processor regardless of how many processors are available.
Multi-threading on a multiple CPU machine increases parallelism.
References- Operating System concepts by Abraham Silberschatz, Peter B. Galvin& Greg Gagne

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/benefits-of-multithreading-in-operating-system/
✍
Write a Testimonial

Deadlock Detection Algorithm in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Deadlock Detection Algorithm in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
If a system does not employ either a deadlock prevention or deadlock avoidance algorithm then a deadlock situation may occur. In this caseApply an algorithm to examine state of system to determine whether deadlock has has occurred or not.
Apply an algorithm to recover from the deadlock. For more refer- Deadlock Recovery
Deadlock Detection Algorithm/ Bankers Algorithm:
The algorithm employs several time varying data structures:
Available- A vector of length m indicates the number of available resources of each type.
Allocation- An n*m matrix defines the number of resources of each type currently allocated to a process. Column represents resource and resource represent process.
Request- An n*m matrix indicates the current request of each process. If request[i][j] equals k then process Pi is requesting k more instances of resource type R j.
We treat rows in the matrices Allocation and Request as vectors, we refer them as Allocationi and Requesti.

Steps of Algorithm:
1. Let Work and Finish be vectors of length m and n respectively. Initialize Work= Available. For i=0, 1, …., n-1, if Requesti = 0, then Finish[i] = true; otherwise, Finish[i]= false.
2. Find an index i such that both
a) Finish[i] == false
b) Requesti <= Work
If no such i exists go to step 4.
3. Work= Work+ Allocationi
Finish[i]= true
Go to Step 2.
4. If Finish[i]== false for some i, 0<=i<n, then the system is in a deadlocked state. Moreover, if Finish[i]==false the process P i is deadlocked.
For example,

1. In this, Work = [0, 0, 0] &
Finish = [false, false, false, false, false]
2. i=0 is selected as both Finish[0] = false and [0, 0, 0]<=[0, 0, 0].
3. Work =[0, 0, 0]+[0, 1, 0] =>[0, 1, 0] &
Finish = [true, false, false, false, false].
4. i=2 is selected as both Finish[2] = false and [0, 0, 0]<=[0, 1, 0].
5. Work =[0, 1, 0]+[3, 0, 3] =>[3, 1, 3] &
Finish = [true, false, true, false, false].
6. i=1 is selected as both Finish[1] = false and [2, 0, 2]<=[3, 1, 3].
7. Work =[3, 1, 3]+[2, 0, 2] =>[5, 1, 5] &
Finish = [true, true, true, false, false].
8. i=3 is selected as both Finish[3] = false and [1, 0, 0]<=[5, 1, 5].
9. Work =[5, 1, 5]+[2, 1, 1] =>[7, 2, 6] &
Finish = [true, true, true, true, false].
10. i=4 is selected as both Finish[4] = false and [0, 0, 2]<=[7, 2, 6].
11. Work =[7, 2, 6]+[0, 0, 2] =>[7, 2, 8] &
Finish = [true, true, true, true, true].
12. Since Finish is a vector of all true it means there is no deadlock in this example.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : EduardoNodarse, VaibhavRai3, rasswanthsenthil, Nishant Raj 1

Source
https://www.geeksforgeeks.org/deadlock-detection-algorithm-in-operating-system/
✍
Write a Testimonial

Techniques to handle Thrashing
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Techniques to handle Thrashing - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Virtual Memory
Thrashing is a condition or a situation when the system is spending a major portion of its time in servicing the page faults, but the actual processing done is very negligible.

The basic concept involved is that if a process is allocated too few frames, then there will be too many and too frequent page faults. As a result, no useful work would be done by the CPU and the CPU utilisation would
fall drastically. The long-term scheduler would then try to improve the CPU utilisation by loading some more processes into the memory thereby increasing the degree of multiprogramming. This would result in a further
decrease in the CPU utilization triggering a chained reaction of higher page faults followed by an increase in the degree of multiprogramming, called Thrashing.
Locality Model –
A locality is a set of pages that are actively used together. The locality model states that as a process executes, it moves from one locality to another. A program is generally composed of several different localities which
may overlap.
For example when a function is called, it defines a new locality where memory references are made to the instructions of the function call, it’s local and global variables, etc. Similarly, when the function is exited, the
process leaves this locality.
Techniques to handle:
1. Working Set Model –
This model is based on the above-stated concept of the Locality Model.
The basic principle states that if we allocate enough frames to a process to accommodate its current locality, it will only fault whenever it moves to some new locality. But if the allocated frames are lesser than the
size of the current locality, the process is bound to thrash.
According to this model, based on a parameter A, the working set is defined as the set of pages in the most recent ‘A’ page references. Hence, all the actively used pages would always end up being a part of the
working set.
The accuracy of the working set is dependant on the value of parameter A. If A is too large, then working sets may overlap. On the other hand, for smaller values of A, the locality might not be covered entirely.
If D is the total demand for frames and

is the working set size for a process i,

Now, if ‘m’ is the number of frames available in the memory, there are 2 possibilities:
(i) D>m i.e. total demand exceeds the number of frames, then thrashing will occur as some processes would not get enough frames.
(ii) D<=m, then there would be no thrashing.
If there are enough extra frames, then some more processes can be loaded in the memory. On the other hand, if the summation of working set sizes exceeds the availability of frames, then some of the processes have
to be suspended(swapped out of memory).

This technique prevents thrashing along with ensuring the highest degree of multiprogramming possible. Thus, it optimizes CPU utilisation.
2. Page Fault Frequency –
A more direct approach to handle thrashing is the one that uses Page-Fault Frequency concept.

The problem associated with Thrashing is the high page fault rate and thus, the concept here is to control the page fault rate.
If the page fault rate is too high, it indicates that the process has too few frames allocated to it. On the contrary, a low page fault rate indicates that the process has too many frames.
Upper and lower limits can be established on the desired page fault rate as shown in the diagram.
If the page fault rate falls below the lower limit, frames can be removed from the process. Similarly, if the page fault rate exceeds the upper limit, more number of frames can be allocated to the process.
In other words, the graphical state of the system should be kept limited to the rectangular region formed in the given diagram.
Here too, if the page fault rate is high with no free frames, then some of the processes can be suspended and frames allocated to them can be reallocated to other processes. The suspended processes can then be
restarted later.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/techniques-to-handle-thrashing/
✍
Write a Testimonial

Program for Preemptive Priority CPU Scheduling
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Program for Preemptive Priority CPU Scheduling - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Implementing priority CPU scheduling. In this problem, we are using Min Heap as the data structure for implementing priority scheduling.
In this problem smaller numbers denote higher priority.
The following functions are used in the given code below:
struct process {​
processID,​
burst time,​
response time,​
priority,​
arrival time.​
}

void quicksort(process array[], low, high)– This function is used to arrange the processes in ascending order according to their arrival time.
int partition(process array[], int low, int high)– This function is used to partition the array for sorting.
void insert(process Heap[], process value, int *heapsize, int *currentTime)– It is used to include all the valid and eligible processes in the heap for execution. heapsize defines the number of processes in execution
depending on the current time currentTime keeps record of the current CPU time.
void order(process Heap[], int *heapsize, int start)– It is used to reorder the heap according to priority if the processes after insertion of new process.
void extractminimum(process Heap[], int *heapsize, int *currentTime)– This function is used to find the process with highest priority from the heap. It also reorders the heap after extracting the highest priority
process.
void scheduling(process Heap[], process array[], int n, int *heapsize, int *currentTime)– This function is responsible for executing the highest priority extracted from Heap[].
void process(process array[], int n)– This function is responsible for managing the entire execution of the processes as they arrive in the CPU according to their arrival time.
filter_none
edit
close
play_arrow
link
brightness_4
code

// CPP program to implement preemptive priority scheduling
#include <bits/stdc++.h>
using namespace std;
struct Process {
int processID;
int burstTime;
int tempburstTime;
int responsetime;
int arrivalTime;
int priority;
int outtime;
int intime;
};
// It is used to include all the valid and eligible
// processes in the heap for execution. heapsize defines
// the number of processes in execution depending on
// the current time currentTime keeps a record of
// the current CPU time.
void insert(Process Heap[], Process value, int* heapsize,
int* currentTime)
{
int start = *heapsize, i;
Heap[*heapsize] = value;
if (Heap[*heapsize].intime == -1)
Heap[*heapsize].intime = *currentTime;
++(*heapsize);
// Ordering the Heap
while (start != 0 && Heap[(start - 1) / 2].priority >
Heap[start].priority) {
Process temp = Heap[(start - 1) / 2];
Heap[(start - 1) / 2] = Heap[start];
Heap[start] = temp;
start = (start - 1) / 2;
}
}
// It is used to reorder the heap according to
// priority if the processes after insertion
// of new process.
void order(Process Heap[], int* heapsize, int start)
{
int smallest = start;
int left = 2 * start + 1;
int right = 2 * start + 2;
if (left < *heapsize && Heap[left].priority <
Heap[smallest].priority)
smallest = left;
if (right < *heapsize && Heap[right].priority <
Heap[smallest].priority)
smallest = right;
// Ordering the Heap
if (smallest != start) {
Process temp = Heap[smallest];
Heap[smallest] = Heap[start];
Heap[start] = temp;
order(Heap, heapsize, smallest);
}
}
// This function is used to find the process with
// highest priority from the heap. It also reorders
// the heap after extracting the highest priority process.
Process extractminimum(Process Heap[], int* heapsize,
int* currentTime)
{
Process min = Heap[0];
if (min.responsetime == -1)
min.responsetime = *currentTime - min.arrivalTime;
--(*heapsize);
if (*heapsize >= 1) {
Heap[0] = Heap[*heapsize];
order(Heap, heapsize, 0);
}
return min;
}
// Compares two intervals according to staring times.
bool compare(Process p1, Process p2)
{
return (p1.arrivalTime < p2.arrivalTime);
}
// This function is responsible for executing
// the highest priority extracted from Heap[].
void scheduling(Process Heap[], Process array[], int n,
int* heapsize, int* currentTime)
{
if (heapsize == 0)
return;
Process min = extractminimum(Heap, heapsize, currentTime);
min.outtime = *currentTime + 1;
--min.burstTime;
printf("process id = %d current time = %d\n",
min.processID, *currentTime);
// If the process is not yet finished
// insert it back into the Heap*/
if (min.burstTime > 0) {
insert(Heap, min, heapsize, currentTime);
return;
}
for (int i = 0; i < n; i++)
if (array[i].processID == min.processID) {
array[i] = min;
break;
}
}
// This function is responsible for
// managing the entire execution of the
// processes as they arrive in the CPU
// according to their arrival time.
void priority(Process array[], int n)
{
sort(array, array + n, compare);
int totalwaitingtime = 0, totalbursttime = 0,
totalturnaroundtime = 0, i, insertedprocess = 0,
heapsize = 0, currentTime = array[0].arrivalTime,
totalresponsetime = 0;
Process Heap[4 * n];
// Calculating the total burst time
// of the processes
for (int i = 0; i < n; i++) {
totalbursttime += array[i].burstTime;
array[i].tempburstTime = array[i].burstTime;
}
// Inserting the processes in Heap
// according to arrival time
do {

if (insertedprocess != n) {
for (i = 0; i < n; i++) {
if (array[i].arrivalTime == currentTime) {
++insertedprocess;
array[i].intime = -1;
array[i].responsetime = -1;
insert(Heap, array[i], &heapsize, &currentTime);
}
}
}
scheduling(Heap, array, n, &heapsize, &currentTime);
++currentTime;
if (heapsize == 0 && insertedprocess == n)
break;
} while (1);
for (int i = 0; i < n; i++) {
totalresponsetime += array[i].responsetime;
totalwaitingtime += (array[i].outtime - array[i].intime array[i].tempburstTime);
totalbursttime += array[i].burstTime;
}
printf("Average waiting time = %f\n",
((float)totalwaitingtime / (float)n));
printf("Average response time =%f\n",
((float)totalresponsetime / (float)n));
printf("Average turn around time = %f\n",
((float)(totalwaitingtime + totalbursttime) / (float)n));
}
// Driver code
int main()
{
int n, i;
Process a[5];
a[0].processID = 1;
a[0].arrivalTime = 4;
a[0].priority = 2;
a[0].burstTime = 6;
a[1].processID = 4;
a[1].arrivalTime = 5;
a[1].priority = 1;
a[1].burstTime = 3;
a[2].processID = 2;
a[2].arrivalTime = 5;
a[2].priority = 3;
a[3].burstTime = 7;
a[3].processID = 3;
a[3].arrivalTime = 1;
a[3].priority = 4;
a[3].burstTime = 2;
a[4].processID = 5;
a[4].arrivalTime = 3;
a[4].priority = 5;
a[4].burstTime = 4;
priority(a, 5);
return 0;
}

chevron_right
filter_none
Output:
​
process
process
process
process
process
process
process
process
process
process
process
process
process
process
process
process
Average
Average
Average

id = 3 current time = 1​
id = 3 current time = 2​
id = 5 current time = 3​
id = 1 current time = 4​
id = 4 current time = 5​
id = 4 current time = 6​
id = 4 current time = 7​
id = 1 current time = 8​
id = 1 current time = 9​
id = 1 current time = 10​
id = 1 current time = 11​
id = 1 current time = 12​
id = 2 current time = 13​
id = 5 current time = 14​
id = 5 current time = 15​
id = 5 current time = 16​
waiting time = 4.400000​
response time =1.600000​
turn around time = 7.200000​

The output displays the order in which the processes are executed in the memory and also shows the average waiting time, average response time and average turn around time for each process.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/program-for-preemptive-priority-cpu-scheduling/
✍
Write a Testimonial

Threads and its types in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Threads and its types in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Thread is a single sequence stream within a process. Threads have same properties as of the process so they are called as light weight processes. Threads are executed one after another but gives the illusion as if they are
executing in parallel. Each thread has different states. Each thread has
1. A program counter
2. A register set
3. A stack space
Threads are not independent of each other as they share the code, data, OS resources etc.
Similarity between Threads and Processes –

Only one thread or process is active at a time
Within process both execute sequentiall
Both can create children
Differences between Threads and Processes –
Threads are not independent, processes are.
Threads are designed to assist each other, processes may or may not do it
Types of Threads:
1. User Level thread (ULT) –
Is implemented in the user level library, they are not created using the system calls. Thread switching does not need to call OS and to cause interrupt to Kernel. Kernel doesn’t know about the user level thread and
manages them as if they were single-threaded processes.
Advantages of ULT –
Can be implemented on an OS that does’t support multithreading.
Simple representation since thread has only program counter, register set, stack space.
Simple to create since no intervention of kernel.
Thread switching is fast since no OS calls need to be made.
Disadvantages of ULT –
No or less co-ordination among the threads and Kernel.
If one thread causes a page fault, the entire process blocks.
2. Kernel Level Thread (KLT) –
Kernel knows and manages the threads. Instead of thread table in each process, the kernel itself has thread table (a master one) that keeps track of all the threads in the system. In addition kernel also maintains the
traditional process table to keep track of the processes. OS kernel provides system call to create and manage threads.
Advantages of KLT –
Since kernel has full knowledge about the threads in the system, scheduler may decide to give more time to processes having large number of threads.
Good for applications that frequently block.
Disadvantages of KLT –
Slow and inefficient.
It requires thread control block so it is an overhead.
Summary:
1. Each ULT has a process that keeps track of the thread using the Thread table.
2. Each KLT has Thread Table (TCB) as well as the Process Table (PCB).

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : wertyu, magbene

Source
https://www.geeksforgeeks.org/threads-and-its-types-in-operating-system/
✍
Write a Testimonial

States of a Process in Operating Systems
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ States of a Process in Operating Systems - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Introduction, Process Scheduler
States of a process are as following:

New (Create) – In this step, the process is about to be created but not yet created, it is the program which is present in secondary memory that will be picked up by OS to create the process.
Ready – New -> Ready to run. After the creation of a process, the process enters the ready state i.e. the process is loaded into the main memory. The process here is ready to run and is waiting to get the CPU time
for its execution. Processes that are ready for execution by the CPU are maintained in a queue for ready processes.
Run – The process is chosen by CPU for execution and the instructions within the process are executed by any one of the available CPU cores.
Blocked or wait – Whenever the process requests access to I/O or needs input from the user or needs access to a critical region(the lock for which is already acquired) it enters the blocked or wait state. The process
continues to wait in the main memory and does not require CPU. Once the I/O operation is completed the process goes to the ready state.
Terminated or completed – Process is killed as well as PCB is deleted.
Suspend ready – Process that was initially in the ready state but were swapped out of main memory(refer Virtual Memory topic) and placed onto external storage by scheduler are said to be in suspend ready state.
The process will transition back to ready state whenever the process is again brought onto the main memory.
Suspend wait or suspend blocked – Similar to suspend ready but uses the process which was performing I/O operation and lack of main memory caused them to move to secondary memory.
When work is finished it may go to suspend ready.
CPU and IO Bound Processes:
If the process is intensive in terms of CPU operations then it is called CPU bound process. Similarly, If the process is intensive in terms of I/O operations then it is called IO bound process.

Types of schedulers:
1. Long term – performance – Makes a decision about how many processes should be made to stay in the ready state, this decides the degree of multiprogramming. Once a decision is taken it lasts for a long time
hence called long term scheduler.
2. Short term – Context switching time – Short term scheduler will decide which process to be executed next and then it will call dispatcher. A dispatcher is a software that moves process from ready to run and vice
versa. In other words, it is context switching.
3. Medium term – Swapping time – Suspension decision is taken by medium term scheduler. Medium term scheduler is used for swapping that is moving the process from main memory to secondary and vice versa.
Multiprogramming – We have many processes ready to run. There are two types of multiprogramming:
1. Pre-emption – Process is forcefully removed from CPU. Pre-emption is also called as time sharing or multitasking.
2. Non pre-emption – Processes are not removed until they complete the execution.
Degree of multiprogramming –
The number of processes that can reside in the ready state at maximum decides the degree of multiprogramming, e.g., if the degree of programming = 100, this means 100 processes can reside in the ready state at
maximum.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : kbapat, nitishanon, shreyashagrawal

Source
https://www.geeksforgeeks.org/states-of-a-process-in-operating-systems/
✍
Write a Testimonial

Inverted Page Table in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Inverted Page Table in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Paging, Page table entries, Segmentation
Most of the Operating Systems implement a separate pagetable for each process, i.e. for ‘n’ number of processes running on a Multiprocessing/ Timesharing operating system, there are ‘n’ number of pagetables stored in
the memory. Sometimes when a process is very large in size and it occupies virtual memory then with the size of the process, it’s pagetable size also increases substantially.
Example: A process of size 2 GB with:​
Page size = 512 Bytes​
Size of page table entry = 4 Bytes, then​
Number of pages in the process = 2 GB / 512 B = 222​
PageTable Size = 222 * 22 = 224 bytes

Through this example, it can be concluded that for multiple processes running simultaneously in an OS, a considerable part of memory is occupied by page tables only.
Operating Systems also incorporate multilevel paging schemes which further increase the space required for storing the page tables and a large amount of memory is invested in storing them. The amount of memory
occupied by the page tables can turn out to be a huge overhead and is always unacceptable as main memory is always a scarce resource. Various efforts are made to utilize the memory efficiently and to maintain a good
balance in the level of multiprogramming and efficient CPU utilization.

Inverted Page Table –
An alternate approach is to use the Inverted Page Table structure that consists of one-page table entry for every frame of the main memory. So the number of page table entries in the Inverted Page Table reduces to the
number of frames in physical memory and a single page table is used to represent the paging information of all the processes.
Through the inverted page table, the overhead of storing an individual page table for every process gets eliminated and only a fixed portion of memory is required to store the paging information of all the processes
together. This technique is called as inverted paging as the indexing is done with respect to the frame number instead of the logical page number. Each entry in the page table contains the following fields.
Page number – It specifies the page number range of the logical address.
Process id – An inverted page table contains the address space information of all the processes in execution. Since two different processes can have similar set of virtual addresses, it becomes necessary in Inverted
Page Table to store a process Id of each process to identify it’s address space uniquely. This is done by using the combination of PId and Page Number. So this Process Id acts as an address space identifier and
ensures that a virtual page for a particular process is mapped correctly to the corresponding physical frame.
Control bits – These bits are used to store extra paging-related information. These include the valid bit, dirty bit, reference bits, protection and locking information bits.
Chained pointer – It may be possible sometime that two or more processes share a part of main memory. In this case, two or more logical pages map to same Page Table Entry then a chaining pointer is used to map
the details of these logical pages to the root page table.
Working – The operation of an inverted page table is shown below.

The virtual address generated by the CPU contains the fields and each page table entry contains and the other relevant information required in paging related mechanism. When a memory reference takes place, this virtual
address is matched by the memory-mapping unit and the Inverted Page table is searched to match the and the corresponding frame number is obtained. If the match is found at the ith entry then the physical address of the
process, , is sent as the real address otherwise if no match is found then Segmentation Fault is generated.
Note: Number of Entries in Inverted page table = Number of frames in Physical address Space(PAS)
Examples – The Inverted Page table and its variations are implemented in various systems like PowerPC, UltraSPARC and the IA-64 architecture. An implementation of the Mach operating system on the RT-PC also uses
this technique.
Advantages and Disadvantages:
Reduced memory space –
Inverted Pagetables typically reduces the amount of memory required to store the page tables to a size bound of physical memory. The maximum number of entries could be the number of page frames in the physical
memory.
Longer lookup time –
Inverted Page tables are sorted in order of frame number but the memory look-up takes place with respect to the virtual address, so, it usually takes a longer time to find the appropriate entry but often these page
tables are implemented using hash data structures for a faster lookup.
Difficult shared memory implementation –
As the Inverted Page Table stores a single entry for each frame, it becomes difficult to implement the shared memory in the page tables. Chaining techniques are used to map more than one virtual address to the
entry specified in order of frame number.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : VaibhavRai3

Source
https://www.geeksforgeeks.org/inverted-page-table-in-operating-system/
✍
Write a Testimonial

Real Time Systems
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Real Time Systems - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Real time system means that the system is subjected to real time, i.e., response should be guaranteed within a specified timing constraint or system should meet the specified deadline. For example: flight control system,
real time monitors etc.
Types of real time systems based on timing constraints:
1. Hard real time system –
This type of sytem can never miss its deadline. Missing the deadline may have disastrous consequences.The usefulness of result produced by a hard real time system decreases abruptly and may become negative if
tardiness increases. Tardiness means how late a real time system completes its task with respect to its deadline. Example: Flight controller system.
2. Soft real time system –
This type of system can miss its deadline occasionally with some acceptably low probability. Missing the deadline have no disastrous consequences. The usefulness of result produced by a soft real time system
decreases gradually with increase in tardiness. Example: Telephone switches.
Reference model of real time system: Our reference model is characterized by three elements:
1. A workload model: It specifies the application supported by system.
2. A resource model: It specifies the resources available to the application.
3. Algorithms: It specifies how the application system will use resources.
Terms related to real time system:
Job – A job is a small piece of work that can be assigned to a processor and may or may not require resources.
Task – A set of related jobs that jointly provide some system functionality.
Release time of a job – It is the time at which job becomes ready for execution.
Execution time of a job – It is the time taken by job to finish its execution.
Deadline of a job – It is the time by which a job should finish its execution. Deadline is of two types: absolute deadline and relative deadline.
Response time of a job – It is the length of time from release time of a job to the instant when it finishes.
Maximum allowable response time of a job is called its relative deadline.
Absolute deadline of a job is equal to its relative deadline plus its release time.
Processors are also known as active resources. They are essential for execution of a job. A job must have one or more processors in order to execute and proceed towards completion. Example: computer,
transmission links.
Resources are also known as passive resources. A job may or may not require a resource during its execution. Example: memory, mutex
Two resources are identical if they can be used interchangeably else they are heterogeneous.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/real-time-systems/
✍
Write a Testimonial

Difference between dispatcher and scheduler
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between dispatcher and scheduler - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Schedulers are special system software which handle process scheduling in various ways. Their main task is to select the jobs to be submitted into the system and to decide which process to run. There are three types of
Scheduler:

1. Long term (job) scheduler – Due to the smaller size of main memory initially all program are stored in secondary memory. When they are stored or loaded in the main memory they are called process. This is the
decision of long term scheduler that how many processes will stay in the ready queue. Hence, in simple words, long term scheduler decides the degree of multi-programming of system.
2. Medium term scheduler – Most often, a running process needs I/O operation which doesn’t requires CPU. Hence during the execution of a process when a I/O operation is required then the operating system sends
that process from running queue to blocked queue. When a process completes its I/O operation then it should again be shifted to ready queue. ALL these decisions are taken by the medium-term scheduler. Mediumterm scheduling is a part of swapping.
3. Short term (CPU) scheduler – When there are lots of processes in main memory initially all are present in the ready queue. Among all of the process, a single process is to be selected for execution. This decision is
handled by short term scheduler.
Let’s have a look at the figure given below. It may make a more clear view for you.

Dispatcher –
A dispatcher is a special program which comes into play after the scheduler. When the scheduler completes its job of selecting a process, it is the dispatcher which takes that process to the desired state/queue. The
dispatcher is the module that gives a process control over the CPU after it has been selected by the short-term scheduler. This function involves the following:

Switching context
Switching to user mode
Jumping to the proper location in the user program to restart that program
The Difference between the Scheduler and Dispatcher –
Consider a situation, where various processes are residing in the ready queue waiting to be executed. The CPU cannot execute all of these processes simultaneously, so the operating system has to choose a particular
process on the basis of the scheduling algorithm used. So, this procedure of selecting a process among various processes is done by the scheduler. Once the scheduler has selected a process from the queue, the dispatcher
comes into the picture, and it is the dispatcher who takes that process from the ready queue and moves it into the running state. Therefore, the scheduler gives the dispatcher an ordered list of processes which the
dispatcher moves to the CPU over time.
Example –
There are 4 processes in the ready queue, P1, P2, P3, P4; Their arrival times are t0, t1, t2, t3 respectively. A First in First out (FIFO) scheduling algorithm is used. Because P1 arrived first, the scheduler will decide it is the
first process that should be executed, and the dispatcher will remove P1 from the ready queue and give it to the CPU. The scheduler will then determine P2 to be the next process that should be executed, so when the
dispatcher returns to the queue for a new process, it will take P2 and give it to the CPU. This continues in the same way for P3, and then P4.

Properties
Definition:
Types:

DISPATCHER
Dispatcher is a module that gives control of CPU to the process selected by short term scheduler
There are no diifrent types in dispatcher.It is just a code segment.

Dependency: Working of dispatcher is dependednt on scheduler.Means dispatcher have to wait untill scheduler selects a process.
Algorithm: Dispatcher has no specific algorithm for its implementation
Time Taken: The time taken by dispatcher is called dispatch latency.
Functions: Dispatcher is also responsible for:Context Switching, Switch to user mode, Jumping to proper location when process again restarted

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : VaibhavRai3, BenTrono

Source
https://www.geeksforgeeks.org/difference-between-dispatcher-and-scheduler/
✍
Write a Testimonial

SCHEDULER
Scheduler is something which selects a process among various processes
There are 3 types of scheduler i.e. Long-term, Short-term, Medium-term
Scheduler works idependently.It works immediately when needed
Scheduler works on various algorithm such as FCFS, SJF, RR etc.
TIme taken by scheduler is usually negligible.Hence we neglect it.
The only work of scheduler is selection of processes.

Hard Disk Drive (HDD) Secondary memory
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Hard Disk Drive (HDD) Secondary memory - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
A hard disk is a memory storage device which looks like this:

The disk is divided into tracks. Each track is further divided into sectors. The point to be noted here is that outer tracks are bigger in size than the inner tracks but they contain the same number of sectors and have equal
storage capacity. This is because the storage density is high in sectors of the inner tracks where as the bits are sparsely arranged in sectors of the outer tracks. Some space of every sector is used for formatting. So, the
actual capacity of a sector is less than the given capacity.

Read-Write(R-W) head moves over the rotating hard disk. It is this Read-Write head that performs all the read and write operations on the disk and hence, position of the R-W head is a major concern. To perform a read
or write operation on a memory location, we need to place the R-W head over that position. Some important terms must be noted here:
1.
2.
3.
4.
5.

Seek time – The time taken by the R-W head to reach the desired track from it’s current position.
Rotational latency – Time taken by the sector to come under the R-W head.
Data transfer time – Time taken to transfer the required amount of data. It depends upon the rotational speed.
Controller time – The processing time taken by the controller.
Average Access time – seek time + Average Rotational latency + data transfer time + controller time.

Note:Average Rotational latency is mostly 1/2*(Rotetional latency).
In questions, if the seek time and controller time is not mentioned, take them to be zero.
If the amount of data to be transferred is not given, assume that no data is being transferred. Otherwise, calculate the time taken to transfer the given amount of data.
The average of rotational latency is taken when the current position of R-W head is not given. Because, the R-W may be already present at the desired position or it might take a whole rotation to get the desired sector
under the R-W head. But, if the current position of the R-W head is given then the rotational latency must be calculated.
Example –
Consider a hard disk with:
4 surfaces
64 tracks/surface
128 sectors/track
256 bytes/sector
1. What is the capacity of the hard disk?
Disk capacity = surfaces * tracks/surface * sectors/track * bytes/sector
Disk capacity = 4 * 64 * 128 * 256
Disk capacity = 8 MB
2. The disk is rotating at 3600 RPM, what is the data transfer rate?
60 sec -> 3600 rotations
1 sec -> 60 rotations
Data transfer rate = number of rotations per second * track capacity * number of surfaces (since 1 R-W head is used for each surface)
Data transfer rate = 60 * 128 * 256 * 4
Data transfer rate = 7.5 MB/sec
3. The disk is rotating at 3600 RPM, what is the average access time?
Since, seek time, controller time and the amount of data to be transferred is not given, we consider all the three terms as 0.
Therefore, Average Access time = Average rotational delay
Rotational latency => 60 sec -> 3600 rotations
1 sec -> 60 rotations
Rotational latency = (1/60) sec = 16.67 msec.
Average Rotational latency = (16.67)/2
= 8.33 msec.
Average Access time = 8.33 msec.
4. Another example: GATE IT 2007 | Question 44

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : VaibhavRai3

Source
https://www.geeksforgeeks.org/hard-disk-drive-hdd-secondary-memory/
✍
Write a Testimonial

Random Access Memory (RAM) and Read Only Memory (ROM)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Random Access Memory (RAM) and Read Only Memory (ROM) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Memory is the most essential element of a computing system because without it computer can’t perform simple tasks. Computer memory is of two basic type – Primary memory(RAM and ROM) and Secondary
memory(hard drive,CD,etc.). Random Access Memory (RAM) is primary-volatile memory and Read Only Memory (ROM) is primary-non-volatile memory.

1. Random Access Memory (RAM) –

It is also called as read write memory or the main memory or the primary memory.
The programs and data that the CPU requires during execution of a program are stored in this memory.
It is a volatile memory as the data loses when the power is turned off.
RAM is further classified into two types- SRAM (Static Random Access Memory) and DRAM (Dynamic Random Access Memory).

2. Read Only Memory (ROM) –
Stores crucial information essential to operate the system, like the program essential to boot the computer.
It is not volatile.
Always retains its data.
Used in embedded systems or where the programming needs no change.
Used in calculators and peripheral devices.
ROM is further classified into 4 types- ROM, PROM, EPROM, and EEPROM.
Types of Read Only Memory (ROM) –
1. PROM (Programmable read-only memory) – It can be programmed by user. Once programmed, the data and instructions in it cannot be changed.
2. EPROM (Erasable Programmable read only memory) – It can be reprogrammed. To erase data from it, expose it to ultra violet light. To reprogram it, erase all the previous data.
3. EEPROM (Electrically erasable programmable read only memory) – The data can be erased by applying electric field, no need of ultra violet light. We can erase only portions of the chip.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : Roy19, sachinsinghbisht5

Source
https://www.geeksforgeeks.org/random-access-memory-ram-and-read-only-memory-rom/
✍
Write a Testimonial

Path Name in File Directory
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Path Name in File Directory - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – File Systems
Hierarchical Directory Systems –
Directory is maintained in the form of a tree. Each user can have as many directories as are needed so, that files can be grouped together in a natural way.

Advantages of this structure:
Searching is efficient
Groping capability of files increase
When the file system is organized as a directory tree, some way is needed for specifying file names.
Two different methods are commonly used:
1. Absolute Path name – In this method, each file is given an absolute path name consisting of the path from the root directory to the file. As an example, the path /usr/ast/mailbox means that the root directory
contains a subdirectory usr, which in turn contains a subdirectory ast, which contains the file mailbox.
Absolute path names always start at the root directory and are unique.
In UNIX the components of the path are separated by ‘/’. In Windows, the separator is ‘\’.
Windows \usr\ast\mailbox
UNIX /usr/ast/mailbox
2. Relative Path name – This is used in conjunction with the concept of the working directory (also called the current directory).A user can designate one directory as the current working directory, in which case all
path names not beginning at the root directory are taken relative to the working directory.
For example, if the current working directory is /usr/ast, then the file whose absolute path is /usr/ast/mailbox can be referenced simply as mailbox.
In other words, the UNIX
command : cp /usr/ast/mailbox /usr/ast/mailbox.bak
and the command : cp mailbox mailbox.bak
do exactly the same thing if the working directory is /usr/ast.
When to use which approach?
Some programs need to access a specific file without regard to what the working directory is. In that case, they should always use absolute path names. For example, a spelling checker might need to read /usr/lib/dictionary
to do its work. It should use the full, absolute path name in this case because it does not know what the working directory will be when it is called. The absolute path name will always work, no matter what the working
directory is.
Of course, if the spelling checker needs a large number of files from /usr/lib, an alternative approach is for it to issue a system call to change its working directory to /usr/lib, and then use just dictionary as the first
parameter to open. By explicitly changing the working directory, it knows for sure where it is in the directory tree, so it can then use relative paths.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : magbene

Source
https://www.geeksforgeeks.org/path-name-in-file-directory/
✍
Write a Testimonial

Different Types of RAM (Random Access Memory )
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Different Types of RAM (Random Access Memory ) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​

GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
RAM(Random Access Memory) is a part of computer’s Main Memory which is directly accessible by CPU. RAM is used to Read and Write data into it which is accessed by CPU randomly. RAM is volatile in nature, it
means if the power goes off, the stored information is lost. RAM is used to store the data that is currently processed by the CPU. Most of the programs and data that are modifiable are stored in RAM.
Integrated RAM chips are available in two form:
1. SRAM(Static RAM)
2. DRAM(Dynamic RAM)
The block diagram of RAM chip is given below.

SRAM
The SRAM memories consist of circuits capable of retaining the stored information as long as the power is applied. That means this type of memory requires constant power. SRAM memories are used to build Cache
Memory.
SRAM Memory Cell: Static memories(SRAM) are memories that consist of circuits capable of retaining their state as long as power is on. Thus this type of memories is called volatile memories. The below figure shows
a cell diagram of SRAM. A latch is formed by two inverters connected as shown in the figure. Two transistors T1 and T2 are used for connecting the latch with two bit lines. The purpose of these transistors is to act as
switches that can be opened or closed under the control of the word line, which is controlled by the address decoder. When the word line is at 0-level, the transistors are turned off and the latch remains its information. For
example, the cell is at state 1 if the logic value at point A is 1 and at point B is 0. This state is retained as long as the word line is not activated.

For Read operation, the word line is activated by the address input to the address decoder. The activated word line closes both the transistors (switches) T1 and T2. Then the bit values at points A and B can transmit to
their respective bit lines. The sense/write circuit at the end of the bit lines sends the output to the processor.
For Write operation, the address provided to the decoder activates the word line to close both the switches. Then the bit value that to be written into the cell is provided through the sense/write circuit and the signals in bit
lines are then stored in the cell.
DRAM
DRAM stores the binary information in the form of electric charges that applied to capacitors. The stored information on the capacitors tend to lose over a period of time and thus the capacitors must be periodically
recharged to retain their usage. The main memory is generally made up of DRAM chips.
DRAM Memory Cell: Though SRAM is very fast, but it is expensive because of its every cell requires several transistors. Relatively less expensive RAM is DRAM, due to the use of one transistor and one capacitor in
each cell, as shown in the below figure., where C is the capacitor and T is the transistor. Information is stored in a DRAM cell in the form of a charge on a capacitor and this charge needs to be periodically recharged.
For storing information in this cell, transistor T is turned on and an appropriate voltage is applied to the bit line. This causes a known amount of charge to be stored in the capacitor. After the transistor is turned off, due to
the property of the capacitor, it starts to discharge. Hence, the information stored in the cell can be read correctly only if it is read before the charge on the capacitors drops below some threshold value.

Types of DRAM
There are mainly 5 types of DRAM:
1. Asynchronous DRAM (ADRAM): The DRAM described above is the asynchronous type DRAM. The timing of the memory device is controlled asynchronously. A specialized memory controller circuit generates
the necessary control signals to control the timing. The CPU must take into account the delay in the response of the memory.
2. Synchronous DRAM (SDRAM): These RAM chips’ access speed is directly synchronized with the CPU’s clock. For this, the memory chips remain ready for operation when the CPU expects them to be ready.
These memories operate at the CPU-memory bus without imposing wait states. SDRAM is commercially available as modules incorporating multiple SDRAM chips and forming the required capacity for the
modules.
3. Double-Data-Rate SDRAM (DDR SDRAM): This faster version of SDRAM performs its operations on both edges of the clock signal; whereas a standard SDRAM performs its operations on the rising edge of the
clock signal. Since they transfer data on both edges of the clock, the data transfer rate is doubled. To access the data at high rate, the memory cells are organized into two groups. Each group is accessed separately.
4. Rambus DRAM (RDRAM): The RDRAM provides a very high data transfer rate over a narrow CPU-memory bus. It uses various speedup mechanisms, like synchronous memory interface, caching inside the

DRAM chips and very fast signal timing. The Rambus data bus width is 8 or 9 bits.
5. Cache DRAM (CDRAM): This memory is a special type DRAM memory with an on-chip cache memory (SRAM) that acts as a high-speed buffer for the main DRAM.
Difference between SRAM and DRAM
Below table lists some of the differences between SRAM and DRAM:

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/different-types-ram-random-access-memory/
✍
Write a Testimonial

Types of Operating Systems
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Types of Operating Systems - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
An Operating System performs all the basic tasks like managing file,process, and memory. Thus operating system acts as manager of all the resources, i.e. resource manager. Thus operating system becomes an interface
between user and machine.
Types of Operating Systems: Some of the widely used operating systems are as follows1. Batch Operating System –
This type of operating system does not interact with the computer directly. There is an operator which takes similar jobs having same requirement and group them into batches. It is the responsibility of operator to sort the
jobs with similar needs.

Advantages of Batch Operating System:
It is very difficult to guess or know the time required by any job to complete. Processors of the batch systems know how long the job would be when it is in queue
Multiple users can share the batch systems
The idle time for batch system is very less
It is easy to manage large work repeatedly in batch systems
Disadvantages of Batch Operating System:
The computer operators should be well known with batch systems
Batch systems are hard to debug
It is sometime costly
The other jobs will have to wait for an unknown time if any job fails
Examples of Batch based Operating System: Payroll System, Bank Statements etc.
2. Time-Sharing Operating Systems –
Each task is given some time to execute, so that all the tasks work smoothly. Each user gets time of CPU as they use single system. These systems are also known as Multitasking Systems. The task can be from single user
or from different users also. The time that each task gets to execute is called quantum. After this time interval is over OS switches over to next task.

Advantages of Time-Sharing OS:
Each task gets an equal opportunity
Less chances of duplication of software
CPU idle time can be reduced
Disadvantages of Time-Sharing OS:
Reliability problem
One must have to take care of security and integrity of user programs and data
Data communication problem
Examples of Time-Sharing OSs are: Multics, Unix etc.
3. Distributed Operating System –
These types of operating system is a recent advancement in the world of computer technology and are being widely accepted all-over the world and, that too, with a great pace. Various autonomous interconnected
computers communicate each other using a shared communication network. Independent systems possess their own memory unit and CPU. These are referred as loosely coupled systems or distributed systems. These
system’s processors differ in size and function. The major benefit of working with these types of operating system is that it is always possible that one user can access the files or software which are not actually present on
his system but on some other system connected within this network i.e., remote access is enabled within the devices connected in that network.

Advantages of Distributed Operating System:
Failure of one will not affect the other network communication, as all systems are independent from each other
Electronic mail increases the data exchange speed
Since resources are being shared, computation is highly fast and durable
Load on host computer reduces
These systems are easily scalable as many systems can be easily added to the network
Delay in data processing reduces
Disadvantages of Distributed Operating System:
Failure of the main network will stop the entire communication
To establish distributed systems the language which are used are not well defined yet
These types of systems are not readily available as they are very expensive. Not only that the underlying software is highly complex and not understood well yet
Examples of Distributed Operating System are- LOCUS etc.
4. Network Operating System –
These systems run on a server and provide the capability to manage data, users, groups, security, applications, and other networking functions. These type of operating systems allow shared access of files, printers,
security, applications, and other networking functions over a small private network. One more important aspect of Network Operating Systems is that all the users are well aware of the underlying configuration, of all
other users within the network, their individual connections etc. and that’s why these computers are popularly known as tightly coupled systems.

Advantages of Network Operating System:
Highly stable centralized servers
Security concerns are handled through servers
New technologies and hardware up-gradation are easily integrated to the system
Server access are possible remotely from different locations and types of systems
Disadvantages of Network Operating System:
Servers are costly
User has to depend on central location for most operations
Maintenance and updates are required regularly
Examples of Network Operating System are: Microsoft Windows Server 2003, Microsoft Windows Server 2008, UNIX, Linux, Mac OS X, Novell NetWare, and BSD etc.
5. Real-Time Operating System –
These types of OSs serves the real-time systems. The time interval required to process and respond to inputs is very small. This time interval is called response time.

Real-time systems are used when there are time requirements are very strict like missile systems, air traffic control systems, robots etc.
Two types of Real-Time Operating System which are as follows:
Hard Real-Time Systems:
These OSs are meant for the applications where time constraints are very strict and even the shortest possible delay is not acceptable. These systems are built for saving life like automatic parachutes or air bags
which are required to be readily available in case of any accident. Virtual memory is almost never found in these systems.
Soft Real-Time Systems:
These OSs are for applications where for time-constraint is less strict.

Advantages of RTOS:
Maximum Consumption: Maximum utilization of devices and system,thus more output from all the resources
Task Shifting: Time assigned for shifting tasks in these systems are very less. For example in older systems it takes about 10 micro seconds in shifting one task to another and in latest systems it takes 3 micro
seconds.
Focus on Application: Focus on running applications and less importance to applications which are in queue.
Real time operating system in embedded system: Since size of programs are small, RTOS can also be used in embedded systems like in transport and others.
Error Free: These types of systems are error free.
Memory Allocation: Memory allocation is best managed in these type of systems.
Disadvantages of RTOS:
Limited Tasks: Very few tasks run at the same time and their concentration is very less on few applications to avoid errors.
Use heavy system resources: Sometimes the system resources are not so good and they are expensive as well.
Complex Algorithms: The algorithms are very complex and difficult for the designer to write on.
Device driver and interrupt signals: It needs specific device drivers and interrupt signals to response earliest to interrupts.
Thread Priority: It is not good to set thread priority as these systems are very less prone to switching tasks.
Examples of Real-Time Operating Systems are: Scientific experiments, medical imaging systems, industrial control systems, weapon systems, robots, air traffic control systems, etc.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : nkg447, Da Man

Source
https://www.geeksforgeeks.org/types-of-operating-systems/
✍
Write a Testimonial

Microkernel in Operating Systems
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Microkernel in Operating Systems - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Kernel is the core part of an operating system which manages system resources. It also acts like a bridge between application and hardware of the computer. It is one of the first programs loaded on start-up (after the
Bootloader).

Kernel mode and User mode of CPU operation
(Thanks Sulbha Sharma for contributing this section)

The CPU can execute certain instruction only when it is in the kernel mode. These instruction are called privilege instruction. They allow implementation of special operation whose execution by the user program could
interface with the functioning of operating system or activity of another user program. For example, instruction for managing memory protection.
The operating system puts the CPU in kernel mode when it is executing in the kernel so, that kernel can execute some special operation.
The operating system puts the CPU in user mode when a user program is in execution so, that user program cannot interface with the operating system program.
User-level instruction does not require special privilege. Example are ADD,PUSH,etc.

The concept of modes can be extended beyond two, requiring more than a single mode bit CPUs that support virtualization use one of these extra bits to indicate when the virtual machine manager, VMM, is in control of
the system. The VMM has more privileges than ordinary user programs, but not so many as the full kernel.
System calls are typically implemented in the form of software interrupts, which causes the hardware’s interrupt handler to transfer control over to an appropriate interrupt handler, which is part of the operating system,
switching the mode bit to kernel mode in the process. The interrupt handler checks exactly which interrupt was generated, checks additional parameters ( generally passed through registers ) if appropriate, and then calls
the appropriate kernel service routine to handle the service requested by the system call.
User programs’ attempts to execute illegal instructions ( privileged or non-existent instructions ), or to access forbidden memory areas, also generate software interrupts, which are trapped by the interrupt handler and
control is transferred to the OS, which issues an appropriate error message, possibly dumps data to a log ( core ) file for later analysis, and then terminates the offending program.
What is Microkernel?
Microkernel is one of the classification of the kernel. Being a kernel it manages all system resources. But in a microkernel, the user services and kernel services are implemented in different address space. The user
services are kept in user address space, and kernel services are kept under kernel address space, thus also reduces the size of kernel and size of operating system as well.

It provides minimal services of process and memory management. The communication between client program/application and services running in user address space is established through message passing, reducing the
speed of execution microkernel. The Operating System remains unaffected as user services and kernel services are isolated so if any user service fails it does not affect kernel service. Thus it adds to one of the advantages
in a microkernel. It is easily extendable i.e. if any new services are to be added they are added to user address space and hence requires no modification in kernel space. It is also portable, secure and reliable.
Microkernel Architecture –
Since kernel is the core part of the operating system, so it is meant for handling the most important services only. Thus in this architecture only the most important services are inside kernel and rest of the OS services are
present inside system application program. Thus users are able to interact with those not-so important services within the system application. And the microkernel is solely responsible for the most important services of
operating system they are named as follows:
Inter process-Communication
Memory Management
CPU-Scheduling
Advantages of Microkernel –
The architecture of this kernel is small and isolated hence it can function better.
Expansion of the system is easier, it is simply added in the system application without disturbing the kernel.
Eclipse IDE is a good example of Microkernel Architecture.
Read next – Monolithic Kernel and key differences from Microkernel

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/microkernel-in-operating-systems/
✍
Write a Testimonial

Monolithic Kernel and key differences from Microkernel
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Monolithic Kernel and key differences from Microkernel - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Apart from microkernel, Monolithic Kernel is another classification of Kernel. Like microkernel this one also manages system resources between application and hardware, but user services and kernel services are
implemented under same address space. It increases the size of the kernel, thus increases size of operating system as well.
This kernel provides CPU scheduling, memory management, file management and other operating system functions through system calls. As both services are implemented under same address space, this makes operating
system execution faster.
Below is the diagrammatic representation of Monolithic Kernel:

If any service fails the entire system crashes, and it is one of the drawbacks of this kernel. The entire operating system needs modification if user adds a new service.
Advantages of Monolithic Kernel –
One of the major advantage of having monolithic kernel is that it provides CPU scheduling, memory management, file management and other operating system functions through system calls.
The other one is that it is a single large process running entirely in a single address space.
It is a single static binary file. Example of some Monolithic Kernel based OSs are: Unix, Linux, Open VMS, XTS-400, z/TPF.
Disadvantages of Monolithic Kernel –
One of the major disadvantage of monolithic kernel is that, if anyone service fails it leads to entire system failure.
If user has to add any new service. User needs to modify entire operating system.
Key differences between Monolithic Kernel and Microkernel –

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/monolithic-kernel-and-key-differences-from-microkernel/
✍
Write a Testimonial

Swap Space in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Swap Space in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
A computer has sufficient amount of physical memory but most of times we need more so we swap some memory on disk. Swap space is a space on hard disk which is a substitute of physical memory. It is used as virtual
memory which contains process memory image. Whenever our computer run short of physical memory it uses it’s virtual memory and stores information in memory on disk. Swap space helps the computer’s operating
system in pretending that it have more RAM than it actually has. It is also called as swap file.This interchange of data between virtual memory and real memory is called as swapping and space on disk as “swap space”.
Virtual memory is a combination of RAM and disk space that running processes can use. Swap space is the portion of virtual memory that is on the hard disk, used when RAM is full.
Swap space can be useful to computer in various ways:

It can be used as a single contiguous memory which reduces i/o operations to read or write a file.
Applications which are not used or are used less can be kept in swap file.
Having sufficient swap file helps the system keep some physical memory free all the time.

The space in physical memory which has been freed due to swap space can be used by OS for some other important tasks.
In operating systems such as Windows, Linux, etc systems provide a certain amount of swap space by default which can be changed by users according to their needs. If you don’t want to use virtual memory you can easily
disable it all together but in case if you run out of memory then kernel will kill some of the processes in order to create a sufficient amount of space in physical memory. So it totally depends upon user whether he wants to
use swap space or not.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/swap-space-in-operating-system/
✍
Write a Testimonial

Measure the time spent in context switch?
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Measure the time spent in context switch? - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
A Context switch is the time spent between two processes (i.e., bringing a waiting process into execution and sending an executing process into waiting state). This happens in multitasking.The operating system must bring
the state information if waiting process into memory and save the state information of the currently running process.
In order to solve this problem, we would like to record the timestamps of the first and last instruction of the swapping processes.The context switch time is the difference between the two processes.
Let’s take an example: Assume there are only two processes, P1 and P2.
P1 is executing and P2 is waiting for execution. At some point, the operating system must swap P1 and P2, let’s assume it happens at the nth instruction of P1. If t(x, k) indicates the timestamp in microseconds of the kth
instruction of process x, then the context switch would take t(2, 1) – t(1, n).

Another issue is that swapping is governed by the scheduling algorithm of the operating system and there may be many kernel level threads which are also doing context switches. Other processes could be contending for
the CPU or the kernel handling interrupts. The user does not have any control over these extraneous context switches. For instance, if at time t(1, n) the kernel decides to handle an interrupt, then the context switch time
would be overstated.
In order to avoid these obstacles, we must construct an environment such that after P1 executes, the task scheduler immediately selects P2 to run. This may be accomplished by constructing a data channel, such as a pipe
between P1 and P2.
That is, let’s allow P1 to be the initial sender and P2 be the receiver. Initially, P2 is blocked(sleeping) as it awaits the data token. When P1 executes, it delivers the data token over the data channel to P2 and immediately
attempts to read the response token. A context switch results and the task scheduler must selects another process to run.Since P2 is now in a ready-to-run state, it is a desirable candidate to be selected by the task scheduler
for execution.When P2 runs, the role of P1 and P2 are swapped. P2 is now acting as the sender and P1 as the blocked receiver.
To summaries –
1.
2.
3.
4.
5.
6.
7.
8.
9.

P2 blocks awaiting data from P1
P1 marks the starting time.
P1 sends token to P2.
P1 attempts to read a response token from P2. This induces a context switch.
P2 is scheduled and receives the token.
P2 sends a response token to P1.
P2 attempts read a response token from P1. This induces a context switch.
P1 is scheduled and receives the token.
P1 marks the end time.

The key is that the delivery of a data token induces a context switch. Let Td and Tr be the time it takes to deliver and receive a data token, respectively, and let Tc be the amount of time spent in a context switch. At step 2,
P1 records the timestamp of the delivery of the token, and at step 9, it records the timestamp of the response. The amount of time elapsed, T, between these events may be expressed by:
​
T = 2 * (Td + Tc + Tr)​

This formula arises because of the following events:
P1 sends the token (3)
CPU context switches (4)
P2 receives it (5)
P2 then sends the response token (6)
CPU context switches (7)
and finally, P1 receives it (8)
GATE CS Practice Questions –
GATE-CS-2006 | Question 85
GATE CS 1998 | Question 52

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/measure-time-spent-context-switch/
✍
Write a Testimonial

Dual Mode operations in OS
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Dual Mode operations in OS - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
An error in one program can adversely affect many processes, it might modify data of another program, or also can affect the operating system. For example, if a process stuck in infinite loop then this infinite loop could
affect correct operation of other processes. So to ensure the proper execution of the operating system, there are two modes of operation:
User mode –
When the computer system run user applications like creating a text document or using any application program, then the system is in the user mode. When the user application requests for a service from the operating
system or an interrupt occurs or system call, then there will be a transition from user to kernel mode to fulfill the requests.
Note: To switch from kernel mode to user mode, mode bit should be 1.
Given below image describes what happen when an interrupt occurs:

Kernel Mode –
When the system boots, hardware starts in kernel mode and when operating system is loaded, it start user application in user mode. To provide protection to the hardware, we have privileged instructions which execute
only in kernel mode. If user attempt to run privileged instruction in user mode then it will treat instruction as illegal and traps to OS. Some of the privileged instructions are:
1. Handling Interrupts
2. To switch from user mode to kernel mode.
3. Input-Output management.
Note: To switch from user mode to kernel mode mode bit should be 0.
Read next – User Level thread Vs Kernel Level thread

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : midhunxda

Source
https://www.geeksforgeeks.org/dual-mode-operations-os/
✍
Write a Testimonial

Allocating kernel memory (buddy system and slab system)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Allocating kernel memory (buddy system and slab system) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Buddy System
Two strategies for managing free memory that is assigned to kernel processes:
Buddy allocation system is an algorithm in which a larger memory block is divided into small parts to satisfy the request. This algorithm is used to give best fit. The two smaller parts of block are of equal size and called as
buddies. In the same manner one of the two buddies will further divide into smaller parts until the request is fulfilled. Benefit of this technique is that the two buddies can combine to form the block of larger size according
to the memory request.
Example – If the request of 25Kb is made then block of size 32Kb is allocated.

Four Types of Buddy System –
1.
2.
3.
4.

Binary buddy system
Fibonacci buddy system
Weighted buddy system
Tertiary buddy system

Why buddy system?
If the partition size and procees size are different then poor match occurs and may use space inefficiently.
It is easy to implement and efficient then dynamic allocation.
Binary buddy system –
The buddy system maintains a list of the free blocks of each size (called a free list), so that it is easy to find ablock of the desired size, if one is available. If no block of the requested size is available, Allocate searches for
the first nonempty list for blocks of atleast the size requested. In either case, a block is removed from the free list.
Example – Assume the size of memory segment is initially 256kb and the kernel rquests 25kb of memory. The segment is initially divided into two buddies. Let we call A1 and A2 each 128kb in size. One of these
buddies is further divided into two 64kb buddies let say B1 and B2. But the next highest power of 25kb is 32kb so, either B1 or B2 is further divided into two 32kb buddies(C1 and C2) and finally one of these buddies is
used to satisfy the 25kb request. A split block can only be merged with its unique buddy block, which then reforms the larger block they were split from.
Fibonacci buddy system –
This is the system in which blocks are divided into sizes which are fibonacci numbers. It satisfy the following relation:
Zi = Z(i-1)+Z (i-2)

0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 144, 233, 377, 610. The address calculation for the binary and weighted buddy systems is straight forward, but the original procedure for the Fibonacci buddy system was either limited to a
small, fixed number of block sizes or a time consuming computation.
Advantages –
In comparison to other simpler techniques such as dynamic allocation, the buddy memory system has little external fragmentation.
The buddy memory allocation system is implemented with the use of a binary tree to represent used or unused split memory blocks.
The buddy system is very fast to allocate or deallocate memory.
In buddy systems, the cost to allocate and free a block of memory is low compared to that of best-fit or first-fit algorithms.
Other advantage is coalescing.
Address calculation is easy.
What is coalescing?
It is defined as how quickly adjacent buddies can be combined to form larger segments this is known as coalescing.
For example, when the kernel releases the C1 unit it was allocated, the system can coalesce C1 and C2 into a 64kb segment. This segment B1 can in turn be coalesced with its buddy B2 to form a 128kb segment.
Ultimately we can end up with the original 256kb segment.
Drawback –
The main drawback in buddy system is internal fragmentation as larger block of memory is acquired then required. For example if a 36 kb request is made then it can only be satisfied by 64 kb segment and reamining
memory is wasted.

A second strategy for allocating kernel memory is known as slab allocation. It eliminates fragmentation caused by allocations and deallocations. This method is used to retain allocated memory that contains a data object
of a certain type for reuse upon subsequent allocations of objects of the same type. In slab allocation memory chunks suitable to fit data objects of certain type or size are preallocated. Cache does not free the space
immediately after use although it keeps track of data which are required frequently so that whenever request is made the data will reach very fast. Two terms required are:
Slab – A slab is made up of one or more physically contiguous pages. The slab is the actual container of data associated with objects of the specific kind of the containing cache.
Cache – Cache represents a small amount of very fast memory. A cache consists of one or more slabs. There is a single cache for each unique kernel data structure.

Example –
A separate cache for a data structure representing processes descriptors
Separate cache for file objects
Separate cache for semaphores etc.
Each cache is populated with objects that are instantiations of the kernel data structure the cache represents. For example the cache representing semaphores stores instances of semaphore objects, the cache representing
process descriptors stores instances of process descriptor objects.
Implementation –
The slab allocation algorithm uses caches to store kernel objects. When a cache is created a number of objects which are initially marked as free are allocated to the cache. The number of objects in the cache depends on
size of the associated slab.
Example – A 12 kb slab (made up of three contiguous 4 kb pages) could store six 2 kb objects. Initially all objects in the cache are marked as free. When a new object for a kernel data structure is needed, the allocator can
assign any free object from the cache to satisfy the request. The object assigned from the cache is marked as used.
In linux, a slab may in one of three possible states:
1. Full – All objects in the slab are marked as used
2. Empty – All objects in the slab are marked as free
3. Partial – The slab consists of both
The slab allocator first attempts to satisfy the request with a free object in a partial slab. If none exists, a free object is assigned from an empty slab. If no empty slabs are available, a new slab is allocated from contiguous
physical pages and assigned to a cache.
Benefits of slab allocator –
No memory is wasted due to fragmentation because each unique kernel data structure has an associated cache.
Memory request can be satisfied quickly.
The slab allocating scheme is particularly effective for managing when objects are frequently allocated or deallocated. The act of allocating and releasing memory can be a time consuming process. However, objects
are created in advance and thus can be quickly allocated from the cache. When the kernel has finished with an object and releases it, it is marked as free and return to its cache, thus making it immediately available
for subsequent request from the kernel.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/operating-system-allocating-kernel-memory-buddy-system-slab-system/
✍
Write a Testimonial

Selfish Round Robin CPU Scheduling
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Selfish Round Robin CPU Scheduling - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Program for Round Robin scheduling
In the traditional Round Robin scheduling algorithm all processes were treated equally for processing. The objective of the Selfish Round Robin is to give better service to processes that have been executing for a while
than to newcomers. Its a more logical and superior implementation compared to the normal Round Robin algorithm.

Implimentation :Processes in the ready list are partitioned into two lists: NEW and ACCEPTED.
The New processes wait while Accepted processes are serviced by the Round Robin.
Priority of a new process increases at rate ‘a’ while the priority of an accepted process increases at rate ‘b’.
When the priority of a new process reaches the priority of an accepted process, that new process becomes accepted.
If all accepted processes finish, the highest priority new process is accepted.
Let’s trace out the general working of this algorithm :-

STEP 1 : Assume that initially there are no ready processes, when the first one, A, arrives. It has priority 0 to begin with. Since there are no other accepted processes, A is accepted immediately.
STEP 2 : After a while another process, B, arrives. As long as b / a < 1, B’s priority will eventually catch up to A’s, so it is accepted; now both A and B have the same priority.
STEP 3 : All accepted processes share a common priority (which rises at rate b ); that makes this policy easy to implement i.e any new process’s priority is bound to get accepted at some point. So no process has to
experience starvation.
STEP 4 : Even if b / a > 1, A will eventually finish, and then B can be accepted.
Adjusting the parameters a and b : ​
-> If b / a >= 1, a new process is not accepted ​
until all the accepted processes have finished, so SRR becomes FCFS. ​
-> If b / a = 0, all processes are accepted immediately, so SRR becomes RR. ​
-> If 0 < b / a < 1, accepted processes are selfish, but not completely.​

Example on Selfish Round Robin –

Solution (where a = 2 and b = 1) –

Explanation –
Process A gets accepted as soon as it comes at time t = 0. So its priority is increased only by ‘b’ i.e ‘1’ after each second. B enters at time t = 1 and goes to the waiting queue. So its priority gets increased by ‘a’ i.e. ‘2’ at
time t = 2. At this point priority of A = priority of B = 2.
So now both process A & B are in the accepted queue and are executed in a round robin fashion. At time t = 3 process C enters the waiting queue. At time t = 6 the priority of process C catches up to the priority of process
B and then they start executing in a Round Robin manner. When B finishes execution at time t = 10, D is automatically promoted to the accepted queue.
Similarly when D finishes execution at time t = 15, E is automatically promoted to the accepted queue.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/selfish-round-robin-cpu-scheduling/
✍
Write a Testimonial

Introduction of Operating System – Set 1
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Introduction of Operating System - Set 1 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
An operating system acts as an intermediary between the user of a computer and computer hardware. The purpose of an operating system is to provide an environment in which a user can execute programs in a convenient
and efficient manner.
An operating system is a software that manages the computer hardware. The hardware must provide appropriate mechanisms to ensure the correct operation of the computer system and to prevent user programs from
interfering with the proper operation of the system.
Operating System – Definition:

An operating system is a program that controls the execution of application programs and acts as an interface between the user of a computer and the computer hardware.
A more common definition is that the operating system is the one program running at all times on the computer (usually called the kernel), with all else being application programs.
An operating system is concerned with the allocation of resources and services, such as memory, processors, devices, and information. The operating system correspondingly includes programs to manage these
resources, such as a traffic controller, a scheduler, memory management module, I/O programs, and a file system.
Functions of Operating system – Operating system performs three functions:
1. Convenience: An OS makes a computer more convenient to use.
2. Efficiency: An OS allows the computer system resources to be used in an efficient manner.
3. Ability to Evolve: An OS should be constructed in such a way as to permit the effective development, testing and introduction of new system functions at the same time without interfering with service.
Operating system as User Interface –
1.
2.
3.
4.

User
System and application programs
Operating system
Hardware

Every general-purpose computer consists of the hardware, operating system, system programs, and application programs. The hardware consists of memory, CPU, ALU, and I/O devices, peripheral device, and storage
device. System program consists of compilers, loaders, editors, OS, etc. The application program consists of business programs, database programs.

Fig1: Conceptual view of a computer system
Every computer must have an operating system to run other programs. The operating system coordinates the use of the hardware among the various system programs and application programs for various users. It simply
provides an environment within which other programs can do useful work.
The operating system is a set of special programs that run on a computer system that allows it to work properly. It performs basic tasks such as recognizing input from the keyboard, keeping track of files and directories on
the disk, sending output to the display screen and controlling peripheral devices.
OS is designed to serve two basic purposes:
1. It controls the allocation and use of the computing System’s resources among the various user and tasks.
2. It provides an interface between the computer hardware and the programmer that simplifies and makes feasible for coding, creation, debugging of application programs.
The Operating system must support the following tasks. The task are:
1.
2.
3.
4.

Provides the facilities to create, modification of programs and data files using an editor.
Access to the compiler for translating the user program from high level language to machine language.
Provide a loader program to move the compiled program code to the computer’s memory for execution.
Provide routines that handle the details of I/O programming.

I/O System Management –
The module that keeps track of the status of devices is called the I/O traffic controller. Each I/O device has a device handler that resides in a separate process associated with that device.
The I/O subsystem consists of
A memory Management component that includes buffering caching and spooling.
A general device driver interface.
Drivers for specific hardware devices.
Assembler –
The input to an assembler is an assembly language program. The output is an object program plus information that enables the loader to prepare the object program for execution. At one time, the computer programmer
had at his disposal a basic machine that interpreted, through hardware, certain fundamental instructions. He would program this computer by writing a series of ones and Zeros (Machine language), place them into the
memory of the machine.

Compiler –
The High-level languages- examples are FORTRAN, COBOL, ALGOL and PL/I are processed by compilers and interpreters. A compiler is a program that accepts a source program in a “high-level language “and
produces a corresponding object program. An interpreter is a program that appears to execute a source program as if it was machine language. The same name (FORTRAN, COBOL, etc.) is often used to designate both a
compiler and its associated language.
Loader –
A Loader is a routine that loads an object program and prepares it for execution. There are various loading schemes: absolute, relocating and direct-linking. In general, the loader must load, relocate and link the object
program. The loader is a program that places programs into memory and prepares them for execution. In a simple loading scheme, the assembler outputs the machine language translation of a program on a secondary
device and a loader places it in the core. The loader places into memory the machine language version of the user’s program and transfers control to it. Since the loader program is much smaller than the assembler, those
make more core available to the user’s program.
History of Operating system –
Operating system has been evolving through the years. Following Table shows the history of OS.
Generation Year Electronic device used Types of OS Device
First
1945-55 Vaccum Tubes
Plug Boards
Second
1955-65 Transistors
Batch Systems
Third
1965-80 Integrated Circuits(IC) Multiprogramming
Fourth
Since 1980 Large Scale Integration PC
Types of Operating System –
Batch Operating System- Sequence of jobs in a program on a computer without manual interventions.
Time sharing operating System- allows many users to share the computer resources.(Max utilization of the resources).
Distributed operating System- Manages a group of different computers and make appear to be a single computer.
Network operating system- computers running in different operating system can participate in common network (It is used for security purpose).
Real time operating system – meant applications to fix the deadlines.
Examples of Operating System are –
Windows (GUI based, PC)
GNU/Linux (Personal, Workstations, ISP, File and print server, Three-tier client/Server)
macOS (Macintosh), used for Apple’s personal computers and work stations (MacBook, iMac).
Android (Google’s Operating System for smartphones/tablets/smartwatches)
iOS (Apple’s OS for iPhone, iPad and iPod Touch)
References –
Operating System Concepts – Book
Introduction to Operating System – NPTEL

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : hemanthhemanth, rajatgupta1998, AdityaRakhecha, ayushgangwar, WilliamSpichiger, more
triveni0221

Source
https://www.geeksforgeeks.org/introduction-of-operating-system-set-1/
✍
Write a Testimonial

Process-based and Thread-based Multitasking
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Process-based and Thread-based Multitasking - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Thread, Difference between multitasking, multithreading and multiprocessing
A multitasking operating system is an operating system that gives you the perception of 2 or more tasks/jobs/processes running at the same time. It does this by dividing system resources amongst these
tasks/jobs/processes and switching between the tasks/jobs/processes while they are executing over and over again. Usually CPU processes only one task at a time but the switching is so fast that it looks like CPU is
executing multiple processes at a time. They can support either preemptive multitasking, where the OS doles out time to applications (virtually all modern OSes) or cooperative multitasking, where the OS waits for the
program to give back control (Windows 3.x, Mac OS 9 and earlier), leading to hangs and crashes. Also known as Timesharing, multitasking is a logical extension of multiprogramming.
Multitasking programming is of two types –
1. Process-based Multitasking
2. Thread-based Multitasking.
Process Based Multitasking Programming –

In process based multitasking two or more processes and programs can be run concurrently.
In process based multitasking a process or a program is the smallest unit.
Program is a bigger unit.
Process based multitasking requires more overhead.
Process requires its own address space.
Process to Process communication is expensive.
Here, it is unable to gain access over idle time of CPU.
It is comparatively heavy weight.
It has slower data rate multi-tasking.
Example – We can listen to music and browse internet at the same time. The processes in this example are the music player and browser.
Thread Based Multitasking Programming –
In thread based multitasking two or more threads can be run concurrently.
In thread based multitasking a thread is the smallest unit.
Thread is a smaller unit.
Thread based multitasking requires less overhead.
Threads share same address space.
Thread to Thread communication is not expensive.
It allows taking gain access over idle time taken by CPU.
It is comparatively light weight.
It has faster data rate multi-tasking.
Examples – Using a browser we can navigate through the webpage and at the same time download a file. In this example, navigation is one thread and downloading is another thread. Also in a word-processing application
like MS Word, we can type text in one thread and spell checker checks for mistakes in another thread.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/process-based-and-thread-based-multitasking/
✍
Write a Testimonial

Difference between Multiprogramming, multitasking, multithreading and multiprocessing
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Multiprogramming, multitasking, multithreading and multiprocessing - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
1.
2.
3.
4.

Multiprogramming – A computer running more than one program at a time (like running Excel and Firefox simultaneously).
Multiprocessing – A computer using more than one CPU at a time.
Multitasking – Tasks sharing a common resource (like 1 CPU).
Multithreading is an extension of multitasking.

In a modern computing system, there are usually several concurrent application processes which want to execute. Now it is the responsibility of the Operating System to manage all the processes effectively and efficiently.
One of the most important aspects of an Operating System is to multi program.
In a computer system, there are multiple processes waiting to be executed, i.e. they are waiting when the CPU will be allocated to them and they begin their execution. These processes are also known as jobs. Now the
main memory is too small to accommodate all of these processes or jobs into it. Thus, these processes are initially kept in an area called job pool. This job pool consists of all those processes awaiting allocation of main
memory and CPU.
CPU selects one job out of all these waiting jobs, brings it from the job pool to main memory and starts executing it. The processor executes one job until it is interrupted by some external factor or it goes for an I/O task.
Non-multi programmed system’s working –
In a non multi programmed system, As soon as one job leaves the CPU and goes for some other task (say I/O ), the CPU becomes idle. The CPU keeps waiting and waiting until this job (which was executing earlier)
comes back and resumes its execution with the CPU. So CPU remains free for all this while.
Now it has a drawback that the CPU remains idle for a very long period of time. Also, other jobs which are waiting to be executed might not get a chance to execute because the CPU is still allocated to the earlier
job.
This poses a very serious problem that even though other jobs are ready to execute, CPU is not allocated to them as the CPU is allocated to a job which is not even utilizing it (as it is busy in I/O tasks).
It cannot happen that one job is using the CPU for say 1 hour while the others have been waiting in the queue for 5 hours. To avoid situations like this and come up with efficient utilization of CPU, the concept of
multi programming came up.
The main idea of multi programming is to maximize the CPU time.
Multi programmed system’s working –

In a multi-programmed system, as soon as one job goes for an I/O task, the Operating System interrupts that job, chooses another job from the job pool (waiting queue), gives CPU to this new job and starts its
execution. The previous job keeps doing its I/O operation while this new job does CPU bound tasks. Now say the second job also goes for an I/O task, the CPU chooses a third job and starts executing it. As soon as a
job completes its I/O operation and comes back for CPU tasks, the CPU is allocated to it.
In this way, no CPU time is wasted by the system waiting for the I/O task to be completed.
Therefore, the ultimate goal of multi programming is to keep the CPU busy as long as there are processes ready to execute. This way, multiple programs can be executed on a single processor by executing a part of a
program at one time, a part of another program after this, then a part of another program and so on, hence executing multiple programs. Hence, the CPU never remains idle.
In the image below, program A runs for some time and then goes to waiting state. In the mean time program B begins its execution. So the CPU does not waste its resources and gives program B an opportunity to run.

In a uni-processor system, only one process executes at a time.
Multiprocessing is the use of two or more CPUs (processors) within a single Computer system. The term also refers to the ability of a system to support more than one processor within a single computer system. Now since
there are multiple processors available, multiple processes can be executed at a time. These multi processors share the computer bus, sometimes the clock, memory and peripheral devices also.
Multi processing system’s working –
With the help of multiprocessing, many processes can be executed simultaneously. Say processes P1, P2, P3 and P4 are waiting for execution. Now in a single processor system, firstly one process will execute, then
the other, then the other and so on.
But with multiprocessing, each process can be assigned to a different processor for its execution. If its a dual-core processor (2 processors), two processes can be executed simultaneously and thus will be two times
faster, similarly a quad core processor will be four times as fast as a single processor.
Why use multi processing –
The main advantage of multiprocessor system is to get more work done in a shorter period of time. These types of systems are used when very high speed is required to process a large volume of data. Multi
processing systems can save money in comparison to single processor systems because the processors can share peripherals and power supplies.
It also provides increased reliability in the sense that if one processor fails, the work does not halt, it only slows down. e.g. if we have 10 processors and 1 fails, then the work does not halt, rather the remaining 9
processors can share the work of the 10th processor. Thus the whole system runs only 10 percent slower, rather than failing altogether.

Multiprocessing refers to the hardware (i.e., the CPU units) rather than the software (i.e., running processes). If the underlying hardware provides more than one processor then that is multiprocessing. It is the ability of the
system to leverage multiple processors’ computing power.
Difference between Multi programming and Multi processing –
A System can be both multi programmed by having multiple programs running at the same time and multiprocessing by having more than one physical processor. The difference between multiprocessing and multi
programming is that Multiprocessing is basically executing multiple processes at the same time on multiple processors, whereas multi programming is keeping several programs in main memory and executing them
concurrently using a single CPU only.
Multiprocessing occurs by means of parallel processing whereas Multi programming occurs by switching from one process to other (phenomenon called as context switching).
As the name itself suggests, multi tasking refers to execution of multiple tasks (say processes, programs, threads etc.) at a time. In the modern operating systems, we are able to play MP3 music, edit documents in
Microsoft Word, surf the Google Chrome all simultaneously, this is accomplished by means of multi tasking.
Multitasking is a logical extension of multi programming. The major way in which multitasking differs from multi programming is that multi programming works solely on the concept of context switching whereas
multitasking is based on time sharing alongside the concept of context switching.

Multi tasking system’s working –
In a time sharing system, each process is assigned some specific quantum of time for which a process is meant to execute. Say there are 4 processes P1, P2, P3, P4 ready to execute. So each of them are assigned
some time quantum for which they will execute e.g time quantum of 5 nanoseconds (5 ns). As one process begins execution (say P2), it executes for that quantum of time (5 ns). After 5 ns the CPU starts the
execution of the other process (say P3) for the specified quantum of time.
Thus the CPU makes the processes to share time slices between them and execute accordingly. As soon as time quantum of one process expires, another process begins its execution.
Here also basically a context switch is occurring but it is occurring so fast that the user is able to interact with each program separately while it is running. This way, the user is given the illusion that multiple
processes/ tasks are executing simultaneously. But actually only one process/ task is executing at a particular instant of time. In multitasking, time sharing is best manifested because each running process takes only a
fair quantum of the CPU time.
In a more general sense, multitasking refers to having multiple programs, processes, tasks, threads running at the same time. This term is used in modern operating systems when multiple tasks share a common processing
resource (e.g., CPU and Memory).

As depicted in the above image, At any time the CPU is executing only one task while other tasks are waiting for their turn. The illusion of parallelism is achieved when the CPU is reassigned to another task. i.e all
the three tasks A, B and C are appearing to occur simultaneously because of time sharing.
So for multitasking to take place, firstly there should be multiprogramming i.e. presence of multiple programs ready for execution. And secondly the concept of time sharing.
A thread is a basic unit of CPU utilization. Multi threading is an execution model that allows a single process to have multiple code segments (i.e., threads) running concurrently within the “context” of that process.
e.g. VLC media player, where one thread is used for opening the VLC media player, one thread for playing a particular song and another thread for adding new songs to the playlist.
Multi threading is the ability of a process to manage its use by more than one user at a time and to manage multiple requests by the same user without having to have multiple copies of the program.
Multi threading system’s working –
Example 1 –
Say there is a web server which processes client requests. Now if it executes as a single threaded process, then it will not be able to process multiple requests at a time. Firstly one client will make its request and
finish its execution and only then, the server will be able to process another client request. This is really costly, time consuming and tiring task. To avoid this, multi threading can be made use of.
Now, whenever a new client request comes in, the web server simply creates a new thread for processing this request and resumes its execution to hear more client requests. So the web server has the task of listening
to new client requests and creating threads for each individual request. Each newly created thread processes one client request, thus reducing the burden on web server.

Example 2 –
We can think of threads as child processes that share the parent process resources but execute independently. Now take the case of a GUI. Say we are performing a calculation on the GUI (which is taking very long
time to finish). Now we can not interact with the rest of the GUI until this command finishes its execution. To be able to interact with the rest of the GUI, this command of calculation should be assigned to a separate
thread. So at this point of time, 2 threads will be executing i.e. one for calculation, and one for the rest of the GUI. Hence here in a single process, we used multiple threads for multiple functionality.
The image below completely describes the VLC player example:

Advantages of Multi threading –
Benefits of Multi threading include increased responsiveness. Since there are multiple threads in a program, so if one thread is taking too long to execute or if it gets blocked, the rest of the threads keep executing
without any problem. Thus the whole program remains responsive to the user by means of remaining threads.
Another advantage of multi threading is that it is less costly. Creating brand new processes and allocating resources is a time consuming task, but since threads share resources of the parent process, creating threads
and switching between them is comparatively easy. Hence multi threading is the need of modern Operating Systems.

Improved By : Darshan L., harshitSingh_11

Source
https://www.geeksforgeeks.org/difference-between-multitasking-multithreading-and-multiprocessing/
✍
Write a Testimonial

Reader-Writers solution using Monitors
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Reader-Writers solution using Monitors - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Process Synchronization, Monitors, Readers-Writers Problem
Considering a shared Database our objectives are:
Readers can access database only when there are no writers.
Writers can access database only when there are no readers or writers.
Only one thread can manipulate the state variables at a time.
Basic structure of a solution –
​
Reader()​
Wait until no writers​
Access database​
Check out – wake up a waiting writer​
Writer()​
Wait until no active readers or writers​
Access database​
Check out – wake up waiting readers or writer​

–Now let’s suppose that a writer is active and a mixture of readers and writers now show up.
Who should get in next?
–Or suppose that a writer is waiting and an endless of stream of readers keep showing up.

Would it be fair for them to become active?
So we’ll implement a kind of back-and-forth form of fairness:

Once a reader is waiting, readers will get in next.
If a writer is waiting, one writer will get in next.
Implementation of the solution using monitors:1.
2.
3.
4.
5.

The methods should be executed with mutual exclusion i.e. At each point in time, at most one thread may be executing any of its methods.
Monitors also provide a mechanism for threads to temporarily give up exclusive access, in order to wait for some condition to be met, before regaining exclusive access and resuming their task.
Monitors also have a mechanism for signaling other threads that such conditions have been met.
So in this implementation only mutual exclusion is not enough. Threads attempting an operation may need to wait until some assertion P holds true.
While a thread is waiting upon a condition variable, that thread is not considered to occupy the monitor, and so other threads may enter the monitor to change the monitor’s state.

Code –
filter_none
edit
close
play_arrow
link
brightness_4
code
// STATE VARIABLES
// Number of active readers; initially = 0
int NReaders = 0;
// Number of waiting readers; initially = 0
int WaitingReaders = 0;
// Number of active writers; initially = 0
int NWriters = 0;
// Number of waiting writers; initially = 0
int WaitingWriters = 0;
Condition canRead = NULL;
Condition canWrite = NULL;
Void BeginWrite()
{
// A writer can enter if there are no other
// active writers and no readers are waiting
if (NWriters == 1 || NReaders > 0) {
++WaitingWriters;
wait(CanWrite);
--WaitingWriters;
}
NWriters = 1;
}
Void EndWrite()
{
NWriters = 0;
// Checks to see if any readers are waiting
if (WaitingReaders)
Signal(CanRead);
else
Signal(CanWrite);
}
Void BeginRead()
{
//
//
//
if

A reader can enter if there are no writers
active or waiting, so we can have
many readers active all at once
(NWriters == 1 || WaitingWriters > 0) {
++WaitingReaders;
// Otherwise, a reader waits (maybe many do)
Wait(CanRead);
--WaitingReaders;

}
++NReaders;
Signal(CanRead);
}
Void EndRead()
{
// When a reader finishes, if it was the last reader,
// it lets a writer in (if any is there).
if (--NReaders == 0)
Signal(CanWrite);
}

chevron_right
filter_none
Understanding the solution:It wants to be fair.
1. If a writer is waiting, readers queue up.
2. If a reader (or another writer) is active or waiting, writers queue up.
3. This is mostly fair, although once it lets a reader in, it lets ALL waiting readers in all at once, even if some showed up “after” other waiting writers.

The code is “simplified” because we know there can only be one writer at a time.

It also takes advantage of the fact that signal is a no-op if nobody is waiting.
1. In the “EndWrite” code (it signals CanWrite without checking for waiting writers)
2. In the EndRead code (same thing)

3. In StartRead (signals CanRead at the end)
With Semaphores we never did have a “fair” solution of this sort. In fact it can be done but the code is quite tricky. Here the straightforward solution works in the desired way! Monitors are less error-prone
and also easier to understand.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/reader-writers-solution-using-monitors/
✍
Write a Testimonial

IPC through shared memory
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ IPC through shared memory - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Inter Process Communication through shared memory is a concept where two or more process can access the common memory. And communication is done via this shared memory where changes made by one process
can be viewed by another process.
The problem with pipes, fifo and message queue – is that for two process to exchange information. The information has to go through the kernel.
Server reads from the input file.
The server writes this data in a message using either a pipe, fifo or message queue.
The client reads the data from the IPC channel,again requiring the data to be copied from kernel’s IPC buffer to the client’s buffer.
Finally the data is copied from the client’s buffer.

A total of four copies of data are required (2 read and 2 write). So, shared memory provides a way by letting two or more processes share a memory segment. With Shared Memory the data is only copied twice –
from input file into shared memory and from shared memory to the output file.
SYSTEM CALLS USED ARE:
ftok(): is use to generate a unique key.
shmget(): int shmget(key_t,size_tsize,intshmflg); upon successful completion, shmget() returns an identifier for the shared memory segment.
shmat(): Before you can use a shared memory segment, you have to attach yourself
to it using shmat(). void *shmat(int shmid ,void *shmaddr ,int shmflg);
shmid is shared memory id. shmaddr specifies specific address to use but we should set
it to zero and OS will automatically choose the address.
shmdt(): When you’re done with the shared memory segment, your program should
detach itself from it using shmdt(). int shmdt(void *shmaddr);
shmctl(): when you detach from shared memory,it is not destroyed. So, to destroy
shmctl() is used. shmctl(int shmid,IPC_RMID,NULL);
SHARED MEMORY FOR WRITER PROCESS
filter_none
edit
close
play_arrow
link
brightness_4
code
#include <iostream>
#include <sys/ipc.h>
#include <sys/shm.h>
#include <stdio.h>
using namespace std;
int main()
{
// ftok to generate unique key
key_t key = ftok("shmfile",65);
// shmget returns an identifier in shmid
int shmid = shmget(key,1024,0666|IPC_CREAT);
// shmat to attach to shared memory
char *str = (char*) shmat(shmid,(void*)0,0);
cout<<"Write Data : ";
gets(str);
printf("Data written in memory: %s\n",str);

//detach from shared memory
shmdt(str);
return 0;
}

chevron_right
filter_none
SHARED MEMORY FOR READER PROCESS
filter_none
edit
close
play_arrow
link
brightness_4
code
#include <iostream>
#include <sys/ipc.h>
#include <sys/shm.h>
#include <stdio.h>
using namespace std;
int main()
{
// ftok to generate unique key
key_t key = ftok("shmfile",65);
// shmget returns an identifier in shmid
int shmid = shmget(key,1024,0666|IPC_CREAT);
// shmat to attach to shared memory
char *str = (char*) shmat(shmid,(void*)0,0);
printf("Data read from memory: %s\n",str);
//detach from shared memory
shmdt(str);
// destroy the shared memory
shmctl(shmid,IPC_RMID,NULL);
return 0;
}

chevron_right
filter_none
Output:

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : ankursgh70

Source
https://www.geeksforgeeks.org/ipc-shared-memory/
✍
Write a Testimonial

IPC using Message Queues
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ IPC using Message Queues - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite : Inter Process Communication
A message queue is a linked list of messages stored within the kernel and identified by a message queue identifier. A new queue is created or an existing queue opened by msgget().
New messages are added to the end of a queue by msgsnd(). Every message has a positive long integer type field, a non-negative length, and the actual data bytes (corresponding to the length), all of which are specified to
msgsnd() when the message is added to a queue. Messages are fetched from a queue by msgrcv(). We don’t have to fetch the messages in a first-in, first-out order. Instead, we can fetch messages based on their type field.
All processes can exchange information through access to a common system message queue. The sending process places a message (via some (OS) message-passing module) onto a queue which can be read by another
process. Each message is given an identification or type so that processes can select the appropriate message. Process must share a common key in order to gain access to the queue in the first place.

System calls used for message queues:
​
ftok(): is use to generate a unique key.​
​
msgget(): either returns the message queue identifier for a newly created message ​
queue or returns the identifiers for a queue which exists with the same key value.​
​
msgsnd(): Data is placed on to a message queue by calling msgsnd().​
​
msgrcv(): messages are retrieved from a queue.​
​
msgctl(): It performs various operations on a queue. Generally it is use to ​
destroy message queue.​

MESSAGE QUEUE FOR WRITER PROCESS
filter_none
edit
close
play_arrow
link
brightness_4
code
// C Program for Message Queue (Writer Process)
#include <stdio.h>
#include <sys/ipc.h>
#include <sys/msg.h>
// structure for message queue
struct mesg_buffer {
long mesg_type;
char mesg_text[100];
} message;
int main()
{
key_t key;
int msgid;
// ftok to generate unique key
key = ftok("progfile", 65);
// msgget creates a message queue
// and returns identifier
msgid = msgget(key, 0666 | IPC_CREAT);
message.mesg_type = 1;
printf("Write Data : ");
gets(message.mesg_text);
// msgsnd to send message
msgsnd(msgid, &message, sizeof(message), 0);
// display the message
printf("Data send is : %s \n", message.mesg_text);
return 0;
}

chevron_right
filter_none
MESSAGE QUEUE FOR READER PROCESS
filter_none
edit
close
play_arrow
link
brightness_4
code
// C Program for Message Queue (Reader Process)
#include <stdio.h>
#include <sys/ipc.h>
#include <sys/msg.h>
// structure for message queue
struct mesg_buffer {
long mesg_type;
char mesg_text[100];
} message;
int main()
{
key_t key;
int msgid;
// ftok to generate unique key
key = ftok("progfile", 65);

// msgget creates a message queue
// and returns identifier
msgid = msgget(key, 0666 | IPC_CREAT);
// msgrcv to receive message
msgrcv(msgid, &message, sizeof(message), 1, 0);
// display the message
printf("Data Received is : %s \n",
message.mesg_text);
// to destroy the message queue
msgctl(msgid, IPC_RMID, NULL);
return 0;
}

chevron_right
filter_none
Output:

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/ipc-using-message-queues/
✍
Write a Testimonial

Bakery Algorithm in Process Synchronization
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Bakery Algorithm in Process Synchronization - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Critical Section, Process Synchronization, Inter Process Communication
The Bakery algorithm is one of the simplest known solutions to the mutual exclusion problem for the general case of N process. Bakery Algorithm is a critical section solution for N processes. The algorithm preserves
the first come first serve property.
Before entering its critical section, the process receives a number. Holder of the smallest number enters the critical section.
If processes Pi and Pj receive the same number,
if i < j ​
Pi is served first; ​
else ​
Pj is served first.

The numbering scheme always generates numbers in increasing order of enumeration; i.e., 1, 2, 3, 3, 3, 3, 4, 5, …
Notation – lexicographical order (ticket #, process id #) – Firstly the ticket number is compared. If same then the process ID is compared next, i.e.-

– (a, b) < (c, d) if a < c or if a = c and b < d​
– max(a [0], . . ., a [n-1]) is a number, k, such that k >= a[i]

for i = 0, . . ., n - 1

Shared data – choosing is an array [0..n – 1] of boolean values; & number is an array [0..n – 1] of integer values. Both are initialized to False & Zero respectively.
Algorithm Pseudocode –
​
repeat​
choosing[i] := true;​
number[i] := max(number[0], number[1], ..., number[n - 1])+1;​
choosing[i] := false;​
for j := 0 to n - 1​
do begin​
while choosing[j] do no-op;​
while number[j] != 0​
and (number[j], j) < (number[i], i) do no-op;​
end;​
​
critical section​
​
number[i] := 0;​
​
remainder section​
​
until false;​

Explanation –
Firstly the process sets its “choosing” variable to be TRUE indicating its intent to enter critical section. Then it gets assigned the highest ticket number corresponding to other processes. Then the “choosing” variable is set
to FALSE indicating that it now has a new ticket number. This is in-fact the most important and confusing part of the algorithm.
It is actually a small critical section in itself ! The very purpose of the first three lines is that if a process is modifying its TICKET value then at that time some other process should not be allowed to check its old ticket
value which is now obsolete. This is why inside the for loop before checking ticket value we first make sure that all other processes have the “choosing” variable as FALSE.
After that we proceed to check the ticket values of processes where process with least ticket number/process id gets inside the critical section. The exit section just resets the ticket value to zero.
Code – Here’s the C code implementation of the Bakery Algorithm. Run the following in a UNIX environment –
filter_none
edit

close
play_arrow
link
brightness_4
code
// Importing the thread library
#include "pthread.h"
#include "stdio.h"
// Importing POSIX Operating System API library
#include "unistd.h"
#include "string.h"
// This is a memory barrier instruction.
// Causes compiler to enforce an ordering
// constraint on memory operations.
// This means that operations issued prior
// to the barrier will be performed
// before operations issued after the barrier.
#define MEMBAR __sync_synchronize()
#define THREAD_COUNT 8
volatile int tickets[THREAD_COUNT];
volatile int choosing[THREAD_COUNT];
// VOLATILE used to prevent the compiler
// from applying any optimizations.
volatile int resource;
void lock(int thread)
{
// Before getting the ticket number
//"choosing" variable is set to be true
choosing[thread] = 1;
MEMBAR;
// Memory barrier applied
int max_ticket = 0;
// Finding Maximum ticket value among current threads
for (int i = 0; i < THREAD_COUNT; ++i) {
int ticket = tickets[i];
max_ticket = ticket > max_ticket ? ticket : max_ticket;
}
// Allotting a new ticket value as MAXIMUM + 1
tickets[thread] = max_ticket + 1;
MEMBAR;
choosing[thread] = 0;
MEMBAR;
// The ENTRY Section starts from here
for (int other = 0; other < THREAD_COUNT; ++other) {
// Applying the bakery algorithm conditions
while (choosing[other]) {
}
MEMBAR;
while (tickets[other] != 0 && (tickets[other]
< tickets[thread]
|| (tickets[other]
== tickets[thread]
&& other < thread))) {
}
}
}
// EXIT Section
void unlock(int thread)
{
MEMBAR;
tickets[thread] = 0;
}
// The CRITICAL Section
void use_resource(int thread)
{
if (resource != 0) {
printf("Resource was acquired by %d, but is still in-use by %d!\n",
thread, resource);
}
resource = thread;
printf("%d using resource...\n", thread);
MEMBAR;
sleep(2);
resource = 0;
}
// A simplified function to show the implementation
void* thread_body(void* arg)
{
long thread = (long)arg;
lock(thread);
use_resource(thread);
unlock(thread);
return NULL;
}
int main(int argc, char** argv)
{
memset((void*)tickets, 0, sizeof(tickets));
memset((void*)choosing, 0, sizeof(choosing));
resource = 0;
// Declaring the thread variables
pthread_t threads[THREAD_COUNT];
for (int i = 0; i < THREAD_COUNT; ++i) {
// Creating a new thread with the function
//"thread_body" as its thread routine
pthread_create(&threads[i], NULL, &thread_body, (void*)((long)i));
}
for (int i = 0; i < THREAD_COUNT; ++i) {
// Reaping the resources used by
// all threads once their task is completed !
pthread_join(threads[i], NULL);
}

return 0;
}

chevron_right
filter_none
Output:

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/bakery-algorithm-in-process-synchronization/
✍
Write a Testimonial

Dekker’s algorithm in Process Synchronization
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Dekker's algorithm in Process Synchronization - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Process Synchronization, Inter Process Communication
To obtain such a mutual exclusion, bounded waiting, and progress there have been several algorithms implemented, one of which is Dekker’s Algorithm. To understand the algorithm let’s understand the solution to the
critical section problem first.
A process is generally represented as :
​
do {​
//entry section​
critical section​
//exit section​
remainder section​
} while (TRUE);​

The solution to critical section problem must ensure following three conditions:
1. Mutual Exclusion
2. Progress
3. Bounded Waiting
One of solution for ensuring above all factors is Peterson’s solution.

Another one is Dekker’s Solution. Dekker’s algorithm was the first provably-correct solution to the critical section problem. It allows two threads to share a single-use resource without conflict, using only shared memory
for communication. It avoids the strict alternation of a naïve turn-taking algorithm, and was one of the first mutual exclusion algorithms to be invented.
Although there are many versions of Dekker’s Solution, the final or 5th version is the one that satisfies all of the above conditions and is the most efficient of them all.
Note – Dekker’s Solution, mentioned here, ensures mutual exclusion between two processes only, it could be extended to more than two processes with the proper use of arrays and variables.
Algorithm – It requires both an array of Boolean values and an integer variable:
​
var flag: array [0..1] of boolean;​
turn: 0..1;​
repeat​
​
flag[i] := true;​
while flag[j] do​
if turn = j then​
begin​
flag[i] := false;​
while turn = j do no-op;​
flag[i] := true;​
end;​
​
critical section​
​
turn := j;​
flag[i] := false;​
​

remainder section​
​
until false;​

First Version of Dekker’s Solution – The idea is to use common or shared thread number between processes and stop the other process from entering its critical section if the shared thread indicates the former one
already running.
filter_none
edit
close
play_arrow
link
brightness_4
code
Main()
{
int thread_number = 1;
startThreads();
}
Thread1()
{
do {
// entry section
// wait until threadnumber is 1
while (threadnumber == 2)
;
// critical section
// exit section
// give access to the other thread
threadnumber = 2;
// remainder section
} while (completed == false)
}
Thread2()
{
do {
// entry section
// wait until threadnumber is 2
while (threadnumber == 1)
;
// critical section
// exit section
// give access to the other thread
threadnumber = 1;
// remainder section
} while (completed == false)
}

chevron_right
filter_none
The problem arising in the above implementation is lockstep synchronization, i.e each thread depends on the other for its execution. If one of the processes completes, then the second process runs, gives access to the
completed one and waits for its turn, however, the former process is already completed and would never run to return the access back to the latter one. Hence, the second process waits infinitely then.
Second Version of Dekker’s Solution – To remove lockstep synchronization, it uses two flags to indicate its current status and updates them accordingly at the entry and exit section.
filter_none
edit
close
play_arrow
link
brightness_4
code
Main()
{
// flags to indicate if each thread is in
// its critial section or not.
boolean thread1 = false;
boolean thread2 = false;
startThreads();
}
Thread1()
{
do {
// entry section
// wait until thread2 is in its critical section
while (thread2 == true)
;
// indicate thread1 entering its critical section
thread1 = true;
// critical section
// exit section
// indicate thread1 exiting its critical section
thread1 = false;
// remainder section
} while (completed == false)
}
Thread2()
{
do {
// entry section
// wait until thread1 is in its critical section
while (thread1 == true)
;
// indicate thread2 entering its critical section
thread2 = true;

// critical section
// exit section
// indicate thread2 exiting its critical section
thread2 = false;
// remainder section
} while (completed == false)
}

chevron_right
filter_none
The problem arising in the above version is mutual exclusion itself. If threads are preempted (stopped) during flag updation ( i.e during current_thread = true ) then, both the threads enter their critical section once the
preempted thread is restarted, also the same can be observed at the start itself, when both the flags are false.
Third Version of Dekker’s Solution – To re-ensure mutual exclusion, it sets the flags before entry section itself.
filter_none
edit
close
play_arrow
link
brightness_4
code
Main()
{
// flags to indicate if each thread is in
// queue to enter its critical section
boolean thread1wantstoenter = false;
boolean thread2wantstoenter = false;
startThreads();
}
Thread1()
{
do {
thread1wantstoenter = true;
// entry section
// wait until thread2 wants to enter
// its critical section
while (thread2wantstoenter == true)
;
// critical section
// exit section
// indicate thread1 has completed
// its critical section
thread1wantstoenter = false;
// remainder section
} while (completed == false)
}
Thread2()
{
do {
thread2wantstoenter = true;
// entry section
// wait until thread1 wants to enter
// its critical section
while (thread1wantstoenter == true)
;
// critical section
// exit section
// indicate thread2 has completed
// its critical section
thread2wantstoenter = false;
// remainder section
} while (completed == false)
}

chevron_right
filter_none
The problem with this version is deadlock possibility. Both threads could set their flag as true simultaneously and both will wait infinitely later on.

Fourth Version of Dekker’s Solution – Uses small time interval to recheck the condition, eliminates deadlock and ensures mutual exclusion as well.
filter_none
edit
close
play_arrow
link
brightness_4
code
Main()
{
// flags to indicate if each thread is in
// queue to enter its critical section
boolean thread1wantstoenter = false;
boolean thread2wantstoenter = false;
startThreads();
}
Thread1()
{
do {
thread1wantstoenter = true;

while (thread2wantstoenter == true) {
// gives access to other thread
// wait for random amount of time
thread1wantstoenter = false;
thread1wantstoenter = true;
}
// entry section
// wait until thread2 wants to enter
// its critical section
// critical section
// exit section
// indicate thread1 has completed
// its critical section
thread1wantstoenter = false;
// remainder section
} while (completed == false)
}
Thread2()
{
do {
thread2wantstoenter = true;
while (thread1wantstoenter == true) {
// gives access to other thread
// wait for random amount of time
thread2wantstoenter = false;
thread2wantstoenter = true;
}
// entry section
// wait until thread1 wants to enter
// its critical section
// critical section
// exit section
// indicate thread2 has completed
// its critical section
thread2wantstoenter = false;
// remainder section
} while (completed == false)
}

chevron_right
filter_none
The problem with this version is the indefinite postponement. Also, random amount of time is erratic depending upon the situation in which the algorithm is being implemented, hence not an acceptable solution in business
critical systems.
Dekker’s Algorithm : Final and completed Solution – -Idea is to use favoured thread notion to determine entry to the critical section. Favoured thread alternates between the thread providing mutual exclusion and
avoiding deadlock, indefinite postponement or lockstep synchronization.
filter_none
edit
close
play_arrow
link
brightness_4
code
Main()
{
// to denote which thread will enter next
int favouredthread = 1;
// flags to indicate if each thread is in
// queue to enter its critical section
boolean thread1wantstoenter = false;
boolean thread2wantstoenter = false;
startThreads();
}
Thread1()
{
do {
thread1wantstoenter = true;
// entry section
// wait until thread2 wants to enter
// its critical section
while (thread2wantstoenter == true) {
// if 2nd thread is more favored
if (favaouredthread == 2) {
// gives access to other thread
thread1wantstoenter = false;
// wait until this thread is favored
while (favouredthread == 2)
;
thread1wantstoenter = true;
}
}
// critical section
// favor the 2nd thread
favouredthread = 2;
// exit section
// indicate thread1 has completed
// its critical section
thread1wantstoenter = false;
// remainder section
} while (completed == false)
}
Thread2()
{

do {
thread2wantstoenter = true;
// entry section
// wait until thread1 wants to enter
// its critical section
while (thread1wantstoenter == true) {
// if 1st thread is more favored
if (favaouredthread == 1) {
// gives access to other thread
thread2wantstoenter = false;
// wait until this thread is favored
while (favouredthread == 1)
;
thread2wantstoenter = true;
}
}
// critical section
// favour the 1st thread
favouredthread = 1;
// exit section
// indicate thread2 has completed
// its critical section
thread2wantstoenter = false;
// remainder section
} while (completed == false)
}

chevron_right
filter_none
This version guarantees a complete solution to the critical solution problem.
References –
Dekker’s Algorithm -csisdmz.ul.ie
Dekker’s algorithm – Wikipedia

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/dekkers-algorithm-in-process-synchronization/
✍
Write a Testimonial

Introduction of System Call
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Introduction of System Call - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
In computing, a system call is the programmatic way in which a computer program requests a service from the kernel of the operating system it is executed on. A system call is a way for programs to interact with the
operating system. A computer program makes a system call when it makes a request to the operating system’s kernel. System call provides the services of the operating system to the user programs via Application
Program Interface(API). It provides an interface between a process and operating system to allow user-level processes to request services of the operating system. System calls are the only entry points into the kernel
system. All programs needing resources must use system calls.
Services Provided by System Calls :
1.
2.
3.
4.
5.
6.

Process creation and management
Main memory management
File Access, Directory and File system management
Device handling(I/O)
Protection
Networking, etc.
Types of System Calls : There are 5 different categories of system calls –

1.
2.
3.
4.
5.

Process control: end, abort, create, terminate, allocate and free memory.
File management: create, open, close, delete, read file etc.
Device management
Information maintenance
Communication

Examples of Windows and Unix System Calls –
Windows

Unix

CreateProcess()
fork()
ExitProcess()
exit()
WaitForSingleObject()
wait()
CreateFile()
open()
ReadFile()
read()
File Manipulation
WriteFile()
write()
CloseHandle()
close()
SetConsoleMode()
ioctl()
Device Manipulation
ReadConsole()
read()
WriteConsole()
write()
GetCurrentProcessID()
getpid()
Information Maintenance SetTimer()
alarm()
Sleep()
sleep()
CreatePipe()
pipe()
Communication
CreateFileMapping()
shmget()
MapViewOfFile()
mmap()
SetFileSecurity()
chmod()
Protection
InitlializeSecurityDescriptor() umask()
SetSecurityDescriptorGroup() chown()
Process Control

Reference –http://www.cs.columbia.edu/~jae/4118/L02-intro2-osc-ch2.pdf
Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : manjit

Source
https://www.geeksforgeeks.org/introduction-of-system-call/
✍
Write a Testimonial

Banker’s Algorithm in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Banker's Algorithm in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
The banker’s algorithm is a resource allocation and deadlock avoidance algorithm that tests for safety by simulating the allocation for predetermined maximum possible amounts of all resources, then makes an “s-state”
check to test for possible activities, before deciding whether allocation should be allowed to continue.
Following Data structures are used to implement the Banker’s Algorithm:
Let ‘n’ be the number of processes in the system and ‘m’ be the number of resources types.
Available :
It is a 1-d array of size ‘m’ indicating the number of available resources of each type.
Available[ j ] = k means there are ‘k’ instances of resource type R j
Max :
It is a 2-d array of size ‘n*m’ that defines the maximum demand of each process in a system.
Max[ i, j ] = k means process Pi may request at most ‘k’ instances of resource type R j.
Allocation :
It is a 2-d array of size ‘n*m’ that defines the number of resources of each type currently allocated to each process.
Allocation[ i, j ] = k means process Pi is currently allocated ‘k’ instances of resource type R j
Need :
It is a 2-d array of size ‘n*m’ that indicates the remaining resource need of each process.
Need [ i, j ] = k means process Pi currently need ‘k’ instances of resource type R j
for its execution.
Need [ i, j ] = Max [ i, j ] – Allocation [ i, j ]

Allocationi specifies the resources currently allocated to process P i and Needi specifies the additional resources that process P i may still request to complete its task.
Banker’s algorithm consists of Safety algorithm and Resource request algorithm
Safety Algorithm
The algorithm for finding out whether or not a system is in a safe state can be described as follows:
1) Let Work and Finish be vectors of length ‘m’ and ‘n’ respectively.
Initialize: Work = Available
Finish[i] = false; for i=1, 2, 3, 4….n
2) Find an i such that both
a) Finish[i] = false
b) Needi <= Work
if no such i exists goto step (4)
3) Work = Work + Allocation[i]
Finish[i] = true
goto step (2)
4) if Finish [i] = true for all i
then the system is in a safe state

Resource-Request Algorithm
Let Requesti be the request array for process P i. Request i [j] = k means process Pi wants k instances of resource type R j. When a request for resources is made by process Pi, the following actions are taken:
1) If Request i <= Needi
Goto step (2) ; otherwise, raise an error condition, since the process has exceeded its maximum claim.
2) If Request i <= Available
Goto step (3); otherwise, Pi must wait, since the resources are not available.
3) Have the system pretend to have allocated the requested resources to process Pi by modifying the state as
follows:
Available = Available – Requesti
Allocationi = Allocationi + Request i
Needi = Needi– Request i
Example:
Considering a system with five processes P 0 through P4 and three resources of type A, B, C. Resource type A has 10 instances, B has 5 instances and type C has 7 instances. Suppose at time t 0 following
snapshot of the system has been taken:

Question1. What will be the content of the Need matrix?
Need [i, j] = Max [i, j] – Allocation [i, j]
So, the content of Need Matrix is:

Question2. Is the system in a safe state? If Yes, then what is the safe sequence?
Applying the Safety algorithm on the given system,

Question3. What will happen if process P1 requests one additional instance of resource type A and two instances of resource type C?

We must determine whether this new system state is safe. To do so, we again execute Safety algorithm on the above data structures.

Hence the new system state is safe, so we can immediately grant the request for process P1 .
Code for Banker’s Algorithm
C/C++
filter_none
edit
close
play_arrow
link
brightness_4
code
// Banker's Algorithm
#include <stdio.h>
int main()
{
// P0, P1, P2, P3, P4 are the Process names here
int
n =
m =
int

n, m, i, j, k;
5; // Number of processes
3; // Number of resources
alloc[5][3] = { { 0, 1, 0 }, // P0
{ 2, 0, 0 }, // P1
{ 3, 0, 2 }, // P2
{ 2, 1, 1 }, // P3
{ 0, 0, 2 } }; // P4

int max[5][3] = { {
{
{
{
{

7,
3,
9,
2,
4,

5,
2,
0,
2,
3,

3
2
2
2
3

}, // P0
}, // P1
}, // P2
}, // P3
} }; // P4

// Allocation Matrix

// MAX Matrix

int avail[3] = { 3, 3, 2 }; // Available Resources
int f[n], ans[n], ind = 0;
for (k = 0; k < n; k++) {
f[k] = 0;
}
int need[n][m];
for (i = 0; i < n; i++) {
for (j = 0; j < m; j++)
need[i][j] = max[i][j] - alloc[i][j];
}
int y = 0;
for (k = 0; k < 5; k++) {
for (i = 0; i < n; i++) {
if (f[i] == 0) {
int flag = 0;
for (j = 0; j < m; j++) {
if (need[i][j] > avail[j]){
flag = 1;
break;
}
}
if (flag == 0) {
ans[ind++] = i;
for (y = 0; y < m; y++)
avail[y] += alloc[i][y];
f[i] = 1;
}
}
}
}
printf("Following is the SAFE Sequence\n");
for (i = 0; i < n - 1; i++)
printf(" P%d ->", ans[i]);
printf(" P%d", ans[n - 1]);
return (0);
// This code is contributed by Deep Baldha (CandyZack)
}

chevron_right
filter_none
Java
filter_none
edit
close
play_arrow
link
brightness_4
code
//Java Program for Bankers Algorithm
public class GfGBankers
{
int n = 5; // Number of processes
int m = 3; // Number of resources

int
int
int
int
int

need[][] = new int[n][m];
[][]max;
[][]alloc;
[]avail;
safeSequence[] = new int[n];

void initializeValues()
{
// P0, P1, P2, P3, P4 are the Process names here
// Allocation Matrix
alloc = new int[][] { { 0, 1, 0 }, //P0
{ 2, 0, 0 }, //P1
{ 3, 0, 2 }, //P2
{ 2, 1, 1 }, //P3
{ 0, 0, 2 } }; //P4
// MAX Matrix
max = new int[][] { { 7, 5, 3 }, //P0
{ 3, 2, 2 }, //P1
{ 9, 0, 2 }, //P2
{ 2, 2, 2 }, //P3
{ 4, 3, 3 } }; //P4
// Available Resources
avail = new int[] { 3, 3, 2 };
}
void isSafe()
{
int count=0;
//visited array to find the already allocated process
boolean visited[] = new boolean[n];
for (int i = 0;i < n; i++)
{
visited[i] = false;
}
//work array to store the copy of available resources
int work[] = new int[m];
for (int i = 0;i < m; i++)
{
work[i] = avail[i];
}
while (count<n)
{
boolean flag = false;
for (int i = 0;i < n; i++)
{
if (visited[i] == false)
{
int j;
for (j = 0;j < m; j++)
{
if (need[i][j] > work[j])
break;
}
if (j == m)
{
safeSequence[count++]=i;
visited[i]=true;
flag=true;
for (j = 0;j < m; j++)
{
work[j] = work[j]+alloc[i][j];
}
}
}
}
if (flag == false)
{
break;
}
}
if (count < n)
{
System.out.println("The System is UnSafe!");
}
else
{
//System.out.println("The given System is Safe");
System.out.println("Following is the SAFE Sequence");
for (int i = 0;i < n; i++)
{
System.out.print("P" + safeSequence[i]);
if (i != n-1)
System.out.print(" -> ");
}
}
}
void calculateNeed()
{
for (int i = 0;i < n; i++)
{
for (int j = 0;j < m; j++)
{
need[i][j] = max[i][j]-alloc[i][j];
}
}
}
public static void main(String[] args)
{
int i, j, k;
GfGBankers gfg = new GfGBankers();
gfg.initializeValues();
//Calculate the Need Matrix
gfg.calculateNeed();
// Check whether system is in safe state or not
gfg.isSafe();
}
}

chevron_right
filter_none
C#
filter_none
edit
close
play_arrow
link
brightness_4
code
// C# Program for Bankers Algorithm

using System;
using System.Collections.Generic;
class GFG
{
static int n = 5; // Number of processes
static int m = 3; // Number of resources
int [,]need = new int[n, m];
int [,]max;
int [,]alloc;
int []avail;
int []safeSequence = new int[n];
void initializeValues()
{
// P0, P1, P2, P3, P4 are the Process
// names here Allocation Matrix
alloc = new int[,] {{ 0, 1, 0 }, //P0
{ 2, 0, 0 }, //P1
{ 3, 0, 2 }, //P2
{ 2, 1, 1 }, //P3
{ 0, 0, 2 }};//P4
// MAX Matrix
max = new int[,] {{ 7, 5, 3
{ 3, 2,
{ 9, 0, 2
{ 2, 2, 2
{ 4, 3, 3

}, //P0
2 }, //P1
}, //P2
}, //P3
}};//P4

// Available Resources
avail = new int[] { 3, 3, 2 };
}
void isSafe()
{
int count = 0;
// visited array to find the
// already allocated process
Boolean []visited = new Boolean[n];
for (int i = 0; i < n; i++)
{
visited[i] = false;
}
// work array to store the copy of
// available resources
int []work = new int[m];
for (int i = 0; i < m; i++)
{
work[i] = avail[i];
}
while (count<n)
{
Boolean flag = false;
for (int i = 0; i < n; i++)
{
if (visited[i] == false)
{
int j;
for (j = 0; j < m; j++)
{
if (need[i, j] > work[j])
break;
}
if (j == m)
{
safeSequence[count++] = i;
visited[i] = true;
flag = true;
for (j = 0; j < m; j++)
{
work[j] = work[j] + alloc[i, j];
}
}
}
}
if (flag == false)
{
break;
}
}
if (count < n)
{
Console.WriteLine("The System is UnSafe!");
}
else
{
//System.out.println("The given System is Safe");
Console.WriteLine("Following is the SAFE Sequence");
for (int i = 0; i < n; i++)
{
Console.Write("P" + safeSequence[i]);
if (i != n - 1)
Console.Write(" -> ");
}
}
}
void calculateNeed()
{
for (int i = 0;i < n; i++)
{
for (int j = 0;j < m; j++)
{
need[i, j] = max[i, j] - alloc[i, j];
}
}
}
// Driver Code
public static void Main(String[] args)
{
GFG gfg = new GFG();
gfg.initializeValues();
// Calculate the Need Matrix
gfg.calculateNeed();
// Check whether system is in
// safe state or not
gfg.isSafe();
}
}
// This code is contributed by Rajput-Ji

chevron_right
filter_none
Output:
​
Following is the SAFE Sequence​
P1 -> P3 -> P4 -> P0 -> P2​

GATE question:
http://quiz.geeksforgeeks.org/gate-gate-cs-2014-set-1-question-41/
Reference:
Operating System Concepts 8th Edition by Abraham Silberschatz, Peter B. Galvin, Greg Gagne
This article has been contributed by Vikash Kumar. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above

Improved By : Deepanshu8391, CandyZack, ShJos, tarlisonbrito, Rajput-Ji

Source
https://www.geeksforgeeks.org/bankers-algorithm-in-operating-system-2/
✍
Write a Testimonial

Multiple-Processor Scheduling in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Multiple-Processor Scheduling in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
In multiple-processor scheduling multiple CPU’s are available and hence Load Sharing becomes possible. However multiple processor scheduling is more complex as compared to single processor scheduling. In
multiple processor scheduling there are cases when the processors are identical i.e. HOMOGENEOUS, in terms of their functionality, we can use any processor available to run any process in the queue.
One approach is when all the scheduling decisions and I/O processing are handled by a single processor which is called the Master Server and the other processors executes only the user code. This is simple and reduces
the need of data sharing. This entire scenario is called Asymmetric Multiprocessing.
A second approach uses Symmetric Multiprocessing where each processor is self scheduling. All processes may be in a common ready queue or each processor may have its own private queue for ready processes. The
scheduling proceeds further by having the scheduler for each processor examine the ready queue and select a process to execute.

Processor Affinity means a processes has an affinity for the processor on which it is currently running.
When a process runs on a specific processor there are certain effects on the cache memory. The data most recently accessed by the process populate the cache for the processor and as a result successive memory access by
the process are often satisfied in the cache memory. Now if the process migrates to another processor, the contents of the cache memory must be invalidated for the first processor and the cache for the second processor
must be repopulated. Because of the high cost of invalidating and repopulating caches, most of the SMP(symmetric multiprocessing) systems try to avoid migration of processes from one processor to another and try to
keep a process running on the same processor. This is known as PROCESSOR AFFINITY.
There are two types of processor affinity:
1. Soft Affinity – When an operating system has a policy of attempting to keep a process running on the same processor but not guaranteeing it will do so, this situation is called soft affinity.
2. Hard Affinity – Some systems such as Linux also provide some system calls that support Hard Affinity which allows a process to migrate between processors.
Load Balancing is the phenomena which keeps the workload evenly distributed across all processors in an SMP system. Load balancing is necessary only on systems where each processor has its own private queue of
process which are eligible to execute. Load balancing is unnecessary because once a processor becomes idle it immediately extracts a runnable process from the common run queue. On SMP(symmetric multiprocessing), it
is important to keep the workload balanced among all processors to fully utilize the benefits of having more than one processor else one or more processor will sit idle while other processors have high workloads along
with lists of processors awaiting the CPU.
There are two general approaches to load balancing :
1. Push Migration – In push migration a task routinely checks the load on each processor and if it finds an imbalance then it evenly distributes load on each processors by moving the processes from overloaded to idle
or less busy processors.
2. Pull Migration – Pull Migration occurs when an idle processor pulls a waiting task from a busy processor for its execution.
In multicore processors multiple processor cores are places on the same physical chip. Each core has a register set to maintain its architectural state and thus appears to the operating system as a separate physical
processor. SMP systems that use multicore processors are faster and consume less power than systems in which each processor has its own physical chip.
However multicore processors may complicate the scheduling problems. When processor accesses memory then it spends a significant amount of time waiting for the data to become available. This situation is called
MEMORY STALL. It occurs for various reasons such as cache miss, which is accessing the data that is not in the cache memory. In such cases the processor can spend upto fifty percent of its time waiting for data to
become available from the memory. To solve this problem recent hardware designs have implemented multithreaded processor cores in which two or more hardware threads are assigned to each core. Therefore if one
thread stalls while waiting for the memory, core can switch to another thread.
There are two ways to multithread a processor :
1. Coarse-Grained Multithreading – In coarse grained multithreading a thread executes on a processor until a long latency event such as a memory stall occurs, because of the delay caused by the long latency event,
the processor must switch to another thread to begin execution. The cost of switching between threads is high as the instruction pipeline must be terminated before the other thread can begin execution on the
processor core. Once this new thread begins execution it begins filling the pipeline with its instructions.
2. Fine-Grained Multithreading – This multithreading switches between threads at a much finer level mainly at the boundary of an instruction cycle. The architectural design of fine grained systems include logic for
thread switching and as a result the cost of switching between threads is small.
In this type of multiple-processor scheduling even a single CPU system acts like a multiple-processor system. In a system with Virtualization, the virtualization presents one or more virtual CPU to each of virtual
machines running on the system and then schedules the use of physical CPU among the virtual machines. Most virtualized environments have one host operating system and many guest operating systems. The host
operating system creates and manages the virtual machines. Each virtual machine has a guest operating system installed and applications run within that guest.Each guest operating system may be assigned for specific use
cases,applications or users including time sharing or even real-time operation. Any guest operating-system scheduling algorithm that assumes a certain amount of progress in a given amount of time will be negatively
impacted by the virtualization. A time sharing operating system tries to allot 100 milliseconds to each time slice to give users a reasonable response time. A given 100 millisecond time slice may take much more than 100
milliseconds of virtual CPU time. Depending on how busy the system is, the time slice may take a second or more which results in a very poor response time for users logged into that virtual machine. The net effect of
such scheduling layering is that individual virtualized operating systems receive only a portion of the available CPU cycles, even though they believe they are receiving all cycles and that they are scheduling all of those
cycles.Commonly, the time-of-day clocks in virtual machines are incorrect because timers take no longer to trigger than they would on dedicated CPU’s.
Virtualizations can thus undo the good scheduling-algorithm efforts of the operating systems within virtual machines.
Reference –
Operating System Principles – Galvin

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : VighneshKamath

Source
https://www.geeksforgeeks.org/multiple-processor-scheduling-in-operating-system/

✍
Write a Testimonial

Buddy System – Memory allocation technique
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Buddy System - Memory allocation technique - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Partition Allocation Methods
Static partition schemes suffer from the limitation of having the fixed number of active processes and the usage of space may also not be optimal. The buddy system is a memory allocation and management algorithm
that manages memory in power of two increments. Assume the memory size is 2 U, suppose a size of S is required.
If 2 U-1 <S<=2U: Allocate the whole block
Else: Recursively divide the block equally and test the condition at each time, when it satisfies, allocate the block and get out the loop.
System also keep the record of all the unallocated blocks each and can merge these different size blocks to make one big chunk.
Advantage –
Easy to implement a buddy system
Allocates block of correct size
It is easy to merge adjacent holes
Fast to allocate memory and de-allocating memory
Disadvantage –

It requires all allocation unit to be powers of two
It leads to internal fragmentation
Example –
Consider a system having buddy system with physical address space 128 KB.Calculate the size of partition for 18 KB process.
Solution –

So, size of partition for 18 KB process = 32 KB. It divides by 2, till possible to get minimum block to fit 18 KB.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : soumya7, RakshithSathish

Source
https://www.geeksforgeeks.org/buddy-system-memory-allocation-technique/
✍
Write a Testimonial

Process Table and Process Control Block (PCB)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Process Table and Process Control Block (PCB) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​

Suggest a Topic

Select a Category​
menu
While creating a process the operating system performs several operations. To identify these process, it must identify each process, hence it assigns a process identification number (PID) to each process. As the operating
system supports multi-programming, it needs to keep track of all the processes. For this task, the process control block (PCB) is used to track the process’s execution status. Each block of memory contains information
about the process state, program counter, stack pointer, status of opened files, scheduling algorithms, etc. All these information is required and must be saved when the process is switched from one state to another. When
the process made transitions from one state to another, the operating system must update information in the process’s PCB.
A process control block (PCB) contains information about the process, i.e. registers, quantum, priority, etc. The process table is an array of PCB’s, that means logically contains a PCB for all of the current processes in the
system.

Pointer – It is a stack pointer which is required to be saved when the process is switched from one state to another to retain the current position of the process.
Process state – It stores the respective state of the process.
Process number – Every process is assigned with a unique id known as process ID or PID which stores the process identifier.
Program counter – It stores the counter which contains the address of the next instruction that is to be executed for the process.
Register – These are the CPU registers which includes: accumulator, base, registers and general purpose registers.
Memory limits – This field contains the information about memory management system used by operating system. This may include the page tables, segment tables etc.
Open files list – This information includes the list of files opened for a process.
Miscellaneous accounting and status data – This field includes information about the amount of CPU used, time constraints, jobs or process number, etc.
The process control block stores the register content also known as execution content of the processor when it was blocked from running. This execution content architecture enables the operating system to restore a
process’s execution context when the process returns to the running state. When the process made transitions from one state to another, the operating system update its information in the process’s PCB. The operating
system maintains pointers to each process’s PCB in a process table so that it can access the PCB quickly.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : magbene

Source
https://www.geeksforgeeks.org/process-table-and-process-control-block-pcb/
✍
Write a Testimonial

Difference between 32-bit and 64-bit operating systems
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between 32-bit and 64-bit operating systems - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
In computing, there exist two type processor i.e., 32-bit and 64-bit. These processor tells us how much memory a processor can have access from a CPU register. For instance,
A 32-bit system can access 232 memory addresses, i.e 4 GB of RAM or physical memory.
A 64-bit system can access 264 memory addresses, i.e actually 18-Quintillion GB of RAM. In short, any amount of memory greater than 4 GB can be easily handled by it.

Most computers made in the 1990s and early 2000s were 32-bit machines. The CPU register stores memory addresses, which is how the processor accesses data from RAM. One bit in the register can reference an
individual byte in memory, so a 32-bit system can address a maximum of 4 GB (4,294,967,296 bytes) of RAM. The actual limit is often less around 3.5 GB, since part of the register is used to store other temporary values
besides memory addresses. Most computers released over the past two decades were built on a 32-bit architecture, hence most operating systems were designed to run on a 32-bit processor.

A 64-bit register can theoretically reference 18,446,744,073,709,551,616 bytes, or 17,179,869,184 GB (16 exabytes) of memory. This is several million times more than an average workstation would need to access.
What’s important is that a 64-bit computer (which means it has a 64-bit processor) can access more than 4 GB of RAM. If a computer has 8 GB of RAM, it better have a 64-bit processor. Otherwise, at least 4 GB of the
memory will be inaccessible by the CPU.
A major difference between 32-bit processors and 64-bit processors is the number of calculations per second they can perform, which affects the speed at which they can complete tasks. 64-bit processors can come in
dual core, quad core, six core, and eight core versions for home computing. Multiple cores allow for an increased number of calculations per second that can be performed, which can increase the processing power and
help make a computer run faster. Software programs that require many calculations to function smoothly can operate faster and more efficiently on the multi-core 64-bit processors, for the most part.
Advantages of 64-bit over 32-bit
Using 64-bit one can do a lot in multi-tasking, user can easily switch between various applications without any windows hanging problems.
Gamers can easily plays High graphical games like Modern Warfare, GTA V, or use high-end softwares like Photoshop or CAD which takes a lot of memory, since it makes multi-tasking with big softwares easy and
efficient for users. However upgrading the video card instead of getting a 64-bit processor would be more beneficial.
Note:
A computer with a 64-bit processor can have a 64-bit or 32-bit version of an operating system installed. However, with a 32-bit operating system, the 64-bit processor would not run at its full capability.
On a computer with a 64-bit processor, we can’t run a 16-bit legacy program. Many 32-bit programs will work with a 64-bit processor and operating system, but some older 32-bit programs may not function
properly, or at all, due to limited or no compatibility.
References: https://www.computerhope.com/issues/ch001498.htm

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Improved By : flick07, DeepaBharti1

Source
https://www.geeksforgeeks.org/difference-32-bit-64-bit-operating-systems/
✍
Write a Testimonial

Dining Philosopher Problem Using Semaphores
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Dining Philosopher Problem Using Semaphores - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Process Synchronization, Semaphores, Dining-Philosophers Solution Using Monitors
The Dining Philosopher Problem – The Dining Philosopher Problem states that K philosophers seated around a circular table with one chopstick between each pair of philosophers. There is one chopstick between each
philosopher. A philosopher may eat if he can pickup the two chopsticks adjacent to him. One chopstick may be picked up by any one of its adjacent followers but not both.

Semaphore Solution to Dining Philosopher –
Each philosopher is represented by the following pseudocode:
​
process P[i]​
while true do​
{ THINK;​
PICKUP(CHOPSTICK[i], CHOPSTICK[i+1 mod 5]);​
EAT;​
PUTDOWN(CHOPSTICK[i], CHOPSTICK[i+1 mod 5])​
}​

There are three states of philosopher : THINKING, HUNGRY and EATING. Here there are two semaphores : Mutex and a semaphore array for the philosophers. Mutex is used such that no two philosophers may access
the pickup or putdown at the same time. The array is used to control the behavior of each philosopher. But, semaphores can result in deadlock due to programming errors.
Code –
filter_none
edit
close
play_arrow
link
brightness_4
code
#include <pthread.h>
#include <semaphore.h>

#include <stdio.h>
#define
#define
#define
#define
#define
#define

N 5
THINKING 2
HUNGRY 1
EATING 0
LEFT (phnum + 4) % N
RIGHT (phnum + 1) % N

int state[N];
int phil[N] = { 0, 1, 2, 3, 4 };
sem_t mutex;
sem_t S[N];
void test(int phnum)
{
if (state[phnum] == HUNGRY
&& state[LEFT] != EATING
&& state[RIGHT] != EATING) {
// state that eating
state[phnum] = EATING;
sleep(2);
printf("Philosopher %d takes fork %d and %d\n",
phnum + 1, LEFT + 1, phnum + 1);
printf("Philosopher %d is Eating\n", phnum + 1);
// sem_post(&S[phnum]) has no effect
// during takefork
// used to wake up hungry philosophers
// during putfork
sem_post(&S[phnum]);
}
}
// take up chopsticks
void take_fork(int phnum)
{
sem_wait(&mutex);
// state that hungry
state[phnum] = HUNGRY;
printf("Philosopher %d is Hungry\n", phnum + 1);
// eat if neighbours are not eating
test(phnum);
sem_post(&mutex);
// if unable to eat wait to be signalled
sem_wait(&S[phnum]);
sleep(1);
}
// put down chopsticks
void put_fork(int phnum)
{
sem_wait(&mutex);
// state that thinking
state[phnum] = THINKING;
printf("Philosopher %d putting fork %d and %d down\n",
phnum + 1, LEFT + 1, phnum + 1);
printf("Philosopher %d is thinking\n", phnum + 1);
test(LEFT);
test(RIGHT);
sem_post(&mutex);
}
void* philospher(void* num)
{
while (1) {
int* i = num;
sleep(1);
take_fork(*i);
sleep(0);
put_fork(*i);
}
}
int main()
{
int i;
pthread_t thread_id[N];
// initialize the semaphores
sem_init(&mutex, 0, 1);
for (i = 0; i < N; i++)
sem_init(&S[i], 0, 0);
for (i = 0; i < N; i++) {
// create philosopher processes
pthread_create(&thread_id[i], NULL,
philospher, &phil[i]);
printf("Philosopher %d is thinking\n", i + 1);
}
for (i = 0; i < N; i++)
pthread_join(thread_id[i], NULL);
}

chevron_right
filter_none
Note – The below program may compile only with C compilers with semaphore and pthread library.
References –
Solution of Dining Philosophers – cs.gordon.edu
Solution of Dining Philosophers – cs.indiana.edu

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/dining-philosopher-problem-using-semaphores/
✍
Write a Testimonial

Optimal Page Replacement Algorithm
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Optimal Page Replacement Algorithm - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite: Page Replacement Algorithms
In operating systems, whenever a new page is referred and not present in memory, page fault occurs and Operating System replaces one of the existing pages with newly needed page. Different page replacement algorithms
suggest different ways to decide which page to replace. The target for all algorithms is to reduce number of page faults.
In this algorithm, OS replaces the page that will not be used for the longest period of time in future.
Examples :

​
Input : Number of frames, fn = 3​
Reference String, pg[] = {7, 0, 1, 2,​
0, 3, 0, 4, 2, 3, 0, 3, 2, 1,​
2, 0, 1, 7, 0, 1};​
Output : No. of hits = 11 ​
No. of misses = 9​
​
Input : Number of frames, fn = 4 ​
Reference String, pg[] = {7, 0, 1, 2, ​
0, 3, 0, 4, 2, 3, 0, 3, 2};​
Output : No. of hits = 7​
No. of misses = 6​

The idea is simple, for every reference we do following :
1. If referred page is already present, increment hit count.
2. If not present, find if a page that is never referenced in future. If such a page exists, replace this page with new page. If no such page exists, find a page that is referenced farthest in future. Replace this page with new
page.

filter_none
edit
close
play_arrow
link
brightness_4
code
// CPP program to demonstrate optimal page
// replacement algorithm.
#include <bits/stdc++.h>
using namespace std;
// Function to check whether a page exists
// in a frame or not
bool search(int key, vector<int>& fr)
{
for (int i = 0; i < fr.size(); i++)
if (fr[i] == key)
return true;
return false;
}
// Function to find the frame that will not be used
// recently in future after given index in pg[0..pn-1]
int predict(int pg[], vector<int>& fr, int pn, int index)
{
// Store the index of pages which are going
// to be used recently in future
int res = -1, farthest = index;
for (int i = 0; i < fr.size(); i++) {
int j;
for (j = index; j < pn; j++) {

if (fr[i] == pg[j]) {
if (j > farthest) {
farthest = j;
res = i;
}
break;
}
}
// If a page is never referenced in future,
// return it.
if (j == pn)
return i;
}
// If all of the frames were not in future,
// return any of them, we return 0. Otherwise
// we return res.
return (res == -1) ? 0 : res;
}
void optimalPage(int pg[], int pn, int fn)
{
// Create an array for given number of
// frames and initialize it as empty.
vector<int> fr;
// Traverse through page reference array
// and check for miss and hit.
int hit = 0;
for (int i = 0; i < pn; i++) {
// Page found in a frame : HIT
if (search(pg[i], fr)) {
hit++;
continue;
}
// Page not found in a frame : MISS
// If there is space available in frames.
if (fr.size() < fn)
fr.push_back(pg[i]);
// Find the page to be replaced.
else {
int j = predict(pg, fr, pn, i + 1);
fr[j] = pg[i];
}
}
cout << "No. of hits = " << hit << endl;
cout << "No. of misses = " << pn - hit << endl;
}
// Driver Function
int main()
{
int pg[] = { 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2 };
int pn = sizeof(pg) / sizeof(pg[0]);
int fn = 4;
optimalPage(pg, pn, fn);
return 0;
}

chevron_right
filter_none
Output:
No. of hits = 7​
No. of misses = 6

The above implementation can optimized using hashing. We can use an unordered_set in place of vector so that search operation can be done in O(1) time.
Note that optimal page replacement algorithm is not practical as we cannot predict future. However it is used as a reference for other page replacement algorithms.

Please Improve this article if you find anything incorrect by clicking on the "Improve Article" button below.

Source
https://www.geeksforgeeks.org/optimal-page-replacement-algorithm/
✍
Write a Testimonial

Priority CPU Scheduling with different arrival time – Set 2
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Priority CPU Scheduling with different arrival time - Set 2 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Program for Priority Scheduling – Set 1
Priority scheduling is a non-preemptive algorithm and one of the most common scheduling algorithms in batch systems. Each process is assigned first arrival time (less arrival time process first) if two processes have same
arrival time, then compare to priorities (highest process first). Also, if two processes have same priority then compare to process number (less process number first). This process is repeated while all process get executed.
Implementation –

1. First input the processes with their arrival time, burst time and priority.
2. Sort the processes, according to arrival time if two process arrival time is same then sort according process priority if two process priority are same then sort according to process number.
3. Now simply apply FCFS algorithm.

Gantt Chart –

Examples –
​
Input :​
process no-> 1 2 3 4 5 ​
arrival time-> 0 1 3 2 4​
burst time-> 3 6 1 2 4​
priority-> 3 4 9 7 8​
Output :​
Process_no Start_time Complete_time
1
0
3
2
3
9
4
9
11
3
11
12
5
12
16
Average Wating Time is : 5.0​
Average Trun Around time is : 8.2​
​

Trun_Around_Time Wating_Time​
3
0​
8
2​
9
7​
9
8​
12
8​

C++
filter_none
edit
close
play_arrow
link
brightness_4
code
// C++ implementation for Priority Scheduling with
//Different Arrival Time priority scheduling
/*1. sort the processes according to arrival time
2. if arrival time is same the acc to priority
3. apply fcfs
*/
#include <bits/stdc++.h>
using namespace std;
#define totalprocess 5
// Making a struct to hold the given input
struct process
{
int at,bt,pr,pno;
};
process proc[50];
/*
Writing comparator function to sort according to priority if
arrival time is same
*/
bool comp(process a,process b)
{
if(a.at == b.at)
{
return a.pr<b.pr;
}
else
{
return a.at<b.at;
}
}
// Using FCFS Algorithm to find Waiting time
void get_wt_time(int wt[])
{
// declaring service array that stores cumulative burst time
int service[50];
// Initilising initial elements of the arrays
service[0]=0;
wt[0]=0;
for(int i=1;i<totalprocess;i++)
{
service[i]=proc[i-1].bt+service[i-1];
wt[i]=service[i]-proc[i].at+1;
// If waiting time is negative, change it into zero
if(wt[i]<0)
{
wt[i]=0;
}
}
}
void get_tat_time(int tat[],int wt[])
{
// Filling turnaroundtime array
for(int i=0;i<totalprocess;i++)
{
tat[i]=proc[i].bt+wt[i];
}
}
void findgc()
{
//Declare waiting time and turnaround time array
int wt[50],tat[50];
double wavg=0,tavg=0;
// Function call to find waiting time array
get_wt_time(wt);
//Function call to find turnaround time
get_tat_time(tat,wt);
int stime[50],ctime[50];
stime[0]=1;
ctime[0]=stime[0]+tat[0];
// calculating starting and ending time
for(int i=1;i<totalprocess;i++)
{
stime[i]=ctime[i-1];

ctime[i]=stime[i]+tat[i]-wt[i];
}
cout<<"Process_no\tStart_time\tComplete_time\tTurn_Around_Time\tWaiting_Time"<<endl;
// display the process details
for(int i=0;i<totalprocess;i++)
{
wavg += wt[i];
tavg += tat[i];
cout<<proc[i].pno<<"\t\t"<<
stime[i]<<"\t\t"<<ctime[i]<<"\t\t"<<
tat[i]<<"\t\t\t"<<wt[i]<<endl;
}
// display the average waiting time
//and average turn around time
cout<<"Average waiting time is : ";
cout<<wavg/(float)totalprocess<<endl;
cout<<"average turnaround time : ";
cout<<tavg/(float)totalprocess<<endl;
}
int
{
int
int
int

main()
arrivaltime[] = { 1, 2, 3, 4, 5 };
bursttime[] = { 3, 5, 1, 7, 4 };
priority[] = { 3, 4, 1, 7, 8 };

for(int i=0;i<totalprocess;i++)
{
proc[i].at=arrivaltime[i];
proc[i].bt=bursttime[i];
proc[i].pr=priority[i];
proc[i].pno=i+1;
}
//Using inbuilt sort function
sort(proc,proc+totalprocess,comp);
//Calling function findgc for finding Gantt Chart
findgc();
return 0;
}
// This code is contributed by Anukul Chand.

chevron_right
filter_none
Java
filter_none
edit
close
play_arrow
link
brightness_4
code
// Java implementation for Priority Scheduling with
//Different Arrival Time priority scheduling
import java.util.*;
/// Data Structure
class Process {
int at, bt, pri, pno;
Process(int pno, int at, int bt, int pri)
{
this.pno = pno;
this.pri = pri;
this.at = at;
this.bt = bt;
}
}
/// Gantt chart structure
class GChart {
// process number, start time, complete time,
// turn around time, waiting time
int pno, stime, ctime, wtime, ttime;
}
// user define comparative method (first arrival first serve,
// if arrival time same then heigh priority first)
class MyComparator implements Comparator {
public int compare(Object o1, Object o2)
{
Process p1 = (Process)o1;
Process p2 = (Process)o2;
if (p1.at < p2.at)
return (-1);
else if (p1.at == p2.at && p1.pri > p2.pri)
return (-1);
else
return (1);
}
}
// class to find Gantt chart
class FindGantChart {
void findGc(LinkedList queue)
{
// initial time = 0
int time = 0;
// priority Queue sort data according
// to arrival time or priority (ready queue)
TreeSet prique = new TreeSet(new MyComparator());
// link list for store processes data
LinkedList result = new LinkedList();
// process in ready queue from new state queue
while (queue.size() > 0)
prique.add((Process)queue.removeFirst());
Iterator it = prique.iterator();

// time set to according to first process
time = ((Process)prique.first()).at;
// scheduling process
while (it.hasNext()) {
// dispatcher dispatch the
// process ready to running state
Process obj = (Process)it.next();
GChart gc1 = new GChart();
gc1.pno = obj.pno;
gc1.stime = time;
time += obj.bt;
gc1.ctime = time;
gc1.ttime = gc1.ctime - obj.at;
gc1.wtime = gc1.ttime - obj.bt;
/// store the exxtreted process
result.add(gc1);
}
// create object of output class and call method
new ResultOutput(result);
}
}

chevron_right
filter_none
Python3
filter_none
edit
close
play_arrow
link
brightness_4
code
# Python3 implementation for Priority Scheduling with
# Different Arrival Time priority scheduling
"""1. sort the processes according to arrival time
2. if arrival time is same the acc to priority
3. apply fcfs """
totalprocess = 5
proc = []
for i in range(5):
l = []
for j in range(4):
l.append(0)
proc.append(l)
# Using FCFS Algorithm to find Waiting time
def get_wt_time( wt):
# declaring service array that stores
# cumulative burst time
service = [0] * 5
# Initilising initial elements
# of the arrays
service[0] = 0
wt[0] = 0
for i in range(1, totalprocess):
service[i] = proc[i - 1][1] + service[i - 1]
wt[i] = service[i] - proc[i][0] + 1
# If waiting time is negative,
# change it o zero
if(wt[i] < 0) :
wt[i] = 0
def get_tat_time(tat, wt):
# Filling turnaroundtime array
for i in range(totalprocess):
tat[i] = proc[i][1] + wt[i]
def findgc():
# Declare waiting time and
# turnaround time array
wt = [0] * 5
tat = [0] * 5
wavg = 0
tavg = 0
# Function call to find waiting time array
get_wt_time(wt)
# Function call to find turnaround time
get_tat_time(tat, wt)
stime = [0] * 5
ctime = [0] * 5
stime[0] = 1
ctime[0] = stime[0] + tat[0]
# calculating starting and ending time
for i in range(1, totalprocess):
stime[i] = ctime[i - 1]
ctime[i] = stime[i] + tat[i] - wt[i]
print("Process_no\tStart_time\tComplete_time",
"\tTurn_Around_Time\tWaiting_Time")
# display the process details
for i in range(totalprocess):
wavg += wt[i]
tavg += tat[i]
print(proc[i][3], "\t\t", stime[i],
"\t\t", end = " ")
print(ctime[i], "\t\t", tat[i], "\t\t\t", wt[i])
# display the average waiting time
# and average turn around time
print("Average waiting time is : ", end = " ")
print(wavg / totalprocess)
print("average turnaround time : " , end = " ")
print(tavg / totalprocess)
# Driver code
if __name__ =="__main__":
arrivaltime = [1, 2, 3, 4, 5]
bursttime = [3, 5, 1, 7, 4]
priority = [3, 4, 1, 7, 8]

for i in range(totalprocess):
proc[i][0]
proc[i][1]
proc[i][2]
proc[i][3]

=
=
=
=

arrivaltime[i]
bursttime[i]
priority[i]
i + 1

# Using inbuilt sort function
proc = sorted (proc, key = lambda x:x[2])
proc = sorted (proc)
# Calling function findgc for
# finding Gantt Chart
findgc()
# This code is contributed by
# Shubham Singh(SHUBHAMSINGH10)

chevron_right
filter_none
Output:
​
Process_no Start_time Complete_time Trun_Around_Time Wating_Time​
1
1
4
3
0​
2
4
9
7
2​
3
9
10
7
6​
4
10
17
13
6​
5
17
21
16
12​
Average Wating Time is : 5.2​
Average Trun Around time is : 9.2​

Improved By : Cyberfreak, SHUBHAMSINGH10

Source
https://www.geeksforgeeks.org/priority-cpu-scheduling-with-different-arrival-time-set-2/
✍
Write a Testimonial

Lock Variable Synchronization Mechanism
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Lock Variable Synchronization Mechanism - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisites – Process Synchronization
A lock variable provides the simplest synchronization mechanism for processes. Some noteworthy points regarding Lock Variables are1. Its a software mechanism implemented in user mode, i.e. no support required from the Operating System.
2. Its a busy waiting solution (keeps the CPU busy even when its technically waiting).
3. It can be used for more than two processes.
When Lock = 0 implies critical section is vacant (initial value ) and Lock = 1 implies critical section occupied.

The pseudocode looks something like this –
​
Entry section - while(lock != 0);​
Lock = 1;​
//critical section​
Exit section - Lock = 0;​

A more formal approach to the Lock Variable method for process synchronization can be seen in the following code snippet :
filter_none
edit
close
play_arrow
link
brightness_4
code
char buffer[SIZE];
int count = 0,
start = 0,
end = 0;
struct lock l;
// initialize lock variable

lock_init(&l);
void put(char c)
{
// entry section
lock_acquire(&l);
// critical section begins
while (count == SIZE) {
lock_release(&l);
lock_acquire(&l);
}
count++;
buffer[start] = c;
start++;
if (start == SIZE) {
start = 0;
}
// critical section ends
// exit section
lock_release(&l);
}
char get()
{
char c;
// entry section
lock_acquire(&l);
// critical section begins
while (count == 0) {
lock_release(&l);
lock_acquire(&l);
}
count--;
c = buffer[end];
end++;
if (end == SIZE) {
end = 0;
}
// critical section ends
// exit section
lock_release(&l);
return c;
}

chevron_right
filter_none
Here we can see a classic implementation of the reader-writer’s problem. The buffer here is the shared memory and many processes are either trying to read or write a character to it. To prevent any ambiguity of data we
restrict concurrent access by using a lock variable. We have also applied a constraint on the number of readers/writers that can have access.
Now every Synchronization mechanism is judged on the basis of three primary parameters :
1. Mutual Exclusion.
2. Progress.
3. Bounded Waiting.
Of which mutual exclusion is the most important of all parameters. The Lock Variable doesn’t provide mutual exclusion in some cases. This fact can be best verified by writing its pseudo-code in the form of an assembly
language code as given below.
​
1. Load Lock, R0 ; (Store the value of Lock in Register R0.)​
2. CMP R0, #0 ; (Compare the value of register R0 with 0.)​
3. JNZ Step 1 ; (Jump to step 1 if value of R0 is not 0.)​
4. Store #1, Lock ; (Set new value of Lock as 1.)​
Enter critical section​
5. Store #0, Lock ; (Set the value of lock as 0 again.)​

Now let’s suppose that processes P1 and P2 are competing for Critical Section and their sequence of execution be as follows (initial alue of Lock = 0) –
1. P1 executes statement 1 and gets pre-empted.
2. P2 executes statement 1, 2, 3, 4 and enters Critical Section and gets pre-empted.
3. P1 executes statement 2, 3, 4 and also enters Critical Section.
Here initially the R0 of process P1 stores lock value as 0 but fails to update the lock value as 1. So when P2 executes it also finds the LOCK value as 0 and enters Critical Section by setting LOCK value as 1. But the real
problem arises when P1 executes again it doesn’t check the updated value of Lock. It only checks the previous value stored in R0 which was 0 and it enters critical section.
This is only one possible sequence of execution among many others. Some may even provide mutual exclusion but we cannot dwell on that. According to murphy’s law “Anything that can go wrong will go wrong “. So
like all easy things the Lock Variable Synchronization method comes with its fair share of Demerits but its a good starting point for us to develop better Synchronization Algorithms to take care of the problems that we
face here.

Source
https://www.geeksforgeeks.org/lock-variable-synchronization-mechanism/
✍
Write a Testimonial

Page Table Entries in Page Table

​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Page Table Entries in Page Table - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – Paging
Page table has page table entries where each page table entry stores a frame number and optional status (like protection) bits. Many of status bits used in the virtual memory system. The mostimportant thing in PTE is
frame Number.
Page table entry has the following information –

1. Frame Number – It gives the frame number in which the current page you are looking for is present. The number of bits required depends on the number of frames.Frame bit is also known as address translation bit.
​
Number of bits for frame = Size of physical memory/frame size​

2. Present/Absent bit – Present or absent bit says whether a particular page you are looking for is present or absent. In case if it is not present, that is called Page Fault. It is set to 0 if the corresponding page is not in
memory. Used to control page fault by the operating system to support virtual memory. Sometimes this bit is also known as valid/invalid bits.
3. Protection bit – Protection bit says that what kind of protection you want on that page. So, these bit for the protection of the page frame (read, write etc).
4. Referenced bit – Referenced bit will say whether this page has been referred in the last clock cycle or not. It is set to 1 by hardware when the page is accessed.
5. Caching enabled/disabled – Some times we need the fresh data. Let us say the user is typing some information from the keyboard and your program should run according to the input given by the user. In that case,
the information will come into the main memory. Therefore main memory contains the latest information which is typed by the user. Now if you try to put that page in the cache, that cache will show the old
information. So whenever freshness is required, we don’t want to go for caching or many levels of the memory.The information present in the closest level to the CPU and the information present in the closest level
to the user might be different. So we want the information has to be consistency, which means whatever information user has given, CPU should be able to see it as first as possible. That is the reason we want to
disable caching. So, this bit enables or disable caching of the page.
6. Modified bit – Modified bit says whether the page has been modified or not. Modified means sometimes you might try to write something on to the page. If a page is modified, then whenever you should replace
that page with some other page, then the modified information should be kept on the hard disk or it has to be written back or it has to be saved back. It is set to 1 by hardware on write-access to page which is used to
avoid writing when swapped out. Sometimes this modified bit is also called as the Dirty bit.
GATE CS Corner Questions
Practicing the following questions will help you test your knowledge. All questions have been asked in GATE in previous years or in GATE Mock Tests. It is highly recommended that you practice them.
1. GATE CS 2001, Question 46
2. GATE CS 2004, Question 66
3. GATE CS 2015 (Set 1), Question 65

Improved By : VaibhavRai3

Source
https://www.geeksforgeeks.org/page-table-entries-in-page-table/
✍
Write a Testimonial

Overlays in Memory Management
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Overlays in Memory Management - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu

The main problem in Fixed partitioning is the size of a process has to be limited by the maximum size of the partition, which means a process can never be span over another.In order to solve this problem, earlier people
have used some solution which is called as Overlays.
The concept of overlays is that whenever a process is running it will not use the complete program at the same time, it will use only some part of it.Then overlays concept says that whatever part you required, you load it
an once the part is done, then you just unload it, means just pull it back and get the new part you required and run it.
Formally,
“The process of transferring a block of program code or other data into internal memory, replacing what is already stored”.
Sometimes it happens that compare to the size of the biggest partition, the size of the program will be even more, then, in that case, you should go with overlays.
So overlay is a technique to run a program that is bigger than the size of the physical memory by keeping only those instructions and data that are needed at any given time.Divide the program into modules in such a way
that not all modules need to be in the memory at the same time.

Advantage –
Reduce memory requirement
Reduce time requirement
Disadvantage –
Overlap map must be specified by programmer
Programmer must know memory requirement
Overlaped module must be completely disjoint
Programmming design of overlays structure is complex and not possible in all cases
Example –
The best example of overlays is assembler.Consider the assembler has 2 passes, 2 pass means at any time it will be doing only one thing, either the 1st pass or the 2nd pass.Which means it will finish 1st pass first and then
2nd pass.Let assume that available main memory size is 150KB and total code size is 200KB
​
Pass 1.......................70KB​
Pass 2.......................80KB​
Symbol table.................30KB​
Common routine...............20KB​

As the total code size is 200KB and main memory size is 150KB, it is not possible to use 2 passes together.So, in this case, we should go with the overlays technique.According to the overlays concept at any time only one
pass will be used and both the passes always need symbol table and common routine.Now the question is if overlays-driver* is 10KB, then what is the minimum partition size required?For pass 1 total memory needed is =
(70KB + 30KB + 20KB + 10KB) = 130KB and for pass 2 total memory needed is = (80KB + 30KB + 20KB + 10KB) = 140KB.So if we have minimum 140KB size partition then we can run this code very easily.
*Overlays driver:-It is the user responsibility to take care of overlaying, the operating system will not provide anything.Which means the user should write even what part is required in the 1st pass and once the 1st pass is
over, the user should write the code to pull out the pass 1 and load the pass 2.That is what is the responsibility of the user, that is known as the Overlays driver.Overlays driver will just help us to move out and move in the
various part of the code.
Question –
The overlay tree for a program is as shown below:

What will be the size of the partition (in physical memory) required to load (and
run) this program?
(a) 12 KB (b) 14 KB (c) 10 KB (d) 8 KB
Explanation –
Using the overlay concept we need not actually have the entire program inside the main memory.Only we need to have the part which are required at that instance of time, either we need Root-A-D or Root-A-E or Root-BF or Root-C-G part.
​
Root+A+D
Root+A+E
Root+B+F
Root+C+G

=
=
=
=

2KB
2KB
2KB
2KB

+
+
+
+

4KB
4KB
6KB
8KB

+
+
+
+

6KB
8KB
2KB
4KB

=
=
=
=

12KB​
14KB​
10KB​
14KB​

So if we have 14KB size of partition then we can run any of them.
Answer -(b) 14KB

Source
https://www.geeksforgeeks.org/overlays-in-memory-management/
✍
Write a Testimonial

Unix File System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Unix File System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Unix file system is a logical method of organizing and storing large amounts of information in a way that makes it easy to manage. A file is a smallest unit in which the information is stored. Unix file system has several
important features. All data in Unix is organized into files. All files are organized into directories. These directories are organized into a tree-like structure called the file system.
Files in Unix System are organized into multi-level hierarchy structure known as a directory tree. At the very top of the file system is a directory called “root” which is represented by a “/”. All other files are “descendants”
of root.

Directories or Files and their description –
/ : The slash / character alone denotes the root of the filesystem tree.
/bin : Stands for “binaries” and contains certain fundamental utilities, such as ls or cp, which are generally needed by all users.
/boot : Contains all the files that are required for successful booting process.
/dev : Stands for “devices”. Contains file representations of peripheral devices and pseudo-devices.
/etc : Contains system-wide configuration files and system databases. Originally also contained “dangerous maintenance utilities” such as init,but these have typically been moved to /sbin or elsewhere.
/home : Contains the home directories for the users.
/lib : Contains system libraries, and some critical files such as kernel modules or device drivers.
/media : Default mount point for removable devices, such as USB sticks, media players, etc.
/mnt : Stands for “mount”. Contains filesystem mount points. These are used, for example, if the system uses multiple hard disks or hard disk partitions. It is also often used for remote (network) filesystems, CDROM/DVD drives, and so on.
/proc : procfs virtual filesystem showing information about processes as files.
/root : The home directory for the superuser “root” – that is, the system administrator. This account’s home directory is usually on the initial filesystem, and hence not in /home (which may be a mount point for
another filesystem) in case specific maintenance needs to be performed, during which other filesystems are not available. Such a case could occur, for example, if a hard disk drive suffers physical failures and
cannot be properly mounted.
/tmp : A place for temporary files. Many systems clear this directory upon startup; it might have tmpfs mounted atop it, in which case its contents do not survive a reboot, or it might be explicitly cleared by a startup
script at boot time.
/usr : Originally the directory holding user home directories,its use has changed. It now holds executables, libraries, and shared resources that are not system critical, like the X Window System, KDE, Perl, etc.
However, on some Unix systems, some user accounts may still have a home directory that is a direct subdirectory of /usr, such as the default as in Minix. (on modern systems, these user accounts are often related to
server or system use, and not directly used by a person).
/usr/bin : This directory stores all binary programs distributed with the operating system not residing in /bin, /sbin or (rarely) /etc.
/usr/include : Stores the development headers used throughout the system. Header files are mostly used by the #include directive in C/C++ programming language.
/usr/lib : Stores the required libraries and data files for programs stored within /usr or elsewhere.
/var : A short for “variable.” A place for files that may change often – especially in size, for example e-mail sent to users on the system, or process-ID lock files.
/var/log : Contains system log files.
/var/mail : The place where all the incoming mails are stored. Users (other than root) can access their own mail only. Often, this directory is a symbolic link to /var/spool/mail.
/var/spool : Spool directory. Contains print jobs, mail spools and other queued tasks.
/var/tmp : A place for temporary files which should be preserved between system reboots.
Types of Unix files – The UNIX files system contains several different types of files :

1. Ordinary files – An ordinary file is a file on the system that contains data, text, or program instructions.
Used to store your information, such as some text you have written or an image you have drawn. This is the type of file that you usually work with.
Always located within/under a directory file.
Do not contain other files.
In long-format output of ls -l, this type of file is specified by the “-” symbol.
2. Directories – Directories store both special and ordinary files. For users familiar with Windows or Mac OS, UNIX directories are equivalent to folders. A directory file contains an entry for every file and subdirectory
that it houses. If you have 10 files in a directory, there will be 10 entries in the directory. Each entry has two components.
(1) The Filename
(2) A unique identification number for the file or directory (called the inode number)
Branching points in the hierarchical tree.
Used to organize groups of files.
May contain ordinary files, special files or other directories.
Never contain “real” information which you would work with (such as text). Basically, just used for organizing files.
All files are descendants of the root directory, ( named / ) located at the top of the tree.
In long-format output of ls –l , this type of file is specified by the “d” symbol.
3. Special Files – Used to represent a real physical device such as a printer, tape drive or terminal, used for Input/Ouput (I/O) operations. Device or special files are used for device Input/Output(I/O) on UNIX and Linux
systems. They appear in a file system just like an ordinary file or a directory.
On UNIX systems there are two flavors of special files for each device, character special files and block special files :
When a character special file is used for device Input/Output(I/O), data is transferred one character at a time. This type of access is called raw device access.

When a block special file is used for device Input/Output(I/O), data is transferred in large fixed-size blocks. This type of access is called block device access.
For terminal devices, it’s one character at a time. For disk devices though, raw access means reading or writing in whole chunks of data – blocks, which are native to your disk.
In long-format output of ls -l, character special files are marked by the “c” symbol.
In long-format output of ls -l, block special files are marked by the “b” symbol.
4. Pipes – UNIX allows you to link commands together using a pipe. The pipe acts a temporary file which only exists to hold data from one command until it is read by another.A Unix pipe provides a one-way flow of
data.The output or result of the first command sequence is used as the input to the second command sequence. To make a pipe, put a vertical bar (|) on the command line between two commands.For example: who | wc -l
In long-format output of ls –l , named pipes are marked by the “p” symbol.

5. Sockets – A Unix socket (or Inter-process communication socket) is a special file which allows for advanced inter-process communication. A Unix Socket is used in a client-server application framework. In essence, it
is a stream of data, very similar to network stream (and network sockets), but all the transactions are local to the filesystem.
In long-format output of ls -l, Unix sockets are marked by “s” symbol.
6. Symbolic Link – Symbolic link is used for referencing some other file of the file system.Symbolic link is also known as Soft link. It contains a text form of the path to the file it references. To an end user, symbolic link
will appear to have its own name, but when you try reading or writing data to this file, it will instead reference these operations to the file it points to. If we delete the soft link itself , the data file would still be there.If we
delete the source file or move it to a different location, symbolic file will not function properly.
In long-format output of ls –l , Symbolic link are marked by the “l” symbol (that’s a lower case L).
Reference –
UNIX – Concepts and Applications | Sumitabha Das |Tata McGraw Hill |4th Edition

Improved By : OmkarManjrekar

Source
https://www.geeksforgeeks.org/unix-file-system/
✍
Write a Testimonial

Page Fault Handling in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Page Fault Handling in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
A page fault occurs when a program attempts to access data or code that is in its address space, but is not currently located in the system RAM. So when page fault occurs then following sequence of events happens :

The computer hardware traps to the kernel and program counter (PC) is saved on the stack. Current instruction state information is saved in CPU registers.
An assembly program is started to save the general registers and other volatile information to keep the OS from destroying it.
Operating system finds that a page fault has occurred and tries to find out which virtual page is needed. Some times hardware register contains this required information. If not, the operating system must retrieve PC,
fetch instruction and find out what it was doing when the fault occurred.
Once virtual address caused page fault is known, system checks to see if address is valid and checks if there is no protection access problem.
If the virtual address is valid, the system checks to see if a page frame is free. If no frames are free, the page replacement algorithm is run to remove a page.
If frame selected is dirty, page is scheduled for transfer to disk, context switch takes place, fault process is suspended and another process is made to run until disk transfer is completed.

As soon as page frame is clean, operating system looks up disk address where needed page is, schedules disk operation to bring it in.
When disk interrupt indicates page has arrived, page tables are updated to reflect its position, and frame marked as being in normal state.
Faulting instruction is backed up to state it had when it began and PC is reset. Faulting is scheduled, operating system returns to routine that called it.
Assembly Routine reloads register and other state information, returns to user space to continue execution.
References –
cs.uttyler.edu
professormerwyn.wordpress.com

Source
https://www.geeksforgeeks.org/page-fault-handling-in-operating-system/
✍
Write a Testimonial

Resource Allocation Graph (RAG) in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Resource Allocation Graph (RAG) in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
As Banker’s algorithm using some kind of table like allocation, request, available all that thing to understand what is the state of the system. Similarly, if you want to understand the state of the system instead of using
those table, actually tables are very easy to represent and understand it, but then still you could even represent the same information in the graph. That graph is called Resource Allocation Graph (RAG).
So, resource allocation graph is explained to us what is the state of the system in terms of processes and resources. Like how many resources are available, how many are allocated and what is the request of each process.
Everything can be represented in terms of the diagram. One of the advantages of having a diagram is, sometimes it is possible to see a deadlock directly by using RAG, but then you might not be able to know that by
looking at the table. But the tables are better if the system contains lots of process and resource and Graph is better if the system contains less number of process and resource.
We know that any graph contains vertices and edges. So RAG also contains vertices and edges. In RAG vertices are two type –
1. Process vertex – Every process will be represented as a process vertex.Generally, the process will be represented with a circle.
2. Resource vertex – Every resource will be represented as a resource vertex. It is also two type –

Single instance type resource – It represents as a box, inside the box, there will be one dot.So the number of dots indicate how many instances are present of each resource type.
Multi-resource instance type resource – It also represents as a box, inside the box, there will be many dots present.

Now coming to the edges of RAG.There are two types of edges in RAG –

1. Assign Edge – If you already assign a resource to a process then it is called Assign edge.
2. Request Edge – It means in future the process might want some resource to complete the execution, that is called request edge.

So, if a process is using a resource, an arrow is drawn from the resource node to the process node. If a process is requesting a resource, an arrow is drawn from the process node to the resource node.
Example 1 (Single instances RAG) –

If there is a cycle in the Resource Allocation Graph and each resource in the cycle provides only one instance, then the processes will be in deadlock. For example, if process P1 holds resource R1, process P2 holds
resource R2 and process P1 is waiting for R2 and process P2 is waiting for R1, then process P1 and process P2 will be in deadlock.

Here’s another example, that shows Processes P1 and P2 acquiring resources R1 and R2 while process P3 is waiting to acquire both resources. In this example, there is no deadlock because there is no circular dependency.
So cycle in single-instance resource type is the sufficient condition for deadlock.
Example 2 (Multi-instances RAG) –

From the above example, it is not possible to say the RAG is in a safe state or in an unsafe state.So to see the state of this RAG, let’s construct the allocation matrix and request matrix.

The total number of processes are three; P1, P2 & P3 and the total number of resources are two; R1 & R2.
Allocation matrix –
For constructing the allocation matrix, just go to the resources and see to which process it is allocated.
R1 is allocated to P1, therefore write 1 in allocation matrix and similarly, R2 is allocated to P2 as well as P3 and for the remaining element just write 0.
Request matrix –
In order to find out the request matrix, you have to go to the process and see the outgoing edges.
P1 is requesting resource R2, so write 1 in the matrix and similarly, P2 requesting R1 and for the remaining element write 0.
So now available resource is = (0, 0).
Checking deadlock (safe or not) –

So, there is no deadlock in this RAG.Even though there is a cycle, still there is no deadlock.Therefore in multi-instance resource cycle is not sufficient condition for deadlock.

Above example is the same as the previous example except that, the process P3 requesting for resource R1.
So the table becomes as shown in below.

So,the Available resource is = (0, 0), but requirement are (0, 1), (1, 0) and (1, 0).So you can’t fulfill any one requirement.Therefore, it is in deadlock.
Therefore, every cycle in a multi-instance resource type graph is not a deadlock, if there has to be a deadlock, there has to be a cycle.So, in case of RAG with multi-instance resource type, the cycle is a necessary
condition for deadlock, but not sufficient.
GATE CS Corner Questions
Practicing the following questions will help you test your knowledge. All questions have been asked in GATE in previous years or in GATE Mock Tests. It is highly recommended that you practice them.
1. GATE CS 2009, Question 60
2. GATE CS 2014 (Set 1), Question 65
Reference –
A. Silberschatz, P. Galvin, G. Gagne, “Operating Systems Concepts (8th Edition)”, Wiley India Pvt. Ltd.

Source
https://www.geeksforgeeks.org/resource-allocation-graph-rag-in-operating-system/
✍
Write a Testimonial

Lottery Process Scheduling in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Lottery Process Scheduling in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – CPU Scheduling, Process Management
Lottery Scheduling is type of process scheduling, somewhat different from other Scheduling. Processes are scheduled in a random manner. Lottery scheduling can be preemptive or non-preemptive. It also solves the
problem of starvation. Giving each process at least one lottery ticket guarantees that it has non-zero probability of being selected at each scheduling operation.
In this scheduling every process have some tickets and scheduler picks a random ticket and process having that ticket is the winner and it is executed for a time slice and then another ticket is picked by the scheduler.
These tickets represent the share of processes. A process having a higher number of tickets give it more chance to get chosen for execution.
Example – If we have two processes A and B having 60 and 40 tickets respectively out of total 100 tickets. CPU share of A is 60% and that of B is 40%.These shares are calculated probabilistically and not
deterministically.

Explanation –
1. We have two processes A and B. A has 60 tickets (ticket number 1 to 60) and B have 40 tickets (ticket no. 61 to 100).
2. Scheduler picks a random number from 1 to 100. If the picked no. is from 1 to 60 then A is executed otherwise B is executed.
3. An example of 10 tickets picked by Scheduler may look like this –
​
Ticket number - 73 82 23 45 32 87 49 39 12 09.​
Resulting Schedule - B B A A A B A A A A.​

4. A is executed 7 times and B is executed 3 times. As you can see that A takes 70% of CPU and B takes 30% which is not the same as what we need as we need A to have 60% of CPU and B should have 40% of
CPU.This happens because shares are calculated probabilistically but in a long run(i.e when no. of tickets picked is more than 100 or 1000) we can achieve a share percentage of approx. 60 and 40 for A and B
respectively.
Ways to manipulate tickets –
Ticket Currency –
Scheduler give a certain number of tickets to different users in a currency and users can give it to there processes in a different currency. E.g. Two users A and B are given 100 and 200 tickets respectively. User A is
running two process and give 50 tickets to each in A’s own currency. B is running 1 process and gives it all 200 tickets in B’s currency. Now at the time of scheduling tickets of each process are converted into global
currency i.e A’s process will have 50 tickets each and B’s process will have 200 tickets and scheduling is done on this basis.
Transfer Tickets –
A process can pass its tickets to another process.
Ticket inflation –
With this technique a process can temporarily raise or lower the number of tickets it own.
References –
Lottery scheduling – Wikipedia
eecs.berkeley.edu

Improved By : dk619

Source
https://www.geeksforgeeks.org/lottery-process-scheduling-in-operating-system/
✍
Write a Testimonial

Highest Response Ratio Next (HRRN) CPU Scheduling
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Highest Response Ratio Next (HRRN) CPU Scheduling - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – CPU Scheduling
Given n processes with their Arrival times and Burst times, the task is to find average waiting time and average turn around time using HRRN scheduling algorithm.
The name itself states that we need to find the response ratio of all available processes and select the one with the highest Response Ratio. A process once selected will run till completion.
Criteria – Response Ratio
Mode – Non-Preemptive
Response Ratio = (W + S)/S

Here, W is the waiting time of the process so far and S is the Burst time of the process.

Performance of HRRN –
1. Shorter Processes are favoured.
2. Aging without service increases ratio, longer jobs can get past shorter jobs.

Gantt Chart –

Explanation –
At t = 0 we have only one process available, so A gets scheduled.
Similarly at t = 3 we have only one process available, so B gets scheduled.
Now at t = 9 we have 3 processes available, C, D and E. Since, C, D and E were available after 4, 6 and 8 units respectively. Therefore, waiting time for C, D and E are (9 – 4 =)5, (9 – 6 =)3, and (9 – 8 =)1 unit
respectively.
Using the formula given above we calculate the Response Ratios of C, D and E respectively as 2.25, 1.6 and 1.5.
Clearly C has the highest Response Ratio and so it gets scheduled
Next at t = 13 we have 2 jobs available D and E.
Response Ratios of D and E are 2.4 and 3.5 respectively.
So process E is selected next and process D is selected last.
Implementation of HRRN Scheduling –
1.
2.
3.
4.
5.
6.
7.

Input the number of processes, their arrival times and burst times.
Sort them according to their arrival times.
At any given time calculate the response ratios and select the appropriate process to be scheduled.
Calculate the turn around time as completion time – arrival time.
Calculate the waiting time as turn around time – burst time.
Turn around time divided by the burst time gives the normalized turn around time.
Sum up the waiting and turn around times of all processes and divide by the number of processes to get the average waiting and turn around time.

Below is the implementation of above approach:
C++
filter_none
edit
close
play_arrow
link
brightness_4
code
// CPP program for Highest Response Ratio Next (HRRN) Scheduling
#include <bits/stdc++.h>
using namespace std;
// Defining process details
struct process {
char name;
int at, bt, ct, wt, tt;
int completed;
float ntt;
} p[10];
int n;
// Sorting Processes by Arrival Time
void sortByArrival()
{
struct process temp;
int i, j;
// Selection Sort applied
for (i = 0; i < n - 1; i++) {
for (j = i + 1; j < n; j++) {
// Check for lesser arrival time

if (p[i].at > p[j].at) {
// Swap earlier process to front
temp = p[i];
p[i] = p[j];
p[j] = temp;
}
}
}
}
int main()
{
int i, j, t, sum_bt = 0;
char c;
float avgwt = 0, avgtt = 0;
n = 5;
// predefined arrival times
int arriv[] = { 0, 2, 4, 6, 8 };
// predefined burst times
int burst[] = { 3, 6, 4, 5, 2 };
// Initializing the structure variables
for (i = 0, c = 'A'; i < n; i++, c++) {
p[i].name = c;
p[i].at = arriv[i];
p[i].bt = burst[i];
// Variable for Completion status
// Pending = 0
// Completed = 1
p[i].completed = 0;
// Variable for sum of all Burst Times
sum_bt += p[i].bt;
}
// Sorting the structure by arrival times
sortByArrival();
cout << "Name " << " Arrival Time " << "
Burst Time
<< " TurnAround Time " << " Normalized TT" ;
for (t = p[0].at; t < sum_bt;) {

"

// Set lower limit to response ratio
float hrr = -9999;
// Response Ratio Variable
float temp;
// Variable to store next processs selected
int loc;
for (i = 0; i < n; i++) {
// Checking if process has arrived and is Incomplete
if (p[i].at <= t && p[i].completed != 1) {
// Calculating Response Ratio
temp = (p[i].bt + (t - p[i].at)) / p[i].bt;
// Checking for Highest Response Ratio
if (hrr < temp) {
// Storing Response Ratio
hrr = temp;
// Storing Location
loc = i;
}
}
}
// Updating time value
t += p[loc].bt;
// Calculation of waiting time
p[loc].wt = t - p[loc].at - p[loc].bt;
// Calculation of Turn Around Time
p[loc].tt = t - p[loc].at;
// Sum Turn Around Time for average
avgtt += p[loc].tt;
// Calculation of Normalized Turn Around Time
p[loc].ntt = ((float)p[loc].tt / p[loc].bt);
// Updating Completion Status
p[loc].completed = 1;
// Sum Waiting Time for average
avgwt += p[loc].wt;
cout<< "\n" << p[loc].name <<"\t" << p[loc].at;
cout << "\t\t" << p[loc].bt <<"\t\t"<< p[loc].wt;
cout <<"\t\t"<< p[loc].tt <<"\t\t"<< p[loc].ntt;
}
cout << "\nAverage waiting time: " << avgwt / n << endl;
cout <<"Average Turn Around time:"<< avgtt / n;
}
//This code is contributed by shivi_Aggarwal

chevron_right
filter_none
C
filter_none
edit
close
play_arrow
link
brightness_4
code
// C program for Highest Response Ratio Next (HRRN) Scheduling
#include <stdio.h>
// Defining process details
struct process {
char name;
int at, bt, ct, wt, tt;
int completed;
float ntt;
} p[10];
int n;
// Sorting Processes by Arrival Time
void sortByArrival()
{
struct process temp;

<<

"

Waiting Time

"

int i, j;
// Selection Sort applied
for (i = 0; i < n - 1; i++) {
for (j = i + 1; j < n; j++) {
// Check for lesser arrival time
if (p[i].at > p[j].at) {
// Swap earlier process to front
temp = p[i];
p[i] = p[j];
p[j] = temp;
}
}
}
}
void main()
{
int i, j, t, sum_bt = 0;
char c;
float avgwt = 0, avgtt = 0;
n = 5;
// predefined arrival times
int arriv[] = { 0, 2, 4, 6, 8 };
// predefined burst times
int burst[] = { 3, 6, 4, 5, 2 };
// Initializing the structure variables
for (i = 0, c = 'A'; i < n; i++, c++) {
p[i].name = c;
p[i].at = arriv[i];
p[i].bt = burst[i];
// Variable for Completion status
// Pending = 0
// Completed = 1
p[i].completed = 0;
// Variable for sum of all Burst Times
sum_bt += p[i].bt;
}
// Sorting the structure by arrival times
sortByArrival();
printf("\nName\tArrival Time\tBurst Time\tWaiting Time");
printf("\tTurnAround Time\t Normalized TT");
for (t = p[0].at; t < sum_bt;) {
// Set lower limit to response ratio
float hrr = -9999;
// Response Ratio Variable
float temp;
// Variable to store next processs selected
int loc;
for (i = 0; i < n; i++) {
// Checking if process has arrived and is Incomplete
if (p[i].at <= t && p[i].completed != 1) {
// Calculating Response Ratio
temp = (p[i].bt + (t - p[i].at)) / p[i].bt;
// Checking for Highest Response Ratio
if (hrr < temp) {
// Storing Response Ratio
hrr = temp;
// Storing Location
loc = i;
}
}
}
// Updating time value
t += p[loc].bt;
// Calculation of waiting time
p[loc].wt = t - p[loc].at - p[loc].bt;
// Calculation of Turn Around Time
p[loc].tt = t - p[loc].at;
// Sum Turn Around Time for average
avgtt += p[loc].tt;
// Calculation of Normalized Turn Around Time
p[loc].ntt = ((float)p[loc].tt / p[loc].bt);
// Updating Completion Status
p[loc].completed = 1;
// Sum Waiting Time for average
avgwt += p[loc].wt;
printf("\n%c\t\t%d\t\t", p[loc].name, p[loc].at);
printf("%d\t\t%d\t\t", p[loc].bt, p[loc].wt);
printf("%d\t\t%f", p[loc].tt, p[loc].ntt);
}
printf("\nAverage waiting time:%f\n", avgwt / n);
printf("Average Turn Around time:%f\n", avgtt / n);
}

chevron_right
filter_none
Output –
​
​
Name
Arrival Time
Burst Time
A
0
3
0
B
2
6
1
C
4
4
5
E
8
2
5
D
6
5
9
Average waiting time:4.000000​
Average Turn Around time:8.000000​
​

3
7
9
7
14

Waiting Time
TurnAround Time
1.000000​
1.166667​
2.250000​
3.500000​
2.800000​

Normalized TT​

Improved By : Shivi_Aggarwal

Source
https://www.geeksforgeeks.org/highest-response-ratio-next-hrrn-cpu-scheduling/
✍
Write a Testimonial

Multilevel Feedback Queue Scheduling (MLFQ) CPU Scheduling
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Multilevel Feedback Queue Scheduling (MLFQ) CPU Scheduling - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – CPU Scheduling, Multilevel Queue Scheduling
This Scheduling is like Multilevel Queue(MLQ) Scheduling but in this process can move between the queues. Multilevel Feedback Queue Scheduling (MLFQ) keep analyzing the behavior (time of execution) of
processes and according to which it changes its priority.Now, look at the diagram and explanation below to understand it properly.

Now let us suppose that queue 1 and 2 follow round robin with time quantum 4 and 8 respectively and queue 3 follow FCFS.One implementation of MFQS is given below –
1. When a process starts executing then it first enters queue 1.
2. In queue 1 process executes for 4 unit and if it completes in this 4 unit or it gives CPU for I/O operation in this 4 unit than the priority of this process does not change and if it again comes in the ready queue than it
again starts its execution in Queue 1.
3. If a process in queue 1 does not complete in 4 unit then its priority gets reduced and it shifted to queue 2.
4. Above points 2 and 3 are also true for queue 2 processes but the time quantum is 8 unit.In a general case if a process does not complete in a time quantum than it is shifted to the lower priority queue.
5. In the last queue, processes are scheduled in FCFS manner.
6. A process in lower priority queue can only execute only when higher priority queues are empty.
7. A process running in the lower priority queue is interrupted by a process arriving in the higher priority queue.
Well, above implementation may differ for example the last queue can also follow Round-robin Scheduling.
Problems in the above implementation – A process in the lower priority queue can suffer from starvation due to some short processes taking all the CPU time.
Solution – A simple solution can be to boost the priority of all the process after regular intervals and place them all in the highest priority queue.

What is the need of such complex Scheduling?
Firstly, it is more flexible than the multilevel queue scheduling.
To optimize turnaround time algorithms like SJF is needed which require the running time of processes to schedule them. But the running time of the process is not known in advance. MFQS runs a process for a time
quantum and then it can change its priority(if it is a long process). Thus it learns from past behavior of the process and then predicts its future behavior.This way it tries to run shorter process first thus optimizing
turnaround time.
MFQS also reduces the response time.
Example –
Consider a system which has a CPU bound process, which requires the burst time of 40 seconds.The multilevel Feed Back Queue scheduling algorithm is used and the queue time quantum ‘2’ seconds and in each level it is
incremented by ‘5’ seconds.Then how many times the process will be interrupted and on which queue the process will terminate the execution?
Solution –
Process P needs 40 Seconds for total execution.
At Queue 1 it is executed for 2 seconds and then interrupted and shifted to queue 2.
At Queue 2 it is executed for 7 seconds and then interrupted and shifted to queue 3.
At Queue 3 it is executed for 12 seconds and then interrupted and shifted to queue 4.
At Queue 4 it is executed for 17 seconds and then interrupted and shifted to queue 5.
At Queue 5 it executes for 2 seconds and then it completes.
Hence the process is interrupted 4 times and completes on queue 5.

Source
https://www.geeksforgeeks.org/multilevel-feedback-queue-scheduling-mlfq-cpu-scheduling/
✍
Write a Testimonial

Shortest Job First CPU Scheduling with predicted burst time
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Shortest Job First CPU Scheduling with predicted burst time - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – CPU Scheduling, SJF – Set 1 (Non- preemptive), Set 2 (Preemptive)
Shortest Job First (SJF) is an optimal scheduling algorithm as it gives maximum Throughput and minimum average waiting time(WT) and turn around time (TAT) but it is not practically implementable because BurstTime of a process can’t be predicted in advance.
We may not know the length of the next CPU burst, but we may be able to predict its value. We expect the next CPU burst will be similar in length to the previous ones. By computing an approximation of the length of the
next CPU burst, we can pick the process with the shortest predicted CPU burst.

There are two methods by which we can predict the burst time of the process :
1. Static method – We can predict the Burst-Time by two factors :
Process size –
Let say we have Process Pold having size 200 KB which is already executed and its Burst-time is 20 Units of time, now lets say we have a New Process Pnew having size 201 KB which is yet to be executed.
We take Burst-Time of already executed process P old which is almost of same size as that of New process as Burst-Time of New Process Pnew.
Process type –
We can predict Burst-Time depending on the Type of Process. Operating System process(like scheduler, dispatcher, segmentation, fragmentation) are faster than User process( Gaming, application softwares ).
Burst-Time for any New O.S process can be predicted from any old O.S process of similar type and same for User process.
Note – Static method for burst time prediction is not reliable as it is always not predicted correctly.
2. Dynamic method – Let ti be the actual Burst-Time of ith process and Τn+1 be the predicted Burst-time for n+1th process.
Simple average – Given n processes ( P1 , P2 … Pn )
Τn+1 = 1/n(Σi=1 to n t i )

Exponential average (Aging) –
Τn+1 = αtn + (1 - α)Τn

where α = is smoothing factor and 0 <= α <= 1 ,
tn = actual burst time of nth process,
Τn = predicted burst time of nth process.
General term,
αt n + (1 - α)αt n-1 + (1 - α)2 αt n-2...+ (1 - α) j αt n-j...+ (1 - α) n+1Τ0

Τ0 is a constant or overall system average.
Smoothening factor (α) – It controls the relative weight of recent and past history in our prediction.
If α = 0, Τn+1 = Τn i.e. no change in value of initial predicted burst time.
If α = 1, Τn+1 = tn i.e. predicted Burst-Time of new process will always change according to actual
If α = 1/2, recent and past history are equally weighted.

Burst-time of nth process.

Example –
Calculate the exponential averaging with T1 = 10, α = 0.5 and the algorithm is SJF with previous runs as 8, 7, 4, 16.
(a) 9
(b) 8
(c) 7.5
(d) None
Explanation :
Initially T1 = 10 and α = 0.5 and the run times given are 8, 7, 4, 16 as it is shortest job first,
So the possible order in which these processes would serve will be 4, 7, 8, 16 since SJF is a non-preemptive technique.
So, using formula: T2 = α*t1 + (1-α)T1
so we have,
T2 = 0.5*4 + 0.5*10 = 7, here t1 = 4 and T1 = 10
T3 = 0.5*7 + 0.5*7 = 7, here t2 = 7 and T2 = 7
T4 = 0.5*8 + 0.5*7 = 7.5, here t3 = 8 and T3 = 7
So the future prediction for 4th process will be T4 = 7.5 which is the option(c).

Improved By : pratikkatariya02

Source
https://www.geeksforgeeks.org/shortest-job-first-cpu-scheduling-with-predicted-burst-time/
✍
Write a Testimonial

Multilevel Queue (MLQ) CPU Scheduling
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Multilevel Queue (MLQ) CPU Scheduling - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite : CPU Scheduling
It may happen that processes in the ready queue can be divided into different classes where each class has its own scheduling needs. For example, a common division is a foreground (interactive) process and
background (batch) processes.These two classes have different scheduling needs. For this kind of situation Multilevel Queue Scheduling is used.Now, let us see how it works.
Ready Queue is divided into separate queues for each class of processes. For example, let us take three different types of process System processes, Interactive processes and Batch Processes. All three process have there
own queue. Now,look at the below figure.

All three different type of processes have there own queue. Each queue have its own Scheduling algorithm. For example, queue 1 and queue 2 uses Round Robin while queue 3 can use FCFS to schedule there processes.
Scheduling among the queues : What will happen if all the queues have some processes? Which process should get the cpu? To determine this Scheduling among the queues is necessary. There are two ways to do so –
1. Fixed priority preemptive scheduling method – Each queue has absolute priority over lower priority queue. Let us consider following priority order queue 1 > queue 2 > queue 3.According to this algorithm no
process in the batch queue(queue 3) can run unless queue 1 and 2 are empty. If any batch process (queue 3) is running and any system (queue 1) or Interactive process(queue 2) entered the ready queue the batch
process is preempted.
2. Time slicing – In this method each queue gets certain portion of CPU time and can use it to schedule its own processes.For instance, queue 1 takes 50 percent of CPU time queue 2 takes 30 percent and queue 3 gets
20 percent of CPU time.
Example Problem :
Consider below table of four processes under Multilevel queue scheduling.Queue number denotes the queue of the process.

Priority of queue 1 is greater than queue 2. queue 1 uses Round Robin (Time Quantum = 2) and queue 2 uses FCFS.
Below is the gantt chart of the problem :
At starting both queues have process so process in queue 1 (P1, P2) runs first (because of higher priority) in the round robin fashion and completes after 7 units then process in queue 2 (P3) starts running (as there is no
process in queue 1) but while it is running P4 comes in queue 1 and interrupts P3 and start running for 5 second and after its completion P3 takes the CPU and completes its execution.

Source
https://www.geeksforgeeks.org/multilevel-queue-mlq-cpu-scheduling/
✍
Write a Testimonial

Program for Next Fit algorithm in Memory Management
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Program for Next Fit algorithm in Memory Management - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite: Partition allocation methods
What is Next Fit ?
Next fit is a modified version of ‘first fit’. It begins as the first fit to find a free partition but when called next time it starts searching from where it left off, not from the beginning. This policy makes use of a roving pointer.
The pointer moves along the memory chain to search for a next fit. This helps in, to avoid the usage of memory always from the head (beginning) of the free block chain.
What are its advantage over first fit ?
First fit is a straight and fast algorithm, but tends to cut large portion of free parts into small pieces due to which, processes that need a large portion of memory block would not get anything even if the sum of all
small pieces is greater than it required which is so-called external fragmentation problem.
Another problem of the first fit is that it tends to allocate memory parts at the beginning of the memory, which may lead to more internal fragments at the beginning. Next fit tries to address this problem by starting
the search for the free portion of parts not from the start of the memory, but from where it ends last time.
Next fit is a very fast searching algorithm and is also comparatively faster than First Fit and Best Fit Memory Management Algorithms.
​
Example:​
Input : blockSize[] = {5, 10, 20};​
processSize[] = {10, 20, 30};​
Output:​
Process No.
Process Size
Block no.​
1
10
2​
2
20
3​
3
30
Not Allocated​
​

Algorithm:

1.
2.
3.
4.

Input the number of memory blocks and their sizes and initializes all the blocks as free.
Input the number of processes and their sizes.
Start by picking each process and check if it can be assigned to the current block, if yes, allocate it the required memory and check for next process but from the block where we left not from starting.
If the current block size is smaller then keep checking the further blocks.

C++
filter_none
edit
close
play_arrow
link

brightness_4
code
// C/C++ program for next fit
// memory management algorithm
#include <bits/stdc++.h>
using namespace std;
// Function to allocate memory to blocks as per Next fit
// algorithm
void NextFit(int blockSize[], int m, int processSize[], int n)
{
// Stores block id of the block allocated to a
// process
int allocation[n], j = 0;
// Initially no block is assigned to any process
memset(allocation, -1, sizeof(allocation));
// pick each process and find suitable blocks
// according to its size ad assign to it
for (int i = 0; i < n; i++) {
// Do not start from beginning
while (j < m) {
if (blockSize[j] >= processSize[i]) {
// allocate block j to p[i] process
allocation[i] = j;
// Reduce available memory in this block.
blockSize[j] -= processSize[i];
break;
}
// mod m will help in traversing the blocks from
// starting block after we reach the end.
j = (j + 1) % m;
}
}
cout << "\nProcess No.\tProcess Size\tBlock no.\n";
for (int i = 0; i < n; i++) {
cout << " " << i + 1 << "\t\t" << processSize[i]
<< "\t\t";
if (allocation[i] != -1)
cout << allocation[i] + 1;
else
cout << "Not Allocated";
cout << endl;
}
}
// Driver program
int main()
{
int blockSize[] = { 5, 10, 20 };
int processSize[] = { 10, 20, 5 };
int m = sizeof(blockSize) / sizeof(blockSize[0]);
int n = sizeof(processSize) / sizeof(processSize[0]);
NextFit(blockSize, m, processSize, n);
return 0;
}

chevron_right
filter_none
Java
filter_none
edit
close
play_arrow
link
brightness_4
code
// Java program for next fit
// memory management algorithm
import java.util.Arrays;
public class GFG {
// Function to allocate memory to blocks as per Next fit
// algorithm
static void NextFit(int blockSize[], int m, int processSize[], int n) {
// Stores block id of the block allocated to a
// process
int allocation[] = new int[n], j = 0;
// Initially no block is assigned to any process
Arrays.fill(allocation, -1);
// pick each process and find suitable blocks
// according to its size ad assign to it
for (int i = 0; i < n; i++) {
// Do not start from beginning
while (j < m) {
if (blockSize[j] >= processSize[i]) {
// allocate block j to p[i] process
allocation[i] = j;
// Reduce available memory in this block.
blockSize[j] -= processSize[i];
break;
}
// mod m will help in traversing the blocks from
// starting block after we reach the end.
j = (j + 1) % m;
}
}
System.out.print("\nProcess No.\tProcess Size\tBlock no.\n");
for (int i = 0; i < n; i++) {
System.out.print( i + 1 + "\t\t" + processSize[i]
+ "\t\t");
if (allocation[i] != -1) {
System.out.print(allocation[i] + 1);
} else {
System.out.print("Not Allocated");
}
System.out.println("");
}

}
// Driver program
static public void main(String[] args) {
int blockSize[] = {5, 10, 20};
int processSize[] = {10, 20, 5};
int m = blockSize.length;
int n = processSize.length;
NextFit(blockSize, m, processSize, n);
}
}
// This code is contributed by Rajput-Ji

chevron_right
filter_none
Python3
filter_none
edit
close
play_arrow
link
brightness_4
code
# Python3 program for next fit
# memory management algorithm
# Function to allocate memory to
# blocks as per Next fit algorithm
def NextFit(blockSize, m, processSize, n):
# Stores block id of the block
# allocated to a process
# Initially no block is assigned
# to any process
allocation = [-1] * n
j = 0
# pick each process and find suitable blocks
# according to its size ad assign to it
for i in range(n):
# Do not start from beginning
while j < m:
if blockSize[j] >= processSize[i]:
# allocate block j to p[i] process
allocation[i] = j
# Reduce available memory in this block.
blockSize[j] -= processSize[i]
break
#
#
#
j

mod m will help in traversing the
blocks from starting block after
we reach the end.
= (j + 1) % m

print("Process No. Process Size Block no.")
for i in range(n):
print(i + 1, "
", processSize[i],
end = "
")
if allocation[i] != -1:
print(allocation[i] + 1)
else:
print("Not Allocated")
# Driver Code
if __name__ == '__main__':
blockSize = [5, 10, 20]
processSize = [10, 20, 5]
m = len(blockSize)
n = len(processSize)
NextFit(blockSize, m, processSize, n)
# This code is contributed by PranchalK

chevron_right
filter_none
C#
filter_none
edit
close
play_arrow
link
brightness_4
code
// C# program for next fit
// memory management algorithm
using System;
using System.Linq;
public class GFG {
// Function to allocate memory to blocks as per Next fit
// algorithm
static void NextFit(int []blockSize, int m,
int []processSize, int n) {
// Stores block id of the block allocated to a
// process
int []allocation = new int[n];
int j = 0;
// Initially no block is assigned to any process
Enumerable.Repeat(-1, n).ToArray();
// pick each process and find suitable blocks
// according to its size ad assign to it
for (int i = 0; i < n; i++) {
// Do not start from beginning
while (j < m) {
if (blockSize[j] >= processSize[i]) {
// allocate block j to p[i] process
allocation[i] = j;

// Reduce available memory in this block.
blockSize[j] -= processSize[i];
break;
}
// mod m will help in traversing the blocks from
// starting block after we reach the end.
j = (j + 1) % m;
}
}
Console.Write("\nProcess No.\tProcess Size\tBlock no.\n");
for (int i = 0; i < n; i++) {
Console.Write( i + 1 + "\t\t" + processSize[i]
+ "\t\t");
if (allocation[i] != -1) {
Console.Write(allocation[i] + 1);
} else {
Console.Write("Not Allocated");
}
Console.WriteLine("");
}
}
// Driver program
static public void Main() {
int []blockSize = {5, 10, 20};
int []processSize = {10, 20, 5};
int m = blockSize.Length;
int n = processSize.Length;
NextFit(blockSize, m, processSize, n);
}
}
/*This code is contributed by Rajput-Ji*/

chevron_right
filter_none
PHP
filter_none
edit
close
play_arrow
link
brightness_4
code
<?php
// PHP program for next fit
// memory management algorithm
// Function to allocate memory to blocks as per Next fit
// algorithm
function NextFit($blockSize, $m, $processSize, $n)
{
// Stores block id of the block allocated to a
// process
$allocation = array_fill(0, $n, -1);
$j = 0;
// Initially no block is assigned to any process above
// pick each process and find suitable blocks
// according to its size ad assign to it
for ($i = 0; $i < $n; $i++)
{
// Do not start from beginning
while ($j < $m)
{
if ($blockSize[$j] >= $processSize[$i])
{
// allocate block j to p[i] process
$allocation[$i] = $j;
// Reduce available memory in this block.
$blockSize[$j] -= $processSize[$i];
break;
}
// mod m will help in traversing the blocks from
// starting block after we reach the end.
$j = ($j + 1) % $m;
}
}
echo "\nProcess No.\tProcess Size\tBlock no.\n";
for ($i = 0; $i < $n; $i++)
{
echo " ".($i + 1)."\t\t".$processSize[$i]."\t\t";
if ($allocation[$i] != -1)
echo ($allocation[$i] + 1);
else
echo "Not Allocated";
echo "\n";
}
}
// Driver program
$blockSize = array( 5, 10, 20 );
$processSize = array( 10, 20, 5 );
$m = count($blockSize);
$n = count($processSize);
NextFit($blockSize, $m, $processSize, $n);
// This code is contributed by mits
?>

chevron_right
filter_none
Output:
​
Process No.
1
2
3

Process Size
10
20
5

Block no.​
2​
3​
1​

Improved By : ishita_thakkar, Rajput-Ji, PranchalKatiyar, Akanksha_Rai, Mithun Kumar

Source
https://www.geeksforgeeks.org/program-for-next-fit-algorithm-in-memory-management/
✍
Write a Testimonial

Peterson’s Algorithm in Process Synchronization
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Peterson's Algorithm in Process Synchronization - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite – synchronization, Critical Section
Problem:The producer consumer problem (or bounded buffer problem) describes two processes, the producer and the consumer, which share a common, fixed-size buffer used as a queue. Producer produce an item and
put it into buffer. If buffer is already full then producer will have to wait for an empty block in buffer. Consumer consume an item from buffer. If buffer is already empty then consumer will have to wait for an item in
buffer. Implement Peterson’s Algorithm for the two processes using shared memory such that there is mutual exclusion between them. The solution should have free from synchronization problems.

Peterson’s algorithm –
filter_none
edit
close
play_arrow
link
brightness_4
code
// code for producer (j)
// producer j is ready
// to produce an item
flag[j] = true;
// but consumer (i) can consume an item
turn = i;
// if consumer is ready to consume an item
// and if its consumer's turn
while (flag[i] == true && turn == i)
{ // then producer will wait }
// otherwise producer will produce
// an item and put it into buffer (critical Section)
// Now, producer is out of critical section
flag[j] = false;
// end of code for producer
//-------------------------------------------------------// code for consumer i
// consumer i is ready
// to consume an item
flag[i] = true;
// but producer (j) can produce an item
turn = j;
// if producer is ready to produce an item
// and if its producer's turn
while (flag[j] == true && turn == j)
{ // then consumer will wait }
// otherwise consumer will consume
// an item from buffer (critical Section)
// Now, consumer is out of critical section
flag[i] = false;
// end of code for consumer

chevron_right
filter_none
Explanation of Peterson’s algorithm –
Peterson’s Algorithm is used to synchronize two processes. It uses two variables, a bool array flag of size 2 and an int variable turn to accomplish it.
In the solution i represents the Consumer and j represents the Producer. Initially the flags are false. When a process wants to execute it’s critical section, it sets it’s flag to true and turn as the index of the other process. This
means that the process wants to execute but it will allow the other process to run first. The process performs busy waiting until the other process has finished it’s own critical section.
After this the current process enters it’s critical section and adds or removes a random number from the shared buffer. After completing the critical section, it sets it’s own flag to false, indication it does not wish to execute
anymore.
The program runs for a fixed amount of time before exiting. This time can be changed by changing value of the macro RT.
filter_none
edit
close

play_arrow
link
brightness_4
code
// C program to implement Peterson’s Algorithm
// for producer-consumer problem.
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <time.h>
#include <sys/types.h>
#include <sys/ipc.h>
#include <sys/shm.h>
#include <stdbool.h>
#define _BSD_SOURCE
#include <sys/time.h>
#include <stdio.h>
#define
#define
#define
#define

BSIZE 8 // Buffer size
PWT 2 // Producer wait time limit
CWT 10 // Consumer wait time limit
RT 10 // Program run-time in seconds

int shmid1, shmid2, shmid3, shmid4;
key_t k1 = 5491, k2 = 5812, k3 = 4327, k4 = 3213;
bool* SHM1;
int* SHM2;
int* SHM3;
int myrand(int n) // Returns a random number between 1 and n
{
time_t t;
srand((unsigned)time(&t));
return (rand() % n + 1);
}
int main()
{
shmid1
shmid2
shmid3
shmid4

=
=
=
=

shmget(k1,
shmget(k2,
shmget(k3,
shmget(k4,

sizeof(bool) * 2, IPC_CREAT | 0660); // flag
sizeof(int) * 1, IPC_CREAT | 0660); // turn
sizeof(int) * BSIZE, IPC_CREAT | 0660); // buffer
sizeof(int) * 1, IPC_CREAT | 0660); // time stamp

if (shmid1 < 0 || shmid2 < 0 || shmid3 < 0 || shmid4 < 0) {
perror("Main shmget error: ");
exit(1);
}
SHM3 = (int*)shmat(shmid3, NULL, 0);
int ix = 0;
while (ix < BSIZE) // Initializing buffer
SHM3[ix++] = 0;
struct timeval t;
time_t t1, t2;
gettimeofday(&t, NULL);
t1 = t.tv_sec;
int* state = (int*)shmat(shmid4, NULL, 0);
*state = 1;
int wait_time;
int i = 0; // Consumer
int j = 1; // Producer
if (fork() == 0) // Producer code
{
SHM1 = (bool*)shmat(shmid1, NULL, 0);
SHM2 = (int*)shmat(shmid2, NULL, 0);
SHM3 = (int*)shmat(shmid3, NULL, 0);
if (SHM1 == (bool*)-1 || SHM2 == (int*)-1 || SHM3 == (int*)-1) {
perror("Producer shmat error: ");
exit(1);
}
bool* flag = SHM1;
int* turn = SHM2;
int* buf = SHM3;
int index = 0;
while (*state == 1) {
flag[j] = true;
printf("Producer is ready now.\n\n");
*turn = i;
while (flag[i] == true && *turn == i)
;
// Critical Section Begin
index = 0;
while (index < BSIZE) {
if (buf[index] == 0) {
int tempo = myrand(BSIZE * 3);
printf("Job %d has been produced\n", tempo);
buf[index] = tempo;
break;
}
index++;
}
if (index == BSIZE)
printf("Buffer is full, nothing can be produced!!!\n");
printf("Buffer: ");
index = 0;
while (index < BSIZE)
printf("%d ", buf[index++]);
printf("\n");
// Critical Section End
flag[j] = false;
if (*state == 0)
break;
wait_time = myrand(PWT);
printf("Producer will wait for %d seconds\n\n", wait_time);
sleep(wait_time);
}
exit(0);
}
if (fork() == 0) // Consumer code
{
SHM1 = (bool*)shmat(shmid1, NULL, 0);
SHM2 = (int*)shmat(shmid2, NULL, 0);
SHM3 = (int*)shmat(shmid3, NULL, 0);
if (SHM1 == (bool*)-1 || SHM2 == (int*)-1 || SHM3 == (int*)-1) {
perror("Consumer shmat error:");
exit(1);
}
bool* flag = SHM1;
int* turn = SHM2;
int* buf = SHM3;
int index = 0;
flag[i] = false;
sleep(5);
while (*state == 1) {
flag[i] = true;
printf("Consumer is ready now.\n\n");
*turn = j;

while (flag[j] == true && *turn == j)
;
// Critical Section Begin
if (buf[0] != 0) {
printf("Job %d has been consumed\n", buf[0]);
buf[0] = 0;
index = 1;
while (index < BSIZE) // Shifting remaining jobs forward
{
buf[index - 1] = buf[index];
index++;
}
buf[index - 1] = 0;
} else
printf("Buffer is empty, nothing can be consumed!!!\n");
printf("Buffer: ");
index = 0;
while (index < BSIZE)
printf("%d ", buf[index++]);
printf("\n");
// Critical Section End
flag[i] = false;
if (*state == 0)
break;
wait_time = myrand(CWT);
printf("Consumer will sleep for %d seconds\n\n", wait_time);
sleep(wait_time);
}
exit(0);
}
// Parent process will now for RT seconds before causing child to terminate
while (1) {
gettimeofday(&t, NULL);
t2 = t.tv_sec;
if (t2 - t1 > RT) // Program will exit after RT seconds
{
*state = 0;
break;
}
}
// Waiting for both processes to exit
wait();
wait();
printf("The clock ran out.\n");
return 0;
}

chevron_right
filter_none
Output:
​
Producer is ready now.​
​
Job 9 has been produced​
Buffer: 9 0 0 0 0 0 0 0 ​
Producer will wait for 1 seconds​
​
Producer is ready now.​
​
Job 8 has been produced​
Buffer: 9 8 0 0 0 0 0 0 ​
Producer will wait for 2 seconds​
​
Producer is ready now.​
​
Job 13 has been produced​
Buffer: 9 8 13 0 0 0 0 0 ​
Producer will wait for 1 seconds​
​
Producer is ready now.​
​
Job 23 has been produced​
Buffer: 9 8 13 23 0 0 0 0 ​
Producer will wait for 1 seconds​
​
Consumer is ready now.​
​
Job 9 has been consumed​
Buffer: 8 13 23 0 0 0 0 0 ​
Consumer will sleep for 9 seconds​
​
Producer is ready now.​
​
Job 15 has been produced​
Buffer: 8 13 23 15 0 0 0 0 ​
Producer will wait for 1 seconds​
​
Producer is ready now.​
​
Job 13 has been produced​
Buffer: 8 13 23 15 13 0 0 0 ​
Producer will wait for 1 seconds​
​
Producer is ready now.​
​
Job 11 has been produced​
Buffer: 8 13 23 15 13 11 0 0 ​
Producer will wait for 1 seconds​
​
Producer is ready now.​
​
Job 22 has been produced​
Buffer: 8 13 23 15 13 11 22 0 ​
Producer will wait for 2 seconds​
​
Producer is ready now.​
​
Job 23 has been produced​
Buffer: 8 13 23 15 13 11 22 23 ​
Producer will wait for 1 seconds​
​
The clock ran out.​

Improved By : Akanksha_Rai

Source
https://www.geeksforgeeks.org/petersons-algorithm-in-process-synchronization/
✍
Write a Testimonial

Mapping Virtual Addresses to Physical Addresses
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Mapping Virtual Addresses to Physical Addresses - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Memory consists of large array of words or arrays, each of which has address associated with it. Now the work of CPU is to fetch instructions from the memory based program counter. Now further these instruction may
cause loading or storing to specific memory address.
Address binding is the process of mapping from one address space to another address space. Logical address is address generated by CPU during execution whereas Physical Address refers to location in memory unit(the
one that is loaded into memory).Note that user deals with only logical address(Virtual address). The logical address undergoes translation by the MMU or address translation unit in particular. The output of this process is
the appropriate physical address or the location of code/data in RAM.
An address binding can be done in three different ways:

Compile Time – If you know that during compile time where process will reside in memory then absolute address is generated i.e physical address is embedded to the executable of the program during compilation.
Loading the executable as a process in memory is very fast. But if the generated address space is preoccupied by other process, then the program crashes and it becomes necessary to recompile the program to change the
address space.
Load time – If it is not known at the compile time where process will reside then relocatable address will be generated. Loader translates the relocatable address to absolute address. The base address of the process in main
memory is added to all logical addresses by the loader to generate absolute address. In this if the base address of the process changes then we need to reload the process again.
Execution time- The instructions are in memory and are being processed by the CPU. Additional memory may be allocated and/or deallocated at this time. This is used if process can be moved from one memory to
another during execution(dynamic linking-Linking that is done during load or run time). e.g – Compaction.
MMU(Memory Management Unit)The run time mapping between Virtual address and Physical Address is done by hardware device known as MMU.
In memory management, Operating System will handle the processes and moves the processes between disk and memory for execution . It keeps the track of available and used memory.
Instruction-execution cycle Follows steps:
1. First instruction is fetched from memory e.g. ADD A,B
2. Then these instructions are decoded i.e., Addition of A and B
3. And further loading or storing at some particular memory location takes place.
Basic Hardware
As main memory and registers are built into processor and CPU can access these only.So every instructions should be written in direct access storage
devices.
1.
2.
3.
4.

If CPU access instruction from register then it can be done in one CPU clock cycle as registers are built into CPU.
If instruction resides in main memory then it will be accessed via memory bus that will take lot of time. So remedy to this add fast memory in between CPU and main memory i.e. adding cache for transaction.
Now we should insure that process resides in legal address.
Legal address consists of base register(holds smallest physical address) and limit register(size of range).

For example:
​
Base register = 300040​
limit register = 120900 ​
then legal address = (300040+120900)= 420940(inclusive).​
legal address = base register+ limit register​

How processes are mapped from disk to memory

1.
2.
3.
4.

Usually process resides in disk in form of binary executable file.
So to execute process it should reside in main memory.
Process is moved from disk to memory based on memory management in use.
The processes waits in disk in form of ready queue to acquire memory.

Procedure of mapping of disk and memory
Normal procedure is that process is selected from input queue and loaded in memory. As process executes it accesses data and instructions from memory and as soon as it completes it will release memory and now
memory will be available for other processes.
MMU scheme –
CPU------- MMU------Memory

1. CPU will generate logical address for eg: 346
2. MMU will generate relocation register(base register) for eg:14000
3. In Memory physical address is located eg:(346+14000= 14346)

Improved By : NEERAJ NEGI

Source
https://www.geeksforgeeks.org/mapping-virtual-addresses-to-physical-addresses/
✍
Write a Testimonial

Semaphores in Process Synchronization
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Semaphores in Process Synchronization - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite: process-synchronization, Mutex vs Semaphore
Semaphore was proposed by Dijkstra in 1965 which is a very significant technique to manage concurrent processes by using a simple integer value, which is known as a semaphore. Semaphore is simply a variable which
is non-negative and shared between threads. This variable is used to solve the critical section problem and to achieve process synchronization in the multiprocessing environment.
Semaphores are of two types:

1. Binary Semaphore – This is also known as mutex lock. It can have only two values – 0 and 1. Its value is initialized to 1. It is used to implement the solution of critical section problem with multiple processes.
2. Counting Semaphore – Its value can range over an unrestricted domain. It is used to control access to a resource that has multiple instances.
Now let us see how it do so.
First, look at two operations which can be used to access and change the value of the semaphore variable.

Some point regarding P and V operation
1. P operation is also called wait, sleep or down operation and V operation is also called signal, wake-up or up operation.
2. Both operations are atomic and semaphore(s) is always initialized to one.Here atomic means that variable on which read, modify and update happens at the same time/moment with no pre-emption i.e. in between
read, modify and update no other operation is performed that may change the variable.
3. A critical section is surrounded by both operations to implement process synchronization.See below image.critical section of Process P is in between P and V operation.

Now, let us see how it implements mutual exclusion. Let there be two processes P1 and P2 and a semaphore s is initialized as 1. Now if suppose P1 enters in its critical section then the value of semaphore s becomes 0.
Now if P2 wants to enter its critical section then it will wait until s > 0, this can only happen when P1 finishes its critical section and calls V operation on semaphore s. This way mutual exclusion is achieved. Look at the
below image for details which is Binary semaphore.

Implementation of binary semaphores:
filter_none
edit
close
play_arrow
link
brightness_4
code
struct semaphore {
enum value(0, 1);
// q contains all Process Control Blocks (PCBs)
// corresponding to processes got blocked
// while performing down operation.
Queue<process> q;
} P(semaphore s)
{
if (s.value == 1) {
s.value = 0;
}
else {
// add the process to the waiting queue
q.push(P)
sleep();
}
}
V(Semaphore s)
{
if (s.q is empty) {
s.value = 1;
}
else {
// select a process from waiting queue
q.pop();
wakeup();
}
}

chevron_right
filter_none
The description above is for binary semaphore which can take only two values 0 and 1 and ensure the mutual exclusion. There is one other type of semaphore called counting semaphore which can take values greater than
one.
Now suppose there is a resource whose number of instance is 4. Now we initialize S = 4 and rest is same as for binary semaphore. Whenever process wants that resource it calls P or wait function and when it is done it
calls V or signal function. If the value of S becomes zero then a process has to wait until S becomes positive. For example, Suppose there are 4 process P1, P2, P3, P4 and they all call wait operation on S(initialized with
4). If another process P5 wants the resource then it should wait until one of the four processes calls signal function and value of semaphore becomes positive.

Limitations
1. One of the biggest limitation of semaphore is priority inversion.
2. Deadlock, suppose a process is trying to wake up another process which is not in sleep state.Therefore a deadlock may block indefinitely.
3. The operating system has to keep track of all calls to wait and to signal the semaphore.
Problem in this implementation of semaphore
Whenever any process waits then it continuously checks for semaphore value (look at this line while (s==0); in P operation) and waste CPU cycle. To avoid this another implementation is provided below.
Implementation of counting semaphore
filter_none
edit
close
play_arrow
link
brightness_4
code
struct Semaphore {
int value;
// q contains all Process Control Blocks(PCBs)

// corresponding to processes got blocked
// while performing down operation.
Queue<process> q;
} P(Semaphore s)
{
s.value = s.value - 1;
if (s.value < 0) {
// add process to queue
// here p is a process which is currently executing
q.push(p);
block();
}
else
return;
}
V(Semaphore s)
{
s.value = s.value + 1;
if (s.value <= 0) {
// remove process p from queue
q.pop();
wakeup(p);
}
else
return;
}

chevron_right
filter_none
In this implementation whenever process waits it is added to a waiting queue of processes associated with that semaphore. This is done through system call block() on that process. When a process is completed it calls
signal function and one process in the queue is resumed. It uses wakeup() system call.

Improved By : Sam_Sparrow, HarshalChavan, Hasanul Islam, soumya7, tushargupta09

Source
https://www.geeksforgeeks.org/semaphores-in-process-synchronization/
✍
Write a Testimonial

Dining-Philosophers Solution Using Monitors
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Dining-Philosophers Solution Using Monitors - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite: Monitor, Process Synchronization
Dining-Philosophers Problem – N philosophers seated around a circular table

There is one chopstick between each philosopher
A philosopher must pick up its two nearest chopsticks in order to eat
A philosopher must pick up first one chopstick, then the second one, not both at once
We need an algorithm for allocating these limited resources(chopsticks) among several processes(philosophers) such that solution is free from deadlock and free from starvation.
There exist some algorithm to solve Dining – Philosopher Problem, but they may have deadlock situation. Also, a deadlock-free solution is not necessarily starvation-free. Semaphores can result in deadlock due to
programming errors. Monitors alone are not sufficiency to solve this, we need monitors with condition variables
Monitor-based Solution to Dining Philosophers
We illustrate monitor concepts by presenting a deadlock-free solution to the dining-philosophers problem. Monitor is used to control access to state variables and condition variables. It only tells when to enter and exit the
segment. This solution imposes the restriction that a philosopher may pick up her chopsticks only if both of them are available.

To code this solution, we need to distinguish among three states in which we may find a philosopher. For this purpose, we introduce the following data structure:
THINKING – When philosopher doesn’t want to gain access to either fork.
HUNGRY – When philosopher wants to enter the critical section.
EATING – When philosopher has got both the forks, i.e., he has entered the section.
Philosopher i can set the variable state[i] = EATING only if her two neighbors are not eating
(state[(i+4) % 5] != EATING) and (state[(i+1) % 5] != EATING).
filter_none
edit
close
play_arrow
link
brightness_4
code
// Dining-Philosophers Solution Using Monitors
monitor DP
{
status state[5];
condition self[5];
// Pickup chopsticks
Pickup(int i)
{
// indicate that I’m hungry
state[i] = hungry;
// set state to eating in test()
// only if my left and right neighbors
// are not eating
test(i);
// if unable to eat, wait to be signaled
if (state[i] != eating)
self[i].wait;
}
// Put down chopsticks
Putdown(int i)
{
// indicate that I’m thinking
state[i] = thinking;
// if right neighbor R=(i+1)%5 is hungry and
// both of R’s neighbors are not eating,
// set R’s state to eating and wake it up by
// signaling R’s CV
test((i + 1) % 5);
test((i + 4) % 5);
}
test(int i)
{
if (state[(i + 1) % 5] != eating
&& state[(i + 4) % 5] != eating
&& state[i] == hungry) {
// indicate that I’m eating
state[i] = eating;
// signal() has no effect during Pickup(),
// but is important to wake up waiting
// hungry philosophers during Putdown()
self[i].signal();
}
}
init()
{
// Execution of Pickup(), Putdown() and test()
// are all mutually exclusive,
// i.e. only one at a time can be executing
for
i = 0 to 4
// Verify that this monitor-based solution is
// deadlock free and mutually exclusive in that
// no 2 neighbors can eat simultaneously
state[i] = thinking;
}
} // end of monitor

chevron_right
filter_none
Above Program is a monitor solution to the dining-philosopher problem.

We also need to declare
condition self[5];

This allows philosopher i to delay herself when she is hungry but is unable to obtain the chopsticks she needs. We are now in a position to describe our solution to the dining-philosophers problem. The distribution of the
chopsticks is controlled by the monitor Dining Philosophers. Each philosopher, before starting to eat, must invoke the operation pickup(). This act may result in the suspension of the philosopher process. After the
successful completion of the operation, the philosopher may eat. Following this, the philosopher invokes the putdown() operation. Thus, philosopher i must invoke the operations pickup() and putdown() in the following
sequence:
​
DiningPhilosophers.pickup(i);​
...​
eat​
...​
DiningPhilosophers.putdown(i);​

It is easy to show that this solution ensures that no two neighbors are eating simultaneously and that no deadlocks will occur. We note, however, that it is possible for a philosopher to starve to death.

Improved By : Palak Jain 5

Source
https://www.geeksforgeeks.org/dining-philosophers-solution-using-monitors/
✍
Write a Testimonial

Remote Procedure Call (RPC) in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Remote Procedure Call (RPC) in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Remote Procedure Call (RPC) is a powerful technique for constructing distributed, client-server based applications . It is based on extending the conventional local procedure calling so that the called procedure need
not exist in the same address space as the calling procedure. The two processes may be on the same system, or they may be on different systems with a network connecting them.
When making a Remote Procedure Call:

1. The calling environment is suspended, procedure parameters are transferred across the network to the environment where the procedure is to execute, and the procedure is executed there.
2. When the procedure finishes and produces its results, its results are transferred back to the calling environment, where execution resumes as if returning from a regular procedure call.
NOTE: RPC is especially well suited for client-server (e.g. query-response) interaction in which the flow of control alternates between the caller and callee. Conceptually, the client and server do not both execute at
the same time. Instead, the thread of execution jumps from the caller to the callee and then back again.
Working of RPC

The following steps take place during a RPC:
1. A client invokes a client stub procedure, passing parameters in the usual way. The client stub resides within the client’s own address space.
2. The client stub marshalls(pack) the parameters into a message. Marshalling includes converting the representation of the parameters into a standard format, and copying each parameter into the message.
3. The client stub passes the message to the transport layer, which sends it to the remote server machine.
4. On the server, the transport layer passes the message to a server stub, which demarshalls(unpack) the parameters and calls the desired server routine using the regular procedure call mechanism.

5. When the server procedure completes, it returns to the server stub (e.g., via a normal procedure call return), which marshalls the return values into a message. The server stub then hands the message to the transport
layer.
6. The transport layer sends the result message back to the client transport layer, which hands the message back to the client stub.
7. The client stub demarshalls the return parameters and execution returns to the caller.
RPC ISSUES
Issues that must be addressed:
1. RPC Runtime: RPC run-time system is a library of routines and a set of services that handle the network communications that underlie the RPC mechanism. In the course of an RPC call, client-side and server-side runtime systems’ code handle binding, establish communications over an appropriate protocol, pass call data between the client and server, and handle communications errors.
2. Stub: The function of the stub is to provide transparency to the programmer-written application code.
On the client side, the stub handles the interface between the client’s local procedure call and the run-time system, marshaling and unmarshaling data, invoking the RPC run-time protocol, and if requested, carrying out
some of the binding steps.
On the server side, the stub provides a similar interface between the run-time system and the local manager procedures that are executed by the server.
3. Binding: How does the client know who to call, and where the service resides?
The most flexible solution is to use dynamic binding and find the server at run time when the RPC is first made. The first time the client stub is invoked, it contacts a name server to determine the transport address at
which the server resides.

Binding consists of two parts:
Naming:
Remote procedures are named through interfaces. An interface uniquely identifies a particular service, describing the types and numbers of its arguments. It is similar in purpose to a type definition in
programming languauges.
Locating:
Finding the transport address at which the server actually resides. Once we have the transport address of the service, we can send messages directly to the server.
A Server having a service to offer exports an interface for it. Exporting an interface registers it with the system so that clients can use it.
A Client must import an (exported) interface before communication can begin.
ADVANTAGES
1. RPC provides ABSTRACTION i.e message-passing nature of network communication is hidden from the user.
2. RPC often omits many of the protocol layers to improve performance. Even a small performance improvement is important because a program may invoke RPCs often.
3. RPC enables the usage of the applications in the distributed environment, not only in the local environment.
4. With RPC code re-writing / re-developing effort is minimized.
5. Process-oriented and thread oriented models supported by RPC.
References:
https://web.cs.wpi.edu/~cs4514/b98/week8-rpc/week8-rpc.html
https://users.cs.cf.ac.uk/Dave.Marshall/C/node33.html

Improved By : Akanksha_Rai

Source
https://www.geeksforgeeks.org/remote-procedure-call-rpc-in-operating-system/
✍
Write a Testimonial

Tracing memory usage in Linux
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Tracing memory usage in Linux - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Often it’s necessary to trace memory usage of the system in order to determine the program that consumes all CPU resources or the program that is responsible to slowing down the activities of the CPU. Tracing memory
usage also becomes necessary to determine the load on the server. Parsing the usage data enables the servers to be able to balance the load and serve the user’s request without slowing down the system.
1. free Displays the amount of memory which is currently available and used by the system(both physical and swapped). free command gathers this data by parsing /proc/meminfo. By default, the amount of memory is
display in kilobytes.
free command in UNIX

watch -n 5 free -m watch command is used to execute a program periodically.

According to the image above, there is a total of 2000 MB of RAM and 1196 MB of swap space allotted to Linux system. Out of this 2000 MB of RAM, 834 MB is currently used where as 590 MB is free. Similarly
for swap space, out of 1196 MB, 0 MB is use and 1196 MB is free currently in the system.
2. vmstat vmstat command is used to display virtual memory statistics of the system. This command reports data about the memory, paging, disk and CPU activities, etc. The first use of this command returns the data
averages since the last reboot. Further uses returns the data based on sampling periods of length delays.

vmstat -d Reports disk statistics

vmstat -s Displays the amount of memory used and available

3. top top command displays all the currently running process in the system. This command displays the list of processes and thread currently being handled by the kernel. top command can also be used to monitor the
total amount of memory usage.

top -H Threads-mode operation​
Displays individual thread that are currently in the system. Without this command ​
option, a summation of all thread in each process is displayed.

4. /proc/meminfo This file contains all the data about the memory usage. It provides the current memory usage details rather than old stored values.

5. htop htop is an interactive process viewer. This command is similar to top command except that it allows to scroll vertically and horizontally to allows users to view all processes running on the system, along with
their full command line as well as viewing them as a process tree, selecting multiple processes and acting on them all at once.
working of htop command in UNIX:

Reference:
Ubuntu Manual

Source
https://www.geeksforgeeks.org/tracing-memory-usage-linux/
✍
Write a Testimonial

Starvation and Aging in Operating Systems
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Starvation and Aging in Operating Systems - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisites : Priority Scheduling

We have already discussed about the priority scheduling in this post. It is one of the most common scheduling algorithms in batch systems. Each process is assigned a priority. Process with the highest priority is to be
executed first and so on.
In this post we will discuss a major problem related to priority scheduling and it’s solution.

Starvation or indefinite blocking is phenomenon associated with the Priority scheduling algorithms, in which a process ready to run for CPU can wait indefinitely because of low priority. In heavily loaded computer
system, a steady stream of higher-priority processes can prevent a low-priority process from ever getting the CPU.
There has been rumors that in 1967 Priority Scheduling was used in IBM 7094 at MIT , and they found a low-priority process that had not been submitted till 1973.

As we see in the above example process having higher priority than other processes getting CPU earlier. We can think of a scenario in which only one process is having very low-priority (for example 127) and we are
giving other process with high-priority, this can lead indefinitely waiting for the process for CPU which is having low-priority, this leads to Starvation. Further we have also discuss about the solution of starvation.
Differences between Deadlock and Starvation in OS :
1. Deadlock occurs when none of the processes in the set is able to move ahead due to occupancy of the required resources by some other process as shown in the figure below, on the other hand Starvation occurs
when a process waits for an indefinite period of time to get the resource it requires.
2. Other name of deadlock is Circular Waiting. Other name of starvation is Lived lock.
3. When deadlock occurs no process can make progress, while in starvation apart from the victim process other processes can progress or proceed.
Solution to Starvation : Aging
Aging is a technique of gradually increasing the priority of processes that wait in the system for a long time.For example, if priority range from 127(low) to 0(high), we could increase the priority of a waiting process by 1
Every 15 minutes. Eventually even a process with an initial priority of 127 would take no more than 32 hours for priority 127 process to age to a priority-0 process.

Source
https://www.geeksforgeeks.org/starvation-and-aging-in-operating-systems/
✍
Write a Testimonial

Program for Shortest Job First (SJF) scheduling | Set 2 (Preemptive)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Program for Shortest Job First (SJF) scheduling | Set 2 (Preemptive) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu

In previous post, we have discussed Set 1 of SJF i.e. non-preemptive. In this post we will discuss the preemptive version of SJF known as Shortest Remaining Time First (SRTF).
In the Shortest Remaining Time First (SRTF) scheduling algorithm, the process with the smallest amount of time remaining until completion is selected to execute. Since the currently executing process is the one with the
shortest amount of time remaining by definition, and since that time should only reduce as execution progresses, processes will always run until they complete or a new process is added that requires a smaller amount of
time.

Process
P1
P2

Duration
9
2

Order
1
2

Arrival Time
0
2

P1 waiting time: 4-2 = 2
P2 waiting time: 0
The average waiting time(AWT): (0 + 2) / 2 = 1
Advantage:
1- Short processes are handled very quickly.
2- The system also requires very little overhead since it only makes a decision when a process completes or a new process is added.
3- When a new process is added the algorithm only needs to compare the currently executing process with the new process, ignoring all other processes currently waiting to execute.
Disadvantage:
1- Like shortest job first, it has the potential for process starvation.
2- Long processes may be held off indefinitely if short processes are continually added.
Source:Wiki
Implementation:
​
1- Traverse until all process gets completely​
executed.​
a) Find process with minimum remaining time at​
every single time lap.​
b) Reduce its time by 1.​
c) Check if its remaining time becomes 0 ​
d) Increment the counter of process completion.​
e) Completion time of current process = ​
current_time +1;​
e) Calculate waiting time for each completed ​
process.​
wt[i]= Completion time - arrival_time-burst_time​
f)Increment time lap by one.​
2- Find turnaround time (waiting_time+burst_time).​

C/C++
filter_none
edit
close
play_arrow
link
brightness_4
code
// C++ program to implement Shortest Remaining Time First
// Shortest Remaining Time First (SRTF)
#include <bits/stdc++.h>
using namespace std;
struct Process {
int pid; // Process ID
int bt; // Burst Time
int art; // Arrival Time
};
// Function to find the waiting time for all
// processes
void findWaitingTime(Process proc[], int n,
int wt[])
{
int rt[n];
// Copy the burst time into rt[]
for (int i = 0; i < n; i++)
rt[i] = proc[i].bt;
int complete = 0, t = 0, minm = INT_MAX;
int shortest = 0, finish_time;
bool check = false;
// Process until all processes gets
// completed
while (complete != n) {
// Find process with minimum
// remaining time among the
// processes that arrives till the
// current time`
for (int j = 0; j < n; j++) {
if ((proc[j].art <= t) &&
(rt[j] < minm) && rt[j] > 0) {
minm = rt[j];
shortest = j;
check = true;
}
}
if (check == false) {
t++;
continue;

}
// Reduce remaining time by one
rt[shortest]--;
// Update minimum
minm = rt[shortest];
if (minm == 0)
minm = INT_MAX;
// If a process gets completely
// executed
if (rt[shortest] == 0) {
// Increment complete
complete++;
check = false;
// Find finish time of current
// process
finish_time = t + 1;
// Calculate waiting time
wt[shortest] = finish_time proc[shortest].bt proc[shortest].art;
if (wt[shortest] < 0)
wt[shortest] = 0;
}
// Increment time
t++;
}
}
// Function to calculate turn around time
void findTurnAroundTime(Process proc[], int n,
int wt[], int tat[])
{
// calculating turnaround time by adding
// bt[i] + wt[i]
for (int i = 0; i < n; i++)
tat[i] = proc[i].bt + wt[i];
}
// Function to calculate average time
void findavgTime(Process proc[], int n)
{
int wt[n], tat[n], total_wt = 0,
total_tat = 0;
// Function to find waiting time of all
// processes
findWaitingTime(proc, n, wt);
// Function to find turn around time for
// all processes
findTurnAroundTime(proc, n, wt, tat);
// Display processes along with all
// details
cout << "Processes "
<< " Burst time "
<< " Waiting time "
<< " Turn around time\n";
// Calculate total waiting time and
// total turnaround time
for (int i = 0; i < n; i++) {
total_wt = total_wt + wt[i];
total_tat = total_tat + tat[i];
cout << " " << proc[i].pid << "\t\t"
<< proc[i].bt << "\t\t " << wt[i]
<< "\t\t " << tat[i] << endl;
}
cout << "\nAverage waiting time = "
<< (float)total_wt / (float)n;
cout << "\nAverage turn around time = "
<< (float)total_tat / (float)n;
}
// Driver code
int main()
{
Process proc[] = { { 1, 6, 1 }, { 2, 8, 1 },
{ 3, 7, 2 }, { 4, 3, 3 } };
int n = sizeof(proc) / sizeof(proc[0]);
findavgTime(proc, n);
return 0;
}

chevron_right
filter_none
Java
filter_none
edit
close
play_arrow
link
brightness_4
code
// Java program to implement Shortest Remaining Time First
// Shortest Remaining Time First (SRTF)
class Process
{
int pid; // Process ID
int bt; // Burst Time
int art; // Arrival Time
public Process(int pid, int bt, int art)
{
this.pid = pid;
this.bt = bt;
this.art = art;
}
}
public class GFG
{
// Method to find the waiting time for all
// processes
static void findWaitingTime(Process proc[], int n,
int wt[])
{
int rt[] = new int[n];

// Copy the burst time into rt[]
for (int i = 0; i < n; i++)
rt[i] = proc[i].bt;
int complete = 0, t = 0, minm = Integer.MAX_VALUE;
int shortest = 0, finish_time;
boolean check = false;
// Process until all processes gets
// completed
while (complete != n) {
// Find process with minimum
// remaining time among the
// processes that arrives till the
// current time`
for (int j = 0; j < n; j++)
{
if ((proc[j].art <= t) &&
(rt[j] < minm) && rt[j] > 0) {
minm = rt[j];
shortest = j;
check = true;
}
}
if (check == false) {
t++;
continue;
}
// Reduce remaining time by one
rt[shortest]--;
// Update minimum
minm = rt[shortest];
if (minm == 0)
minm = Integer.MAX_VALUE;
// If a process gets completely
// executed
if (rt[shortest] == 0) {
// Increment complete
complete++;
check = false;
// Find finish time of current
// process
finish_time = t + 1;
// Calculate waiting time
wt[shortest] = finish_time proc[shortest].bt proc[shortest].art;
if (wt[shortest] < 0)
wt[shortest] = 0;
}
// Increment time
t++;
}
}
// Method to calculate turn around time
static void findTurnAroundTime(Process proc[], int n,
int wt[], int tat[])
{
// calculating turnaround time by adding
// bt[i] + wt[i]
for (int i = 0; i < n; i++)
tat[i] = proc[i].bt + wt[i];
}
// Method to calculate average time
static void findavgTime(Process proc[], int n)
{
int wt[] = new int[n], tat[] = new int[n];
int total_wt = 0, total_tat = 0;
// Function to find waiting time of all
// processes
findWaitingTime(proc, n, wt);
// Function to find turn around time for
// all processes
findTurnAroundTime(proc, n, wt, tat);
// Display processes along with all
// details
System.out.println("Processes " +
" Burst time " +
" Waiting time " +
" Turn around time");
// Calculate total waiting time and
// total turnaround time
for (int i = 0; i < n; i++) {
total_wt = total_wt + wt[i];
total_tat = total_tat + tat[i];
System.out.println(" " + proc[i].pid + "\t\t"
+ proc[i].bt + "\t\t " + wt[i]
+ "\t\t" + tat[i]);
}
System.out.println("Average waiting time = " +
(float)total_wt / (float)n);
System.out.println("Average turn around time = " +
(float)total_tat / (float)n);
}
// Driver Method
public static void main(String[] args)
{
Process proc[] = { new Process(1,
new Process(2,
new Process(3,
new Process(4,
findavgTime(proc, proc.length);
}
}

chevron_right
filter_none
Python3
filter_none
edit
close
play_arrow
link

6,
8,
7,
3,

1),
1),
2),
3)};

brightness_4
code
# Python3 program to implement Shortest Remaining Time First
# Shortest Remaining Time First (SRTF)
# Function to find the waiting time
# for all processes
def findWaitingTime(processes, n, wt):
rt = [0] * n
# Copy the burst time into rt[]
for i in range(n):
rt[i] = processes[i][1]
complete = 0
t = 0
minm = 999999999
short = 0
check = False
# Process until all processes gets
# completed
while (complete != n):
# Find process with minimum remaining
# time among the processes that
# arrives till the current time`
for j in range(n):
if ((processes[j][2] <= t) and
(rt[j] < minm) and rt[j] > 0):
minm = rt[j]
short = j
check = True
if (check == False):
t += 1
continue
# Reduce remaining time by one
rt[short] -= 1
# Update minimum
minm = rt[short]
if (minm == 0):
minm = 999999999
# If a process gets completely
# executed
if (rt[short] == 0):
# Increment complete
complete += 1
check = False
# Find finish time of current
# process
fint = t + 1
# Calculate waiting time
wt[short] = (fint - proc[short][1] proc[short][2])
if (wt[short] < 0):
wt[short] = 0
# Increment time
t += 1
# Function to calculate turn around time
def findTurnAroundTime(processes, n, wt, tat):
# Calculating turnaround time
for i in range(n):
tat[i] = processes[i][1] + wt[i]
# Function to calculate average waiting
# and turn-around times.
def findavgTime(processes, n):
wt = [0] * n
tat = [0] * n
# Function to find waiting time
# of all processes
findWaitingTime(processes, n, wt)
# Function to find turn around time
# for all processes
findTurnAroundTime(processes, n, wt, tat)
# Display processes along with all details
print("Processes
Burst Time
Waiting",
"Time
Turn-Around Time")
total_wt = 0
total_tat = 0
for i in range(n):
total_wt = total_wt + wt[i]
total_tat = total_tat + tat[i]
print(" ", processes[i][0], "\t\t",
processes[i][1], "\t\t",
wt[i], "\t\t", tat[i])
print("\nAverage waiting time = %.5f "%(total_wt /n) )
print("Average turn around time = ", total_tat / n)
# Driver code
if __name__ =="__main__":
# Process id's
proc = [[1, 6, 1], [2, 8, 1],
[3, 7, 2], [4, 3, 3]]
n = 4
findavgTime(proc, n)
# This code is contributed
# Shubham Singh(SHUBHAMSINGH10)

chevron_right
filter_none
C#
filter_none
edit
close
play_arrow
link
brightness_4
code
// C# program to implement Shortest Remaining Time First
// Shortest Remaining Time First (SRTF)

using System;
public class Process
{
public int pid; // Process ID
public int bt; // Burst Time
public int art; // Arrival Time
public Process(int pid, int bt, int art)
{
this.pid = pid;
this.bt = bt;
this.art = art;
}
}
public class GFG
{
// Method to find the waiting
// time for all processes
static void findWaitingTime(Process []proc, int n,
int []wt)
{
int []rt = new int[n];
// Copy the burst time into rt[]
for (int i = 0; i < n; i++)
rt[i] = proc[i].bt;
int complete = 0, t = 0, minm = int.MaxValue;
int shortest = 0, finish_time;
bool check = false;
// Process until all processes gets
// completed
while (complete != n)
{
// Find process with minimum
// remaining time among the
// processes that arrives till the
// current time`
for (int j = 0; j < n; j++)
{
if ((proc[j].art <= t) &&
(rt[j] < minm) && rt[j] > 0)
{
minm = rt[j];
shortest = j;
check = true;
}
}
if (check == false)
{
t++;
continue;
}
// Reduce remaining time by one
rt[shortest]--;
// Update minimum
minm = rt[shortest];
if (minm == 0)
minm = int.MaxValue;
// If a process gets completely
// executed
if (rt[shortest] == 0)
{
// Increment complete
complete++;
check = false;
// Find finish time of current
// process
finish_time = t + 1;
// Calculate waiting time
wt[shortest] = finish_time proc[shortest].bt proc[shortest].art;
if (wt[shortest] < 0)
wt[shortest] = 0;
}
// Increment time
t++;
}
}
// Method to calculate turn around time
static void findTurnAroundTime(Process []proc, int n,
int []wt, int []tat)
{
// calculating turnaround time by adding
// bt[i] + wt[i]
for (int i = 0; i < n; i++)
tat[i] = proc[i].bt + wt[i];
}
// Method to calculate average time
static void findavgTime(Process []proc, int n)
{
int []wt = new int[n];int []tat = new int[n];
int total_wt = 0, total_tat = 0;
// Function to find waiting time of all
// processes
findWaitingTime(proc, n, wt);
// Function to find turn around time for
// all processes
findTurnAroundTime(proc, n, wt, tat);
// Display processes along with all
// details
Console.WriteLine("Processes " +
" Burst time " +
" Waiting time " +
" Turn around time");
// Calculate total waiting time and
// total turnaround time
for (int i = 0; i < n; i++)
{
total_wt = total_wt + wt[i];
total_tat = total_tat + tat[i];
Console.WriteLine(" " + proc[i].pid + "\t\t"
+ proc[i].bt + "\t\t " + wt[i]
+ "\t\t" + tat[i]);
}
Console.WriteLine("Average waiting time = " +
(float)total_wt / (float)n);
Console.WriteLine("Average turn around time = " +
(float)total_tat / (float)n);

}
// Driver Method
public static void Main(String[] args)
{
Process []proc = { new Process(1, 6, 1),
new Process(2, 8, 1),
new Process(3, 7, 2),
new Process(4, 3, 3)};
findavgTime(proc, proc.Length);
}
}
// This code has been contributed by 29AjayKumar

chevron_right
filter_none
Output:
​
Processes Burst time Waiting time
1
6
3
9​
2
8
16
24​
3
7
8
15​
4
3
0
3​
Average waiting time = 6.75​
Average turn around time = 12.75​

Turn around time​

Improved By : eagleateme, SHUBHAMSINGH10, 29AjayKumar

Source
https://www.geeksforgeeks.org/program-shortest-job-first-scheduling-set-2srtf-make-changesdoneplease-review/
✍
Write a Testimonial

Producer-Consumer solution using Semaphores in Java | Set 2
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Producer-Consumer solution using Semaphores in Java | Set 2 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisites – Semaphore in Java, Inter Process Communication, Producer Consumer Problem using Semaphores | Set 1
In computing, the producer–consumer problem (also known as the bounded-buffer problem) is a classic example of a multi-process synchronization problem. The problem describes two processes, the producer and the
consumer, which share a common, fixed-size buffer used as a queue.
The producer’s job is to generate data, put it into the buffer, and start again.
At the same time, the consumer is consuming the data (i.e. removing it from the buffer), one piece at a time.
Problem : To make sure that the producer won’t try to add data into the buffer if it’s full and that the consumer won’t try to remove data from an empty buffer.

Solution : The producer is to either go to sleep or discard data if the buffer is full. The next time the consumer removes an item from the buffer, it notifies the producer, who starts to fill the buffer again. In the same way,
the consumer can go to sleep if it finds the buffer to be empty. The next time the producer puts data into the buffer, it wakes up the sleeping consumer.
An inadequate solution could result in a deadlock where both processes are waiting to be awakened.
In the post Producer-Consumer solution using threads in Java, we have discussed above solution by using inter-thread communication(wait(), notify(), sleep()). In this post, we will use Semaphores to implement the same.
The below solution consists of four classes:
1.
2.
3.
4.

Q : the queue that you’re trying to synchronize
Producer : the threaded object that is producing queue entries
Consumer : the threaded object that is consuming queue entries
PC : the driver class that creates the single Q, Producer, and Consumer.

filter_none
edit
close
play_arrow
link
brightness_4
code
// Java implementation of a producer and consumer
// that use semaphores to control synchronization.
import java.util.concurrent.Semaphore;
class Q {
// an item
int item;
// semCon initialized with 0 permits
// to ensure put() executes first
static Semaphore semCon = new Semaphore(0);
static Semaphore semProd = new Semaphore(1);
// to get an item from buffer
void get()
{
try {
// Before consumer can consume an item,
// it must acquire a permit from semCon
semCon.acquire();
}
catch (InterruptedException e) {
System.out.println("InterruptedException caught");
}
// consumer consuming an item

System.out.println("Consumer consumed item : " + item);
// After consumer consumes the item,
// it releases semProd to notify producer
semProd.release();
}
// to put an item in buffer
void put(int item)
{
try {
// Before producer can produce an item,
// it must acquire a permit from semProd
semProd.acquire();
}
catch (InterruptedException e) {
System.out.println("InterruptedException caught");
}
// producer producing an item
this.item = item;
System.out.println("Producer produced item : " + item);
// After producer produces the item,
// it releases semCon to notify consumer
semCon.release();
}
}
// Producer class
class Producer implements Runnable {
Q q;
Producer(Q q)
{
this.q = q;
new Thread(this, "Producer").start();
}
public void run()
{
for (int i = 0; i < 5; i++)
// producer put items
q.put(i);
}
}
// Consumer class
class Consumer implements Runnable {
Q q;
Consumer(Q q)
{
this.q = q;
new Thread(this, "Consumer").start();
}
public void run()
{
for (int i = 0; i < 5; i++)
// consumer get items
q.get();
}
}
// Driver class
class PC {
public static void main(String args[])
{
// creating buffer queue
Q q = new Q();
// starting consumer thread
new Consumer(q);
// starting producer thread
new Producer(q);
}
}

chevron_right
filter_none
Output:
​
Producer
Consumer
Producer
Consumer
Producer
Consumer
Producer
Consumer
Producer
Consumer

produced
consumed
produced
consumed
produced
consumed
produced
consumed
produced
consumed

item
item
item
item
item
item
item
item
item
item

:
:
:
:
:
:
:
:
:
:

0​
0​
1​
1​
2​
2​
3​
3​
4​
4​

Explanation : As you can see, the calls to put() and get( ) are synchronized, i.e. each call to put() is followed by a call to get( ) and no items are missed. Without the semaphores, multiple calls to put() would have occurred
without matching calls to get(), resulting in items being missed. (To prove this, remove the semaphore code and observe the results.)
The sequencing of put() and get() calls is handled by two semaphores: semProd and semCon.
Before put( ) can produce an item, it must acquire a permit from semProd. After it has produce the item, it releases semCon.
Before get( ) can consume an item, it must acquire a permit from semCon. After it consumes the item, it releases semProd.
This “give and take” mechanism ensures that each call to put( ) must be followed by a call to get( ).
Also notice that semCon is initialized with no available permits. This ensures that put( ) executes first. The ability to set the initial synchronization state is one of the more powerful aspects of a semaphore.

Source
https://www.geeksforgeeks.org/producer-consumer-solution-using-semaphores-java/

✍
Write a Testimonial

Program for Least Recently Used (LRU) Page Replacement algorithm
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Program for Least Recently Used (LRU) Page Replacement algorithm - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite: Page Replacement Algorithms
In operating systems that use paging for memory management, page replacement algorithm are needed to decide which page needed to be replaced when new page comes in. Whenever a new page is referred and not
present in memory, page fault occurs and Operating System replaces one of the existing pages with newly needed page. Different page replacement algorithms suggest different ways to decide which page to replace. The
target for all algorithms is to reduce number of page faults.
In Least Recently Used (LRU) algorithm is a Greedy algorithm where the page to be replaced is least recently used. The idea is based on locality of reference, the least recently used page is not likely

Let say the page reference string 7 0 1 2 0 3 0 4 2 3 0 3 2 . Initially we have 4 page slots empty.
Initially all slots are empty, so when 7 0 1 2 are allocated to the empty slots —> 4 Page faults
0 is already their so —> 0 Page fault.
when 3 came it will take the place of 7 because it is least recently used —>1 Page fault
0 is already in memory so —> 0 Page fault.
4 will takes place of 1 —> 1 Page Fault
Now for the further page reference string —> 0 Page fault because they are already available in the memory.

Given memory capacity (as number of pages it can hold) and a string representing pages to be referred, write a function to find number of page faults.
​
Let capacity be the number of pages that​
memory can hold. Let set be the current​
set of pages in memory.​
​
1- Start traversing the pages.​
i) If set holds less pages than capacity.​
a) Insert page into the set one by one until ​
the size of set reaches capacity or all​
page requests are processed.​
b) Simultaneously maintain the recent occurred​
index of each page in a map called indexes.​
c) Increment page fault​
ii) Else ​
If current page is present in set, do nothing.​
Else ​
a) Find the page in the set that was least ​
recently used. We find it using index array.​
We basically need to replace the page with​
minimum index.​
b) Replace the found page with current page.​
c) Increment page faults.​
d) Update index of current page.​
​
2. Return page faults.​

Below is implementation of above steps.
C++
filter_none
edit
close
play_arrow
link
brightness_4
code
//C++ implementation of above algorithm
#include<bits/stdc++.h>
using namespace std;
// Function to find page faults using indexes
int pageFaults(int pages[], int n, int capacity)
{
// To represent set of current pages. We use
// an unordered_set so that we quickly check
// if a page is present in set or not
unordered_set<int> s;
// To store least recently used indexes
// of pages.
unordered_map<int, int> indexes;
// Start from initial page
int page_faults = 0;
for (int i=0; i<n; i++)
{
// Check if the set can hold more pages
if (s.size() < capacity)
{
// Insert it into set if not present
// already which represents page fault
if (s.find(pages[i])==s.end())
{
s.insert(pages[i]);
// increment page fault
page_faults++;
}

// Store the recently used index of
// each page
indexes[pages[i]] = i;
}
// If the set is full then need to perform lru
// i.e. remove the least recently used page
// and insert the current page
else
{
// Check if current page is not already
// present in the set
if (s.find(pages[i]) == s.end())
{
// Find the least recently used pages
// that is present in the set
int lru = INT_MAX, val;
for (auto it=s.begin(); it!=s.end(); it++)
{
if (indexes[*it] < lru)
{
lru = indexes[*it];
val = *it;
}
}
// Remove the indexes page
s.erase(val);
// insert the current page
s.insert(pages[i]);
// Increment page faults
page_faults++;
}
// Update the current page index
indexes[pages[i]] = i;
}
}
return page_faults;
}
// Driver code
int main()
{
int pages[] = {7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2};
int n = sizeof(pages)/sizeof(pages[0]);
int capacity = 4;
cout << pageFaults(pages, n, capacity);
return 0;
}

chevron_right
filter_none
Java
filter_none
edit
close
play_arrow
link
brightness_4
code
// Java implementation of above algorithm
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
class Test
{
// Method to find page faults using indexes
static int pageFaults(int pages[], int n, int capacity)
{
// To represent set of current pages. We use
// an unordered_set so that we quickly check
// if a page is present in set or not
HashSet<Integer> s = new HashSet<>(capacity);
// To store least recently used indexes
// of pages.
HashMap<Integer, Integer> indexes = new HashMap<>();
// Start from initial page
int page_faults = 0;
for (int i=0; i<n; i++)
{
// Check if the set can hold more pages
if (s.size() < capacity)
{
// Insert it into set if not present
// already which represents page fault
if (!s.contains(pages[i]))
{
s.add(pages[i]);
// increment page fault
page_faults++;
}
// Store the recently used index of
// each page
indexes.put(pages[i], i);
}
// If the set is full then need to perform lru
// i.e. remove the least recently used page
// and insert the current page
else
{
// Check if current page is not already
// present in the set
if (!s.contains(pages[i]))
{
// Find the least recently used pages
// that is present in the set
int lru = Integer.MAX_VALUE, val=Integer.MIN_VALUE;
Iterator<Integer> itr = s.iterator();
while (itr.hasNext()) {
int temp = itr.next();
if (indexes.get(temp) < lru)
{
lru = indexes.get(temp);
val = temp;
}

}
// Remove the indexes page
s.remove(val);
//remove lru from hashmap
indexes.remove(val);
// insert the current page
s.add(pages[i]);
// Increment page faults
page_faults++;
}
// Update the current page index
indexes.put(pages[i], i);
}
}
return page_faults;
}
// Driver method
public static void main(String args[])
{
int pages[] = {7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2};
int capacity = 4;
System.out.println(pageFaults(pages, pages.length, capacity));
}
}
// This code is contributed by Gaurav Miglani

chevron_right
filter_none
C#
filter_none
edit
close
play_arrow
link
brightness_4
code
// C# implementation of above algorithm
using System;
using System.Collections.Generic;
class GFG
{
// Method to find page faults
// using indexes
static int pageFaults(int []pages,
int n, int capacity)
{
// To represent set of current pages.
// We use an unordered_set so that
// we quickly check if a page is
// present in set or not
HashSet<int> s = new HashSet<int>(capacity);
// To store least recently used indexes
// of pages.
Dictionary<int,
int> indexes = new Dictionary<int,
int>();
// Start from initial page
int page_faults = 0;
for (int i = 0; i < n; i++)
{
// Check if the set can hold more pages
if (s.Count < capacity)
{
// Insert it into set if not present
// already which represents page fault
if (!s.Contains(pages[i]))
{
s.Add(pages[i]);
// increment page fault
page_faults++;
}
// Store the recently used index of
// each page
if(indexes.ContainsKey(pages[i]))
indexes[pages[i]] = i;
else
indexes.Add(pages[i], i);
}
// If the set is full then need to
// perform lru i.e. remove the least
// recently used page and insert
// the current page
else
{
// Check if current page is not
// already present in the set
if (!s.Contains(pages[i]))
{
// Find the least recently used pages
// that is present in the set
int lru = int.MaxValue, val = int.MinValue;
foreach (int itr in s)
{
int temp = itr;
if (indexes[temp] < lru)
{
lru = indexes[temp];
val = temp;
}
}
// Remove the indexes page
s.Remove(val);
//remove lru from hashmap
indexes.Remove(val);
// insert the current page
s.Add(pages[i]);
// Increment page faults
page_faults++;
}
// Update the current page index

if(indexes.ContainsKey(pages[i]))
indexes[pages[i]] = i;
else
indexes.Add(pages[i], i);
}
}
return page_faults;
}
// Driver Code
public static void Main(String []args)
{
int []pages = {7, 0, 1, 2, 0, 3,
0, 4, 2, 3, 0, 3, 2};
int capacity = 4;
Console.WriteLine(pageFaults(pages,
pages.Length, capacity));
}
}
// This code is contributed by 29AjayKumar

chevron_right
filter_none
Output:
​
6​

Another approach: (Without using HashMap)
Java
filter_none
edit
close
play_arrow
link
brightness_4
code
// Java program for page replacement algorithms
import java.util.ArrayList;
public class LRU {
// Driver method
public static void main(String[] args) {
int capacity = 4;
int arr[] = {7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2};
// To represent set of current pages.We use
// an Arraylist
ArrayList<Integer> s=new ArrayList<>(capacity);
int count=0;
int page_faults=0;
for(int i:arr)
{
// Insert it into set if not present
// already which represents page fault
if(!s.contains(i))
{
// Check if the set can hold equal pages
if(s.size()==capacity)
{
s.remove(0);
s.add(capacity-1,i);
}
else
s.add(count,i);
// Increment page faults
page_faults++;
++count;
}
else
{
// Remove the indexes page
s.remove((Object)i);
// insert the current page
s.add(s.size(),i);
}
}
System.out.println(page_faults);
}
}

chevron_right
filter_none
C#
filter_none
edit
close
play_arrow
link
brightness_4
code
// C# program for page replacement algorithms
using System;
using System.Collections.Generic;
class LRU
{
// Driver method
public static void Main(String[] args)
{
int capacity = 4;
int []arr = {7, 0, 1, 2, 0, 3, 0,
4, 2, 3, 0, 3, 2};
// To represent set of current pages.
// We use an Arraylist
List<int> s = new List<int>(capacity);
int count = 0;
int page_faults = 0;

foreach(int i in arr)
{
// Insert it into set if not present
// already which represents page fault
if(!s.Contains(i))
{
// Check if the set can hold equal pages
if(s.Count == capacity)
{
s.RemoveAt(0);
s.Insert(capacity - 1, i);
}
else
s.Insert(count, i);
// Increment page faults
page_faults++;
++count;
}
else
{
// Remove the indexes page
s.Remove(i);
// insert the current page
s.Insert(s.Count, i);
}
}
Console.WriteLine(page_faults);
}
}
// This code is contributed by Rajput-Ji

chevron_right
filter_none
Output:
​
6​

Note : We can also find the number of page hits. Just have to maintain a separate count.
If the current page is already in the memory then that must be count as Page-hit.
We will discuss other Page-replacement Algorithms in further sets.

Improved By : RajatNigam1, AkashKhairnar, 29AjayKumar, Rajput-Ji

Source
https://www.geeksforgeeks.org/program-for-least-recently-used-lru-page-replacement-algorithm/
✍
Write a Testimonial

Important Linux Commands (leave, diff, cal, ncal, locate and ln)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Important Linux Commands (leave, diff, cal, ncal, locate and ln) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Linux provides some important tricks. Here’s a few and important one’s:
1. leave — remind you when you have to leave
Syntax:
leave +hhmm

leave waits until the specified time (within the next 12 hours), then reminds you that you have to leave by writing to the TTY that you executed leave on. You are reminded 5 minutes and 1 minute before the actual
time, at the time, and every minute thereafter.
Options: hhmm The time of day is in the form hhmm where hh is a time in hours (on a 12 or 24 hour clock), and mm are minutes.

2. diff – compare files line by line
Syntax:
diff file1 file2

Compare FILES line by line.

diff -q file1 file2

report only when files differ

3. cal, ncal — displays a calendar and the date of Easter
Syntax:
cal

The cal utility displays a simple calendar in traditional format and ncal offers an alternative layout, more options and the date of Easter. The new format is a little cramped but it makes a year fit on a 25×80 terminal.
If arguments are not specified, the current month is displayed.

4. locate – find files by name
Syntax:
locate file_name

locate reads one or more databases prepared by updatedb(8) and writes file names matching at least one of the PATTERNs to standard output, one per line.

5. passwd – change user password
Syntax:
passwd

The passwd command changes passwords for user accounts. A normal user may only change the password for his/her own account, while the superuser may change the password for any account. passwd also
changes the account or associated password validity period.

6. ln – make links between files
Syntax:
ln existing_file_name file2_name

create a link to TARGET with the name specified

Source
https://www.geeksforgeeks.org/important-linux-commands-leave-diff-cal-ncal-locate-ln/
✍
Write a Testimonial

‘crontab’ in Linux with Examples
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ 'crontab' in Linux with Examples - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
The crontab is a list of commands that you want to run on a regular schedule, and also the name of the command used to manage that list. Crontab stands for “cron table, ” because it uses the job scheduler cron to execute
tasks; cron itself is named after “chronos, ” the Greek word for time.cron is the system process which will automatically perform tasks for you according to a set schedule. The schedule is called the crontab, which is also
the name of the program used to edit that schedule.
Linux Crontab Format
​
MIN HOUR DOM MON DOW CMD​

Crontab Fields and Allowed Ranges (Linux Crontab Syntax)

​
Field
MIN
HOUR
DOM
MON
DOW
CMD

Description
Minute field
Hour field
Day of Month
Month field
Day Of Week
Command

Allowed Value​
0 to 59​
0 to 23​
1-31​
1-12​
0-6​
Any command to be executed.​

Examples of Cron jobs
1. Scheduling a Job For a Specific Time
The basic usage of cron is to execute a job in a specific time as shown below. This will execute the Full backup shell script (full-backup) on 10th June 08:30 AM.
The time field uses 24 hours format. So, for 8 AM use 8, and for 8 PM use 20.
​
30 08 10 06 * /home/maverick/full-backup​

30 – 30th Minute
08 – 08 AM
10 – 10th Day
06 – 6th Month (June)
* – Every day of the week
2.To view the Crontab entries
View Current Logged-In User’s Crontab entries : To view your crontab entries type crontab -l from your unix account.

View Root Crontab entries : Login as root user (su – root) and do crontab -l.

To view crontab entries of other Linux users : Login to root and use -u {username} -l.

3.To edit Crontab Entries
Edit Current Logged-In User’s Crontab entries.To edit a crontab entries, use crontab -e. By default this will edit the current logged-in users crontab.

4.To schedule a job for every minute using Cron.
Ideally you may not have a requirement to schedule a job every minute. But understanding this example will will help you understand the other examples.
​
* * * * * CMD​

The * means all the possible unit — i.e every minute of every hour through out the year. More than using this * directly, you will find it very useful in the following cases.
When you specify */5 in minute field means every 5 minutes.
When you specify 0-10/2 in minute field mean every 2 minutes in the first 10 minute.
Thus the above convention can be used for all the other 4 fields.
5.To schedule a job for more than one time (e.g. Twice a Day)
The following script take a incremental backup twice a day every day.
This example executes the specified incremental backup shell script (incremental-backup) at 11:00 and 16:00 on every day. The comma separated value in a field specifies that the command needs to be executed in all the
mentioned time.

​
00 11, 16 * * * /home/maverick/bin/incremental-backup​

00 – 0th Minute (Top of the hour)
11, 16 – 11 AM and 4 PM
* – Every day
* – Every month
* – Every day of the week
6.To schedule a job for certain range of time (e.g. Only on Weekdays)
If you wanted a job to be scheduled for every hour with in a specific range of time then use the following.
Cron Job everyday during working hours :
This example checks the status of the database everyday (including weekends) during the working hours 9 a.m – 6 p.m
​
00 09-18 * * * /home/maverick/bin/check-db-status​

00 – 0th Minute (Top of the hour)
09-18 – 9 am, 10 am, 11 am, 12 am, 1 pm, 2 pm, 3 pm, 4 pm, 5 pm, 6 pm
* – Every day
* – Every month
* – Every day of the week
Cron Job every weekday during working hours :
This example checks the status of the database every weekday (i.e excluding Sat and Sun) during the working hours 9 a.m – 6 p.m.
​
00 09-18 * * 1-5 /home/maverick/bin/check-db-status​

00 – 0th Minute (Top of the hour)
09-18 – 9 am, 10 am, 11 am, 12 am, 1 pm, 2 pm, 3 pm, 4 pm, 5 pm, 6 pm
* – Every day
* – Every month
1-5 -Mon, Tue, Wed, Thu and Fri (Every Weekday)
7.To schedule a background Cron job for every 10 minutes.
Use the following, if you want to check the disk space every 10 minutes.
​
*/10 * * * * /home/maverick/check-disk-space​

It executes the specified command check-disk-space every 10 minutes through out the year. But you may have a requirement of executing the command only during certain hours or vice versa. The above examples shows
how to do those things.Instead of specifying values in the 5 fields, we can specify it using a single keyword as mentioned below.
There are special cases in which instead of the above 5 fields you can use @ followed by a keyword — such as reboot, midnight, yearly, hourly.

Cron special keywords and its meaning
​
Keyword
@yearly
@daily
@hourly
@reboot

Equivalent​
0 0 1 1 *​
0 0 * * *​
0 * * * *​
Run at startup.​

8.To schedule a job for first minute of every year using @yearly
If you want a job to be executed on the first minute of every year, then you can use the @yearly cron keyword as shown below.This will execute the system annual maintenance using annual-maintenance shell script at
00:00 on Jan 1st for every year.

​
@yearly /home/maverick/bin/annual-maintenance​

9.To schedule a Cron job beginning of every month using @monthly
It is as similar as the @yearly as above. But executes the command monthly once using @monthly cron keyword.This will execute the shell script tape-backup at 00:00 on 1st of every month.
​
@monthly /home/maverick/bin/tape-backup​

10.To schedule a background job every day using @daily
Using the @daily cron keyword, this will do a daily log file cleanup using cleanup-logs shell script at 00:00 on every day.
​
@daily /home/maverick/bin/cleanup-logs "day started"​

11.To execute a linux command after every reboot using @reboot
Using the @reboot cron keyword, this will execute the specified command once after the machine got booted every time.
​
@reboot CMD​

Source
https://www.geeksforgeeks.org/crontab-in-linux-with-examples/
✍
Write a Testimonial

Mutex lock for Linux Thread Synchronization
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Mutex lock for Linux Thread Synchronization - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite : Multithreading in C
Thread synchronization is defined as a mechanism which ensures that two or more concurrent processes or threads do not simultaneously execute some particular program segment known as a critical section. Processes’
access to critical section is controlled by using synchronization techniques. When one thread starts executing the critical section (a serialized segment of the program) the other thread should wait until the first thread
finishes. If proper synchronization techniques are not applied, it may cause a race condition where the values of variables may be unpredictable and vary depending on the timings of context switches of the processes or
threads.
Thread Synchronization Problems
An example code to study synchronization problems :

filter_none
edit
close
play_arrow
link
brightness_4
code
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
pthread_t tid[2];
int counter;
void* trythis(void* arg)
{
unsigned long i = 0;
counter += 1;
printf("\n Job %d has started\n", counter);
for (i = 0; i < (0xFFFFFFFF); i++)
;
printf("\n Job %d has finished\n", counter);
return NULL;
}
int main(void)
{
int i = 0;
int error;
while (i < 2) {
error = pthread_create(&(tid[i]), NULL, &trythis, NULL);
if (error != 0)
printf("\nThread can't be created : [%s]", strerror(error));
i++;
}
pthread_join(tid[0], NULL);
pthread_join(tid[1], NULL);
return 0;
}

chevron_right

filter_none
How to compile above program?
To compile a multithreaded program using gcc, we need to link it with the pthreads library. Following is the command used to compile the program.
gfg@ubuntu:~/$ gcc filename.c -lpthread​

In this example, two threads(jobs) are created and in the start function of these threads, a counter is maintained to get the logs about job number which is started and when it is completed.
Output :
​
Job
Job
Job
Job

1
2
2
2

has
has
has
has

started​
started​
finished​
finished​

Problem: From the last two logs, one can see that the log ‘Job 2 has finished’ is repeated twice while no log for ‘Job 1 has finished’ is seen.
Why it has occurred ?
On observing closely and visualizing the execution of the code, we can see that :
The log ‘Job 2 has started’ is printed just after ‘Job 1 has Started’ so it can easily be concluded that while thread 1 was processing the scheduler scheduled the thread 2.
If we take the above assumption true then the value of the ‘ counter’ variable got incremented again before job 1 got finished.
So, when Job 1 actually got finished, then the wrong value of counter produced the log ‘ Job 2 has finished’ followed by the ‘Job 2 has finished’ for the actual job 2 or vice versa as it is dependent on scheduler.
So we see that its not the repetitive log but the wrong value of the ‘counter’ variable that is the problem.
The actual problem was the usage of the variable ‘counter’ by a second thread when the first thread was using or about to use it.
In other words, we can say that lack of synchronization between the threads while using the shared resource ‘counter’ caused the problems or in one word we can say that this problem happened due to
‘Synchronization problem’ between two threads.
How to solve it ?
The most popular way of achieving thread synchronization is by using Mutexes.
A Mutex is a lock that we set before using a shared resource and release after using it.
When the lock is set, no other thread can access the locked region of code.
So we see that even if thread 2 is scheduled while thread 1 was not done accessing the shared resource and the code is locked by thread 1 using mutexes then thread 2 cannot even access that region of code.
So this ensures synchronized access of shared resources in the code.
Working of a mutex
1.
2.
3.
4.
5.
6.

Suppose one thread has locked a region of code using mutex and is executing that piece of code.
Now if scheduler decides to do a context switch, then all the other threads which are ready to execute the same region are unblocked.
Only one of all the threads would make it to the execution but if this thread tries to execute the same region of code that is already locked then it will again go to sleep.
Context switch will take place again and again but no thread would be able to execute the locked region of code until the mutex lock over it is released.
Mutex lock will only be released by the thread who locked it.
So this ensures that once a thread has locked a piece of code then no other thread can execute the same region until it is unlocked by the thread who locked it.

Hence, this system ensures synchronization among the threads while working on shared resources.
A mutex is initialized and then a lock is achieved by calling the following two functions : The first function initializes a mutex and through second function any critical region in the code can be locked.

1. int pthread_mutex_init(pthread_mutex_t *restrict mutex, const pthread_mutexattr_t *restrict attr) : Creates a mutex, referenced by mutex, with attributes specified by attr. If attr is NULL, the default mutex
attribute (NONRECURSIVE) is used.
Returned value
If successful, pthread_mutex_init() returns 0, and the state of the mutex becomes initialized and unlocked.
If unsuccessful, pthread_mutex_init() returns -1.
2. int pthread_mutex_lock(pthread_mutex_t *mutex) : Locks a mutex object, which identifies a mutex. If the mutex is already locked by another thread, the thread waits for the mutex to become available. The
thread that has locked a mutex becomes its current owner and remains the owner until the same thread has unlocked it. When the mutex has the attribute of recursive, the use of the lock may be different. When this
kind of mutex is locked multiple times by the same thread, then a count is incremented and no waiting thread is posted. The owning thread must call pthread_mutex_unlock() the same number of times to decrement
the count to zero.
Returned value
If successful, pthread_mutex_lock() returns 0.
If unsuccessful, pthread_mutex_lock() returns -1.
The mutex can be unlocked and destroyed by calling following two functions :The first function releases the lock and the second function destroys the lock so that it cannot be used anywhere in future.
1. int pthread_mutex_unlock(pthread_mutex_t *mutex) : Releases a mutex object. If one or more threads are waiting to lock the mutex, pthread_mutex_unlock() causes one of those threads to return from
pthread_mutex_lock() with the mutex object acquired. If no threads are waiting for the mutex, the mutex unlocks with no current owner. When the mutex has the attribute of recursive the use of the lock may be
different. When this kind of mutex is locked multiple times by the same thread, then unlock will decrement the count and no waiting thread is posted to continue running with the lock. If the count is decremented to
zero, then the mutex is released and if any thread is waiting for it is posted.
Returned value
If successful, pthread_mutex_unlock() returns 0.
If unsuccessful, pthread_mutex_unlock() returns -1
2. int pthread_mutex_destroy(pthread_mutex_t *mutex) : Deletes a mutex object, which identifies a mutex. Mutexes are used to protect shared resources. mutex is set to an invalid value, but can be reinitialized
using pthread_mutex_init().
Returned value
If successful, pthread_mutex_destroy() returns 0.
If unsuccessful, pthread_mutex_destroy() returns -1.

An example to show how mutexes are used for thread synchronization
filter_none
edit
close
play_arrow

link
brightness_4
code
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
pthread_t tid[2];
int counter;
pthread_mutex_t lock;
void* trythis(void* arg)
{
pthread_mutex_lock(&lock);
unsigned long i = 0;
counter += 1;
printf("\n Job %d has started\n", counter);
for (i = 0; i < (0xFFFFFFFF); i++)
;
printf("\n Job %d has finished\n", counter);
pthread_mutex_unlock(&lock);
return NULL;
}
int main(void)
{
int i = 0;
int error;
if (pthread_mutex_init(&lock, NULL) != 0) {
printf("\n mutex init has failed\n");
return 1;
}
while (i < 2) {
error = pthread_create(&(tid[i]),
NULL,
&trythis, NULL);
if (error != 0)
printf("\nThread can't be created :[%s]",
strerror(error));
i++;
}
pthread_join(tid[0], NULL);
pthread_join(tid[1], NULL);
pthread_mutex_destroy(&lock);
return 0;
}

chevron_right
filter_none
In the above code:
A mutex is initialized in the beginning of the main function.
The same mutex is locked in the ‘trythis()’ function while using the shared resource ‘counter’.
At the end of the function ‘trythis()’ the same mutex is unlocked.
At the end of the main function when both the threads are done, the mutex is destroyed.
Output :
​
Job
Job
Job
Job

1
1
2
2

started​
finished​
started​
finished​

So this time the start and finish logs of both the jobs are present. So thread synchronization took place by the use of Mutex.
References :
Synchronization (computer science)
Lock (computer science)

Improved By : shashibhusan, adenprior

Source
https://www.geeksforgeeks.org/mutex-lock-for-linux-thread-synchronization/
✍
Write a Testimonial

Introduction to Linux Shell and Shell Scripting
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Introduction to Linux Shell and Shell Scripting - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
If you are using any major operating system you are indirectly interacting to shell. If you are running Ubuntu, Linux Mint or any other Linux distribution, you are interacting to shell every time you use terminal. In this
article I will discuss about linux shells and shell scripting so before understanding shell scripting we have to get familiar with following terminologies –
Kernel
Shell
Terminal
What is Kernel
The kernel is a computer program that is the core of a computer’s operating system, with complete control over everything in the system. It manages following resources of the Linux system –

File management

Process management
I/O management
Memory management
Device management etc.
It is often mistaken that Linus Torvalds has developed Linux OS, but actually he is only responsible for development of Linux kernel.
Complete Linux system = Kernel + GNU system utilities and libraries + other management scripts + installation scripts.
What is Shell
A shell is special user program which provide an interface to user to use operating system services. Shell accept human readable commands from user and convert them into something which
kernel can understand. It is a command language interpreter that execute commands read from input devices such as keyboards or from files. The shell gets started when the user logs in or start the
terminal.

linux shell
Shell is broadly classified into two categories –
Command Line Shell
Graphical shell
Command Line Shell
Shell can be accessed by user using a command line interface. A special program called Terminal in linux/macOS or Command Prompt in Windows OS is provided to type in the human readable
commands such as “cat”, “ls” etc. and then it is being execute. The result is then displayed on the terminal to the user. A terminal in Ubuntu 16.4 system looks like this –

linux command line
In above screenshot “ls” command with “-l” option is executed.
It will list all the files in current working directory in long listing format.
Working with command line shell is bit difficult for the beginners because it’s hard to memorize so many commands. It is very powerful, it allows user to store commands in a file and execute
them together. This way any repetitive task can be easily automated. These files are usually called batch files in Windows and Shell Scripts in Linux/macOS systems.
Graphical Shells
Graphical shells provide means for manipulating programs based on graphical user interface (GUI), by allowing for operations such as opening, closing, moving and resizing windows, as well as
switching focus between windows. Window OS or Ubuntu OS can be considered as good example which provide GUI to user for interacting with program. User do not need to type in command
for every actions.A typical GUI in Ubuntu system –
GUI shell

GUI shell
There are several shells are available for Linux systems like –
BASH (Bourne Again SHell) – It is most widely used shell in Linux systems. It is used as default login shell in Linux systems and in macOS. It can also be installed on Windows OS.
CSH (C SHell) – The C shell’s syntax and usage are very similar to the C programming language.
KSH (Korn SHell) – The Korn Shell also was the base for the POSIX Shell standard specifications etc.
Each shell does the same job but understand different commands and provide different built in functions.

Shell Scripting
Usually shells are interactive that mean, they accept command as input from users and execute them. However some time we want to execute a bunch of commands routinely, so we have type in
all commands each time in terminal.
As shell can also take commands as input from file we can write these commands in a file and can execute them in shell to avoid this repetitive work. These files are called Shell Scripts or Shell
Programs. Shell scripts are similar to the batch file in MS-DOS. Each shell script is saved with .sh file extension eg. myscript.sh
A shell script have syntax just like any other programming language. If you have any prior experience with any programming language like Python, C/C++ etc. it would be very easy to get started
with it.
A shell script comprises following elements –
Shell Keywords – if, else, break etc.
Shell commands – cd, ls, echo, pwd, touch etc.
Functions
Control flow – if..then..else, case and shell loops etc.
Why do we need shell scripts
There are many reasons to write shell scripts –
To avoid repetitive work and automation
System admins use shell scripting for routine backups
System monitoring
Adding new functionality to the shell etc.
Advantages of shell scripts
The command and syntax are exactly the same as those directly entered in command line, so programmer do not need to switch to entirely different syntax
Writing shell scripts are much quicker
Quick start
Interactive debugging etc.
Disadvantages of shell scripts
Prone to costly errors, a single mistake can change the command which might be harmful
Slow execution speed
Design flaws within the language syntax or implementation
Not well suited for large and complex task
Provide minimal data structure unlike other scripting languages. etc
Simple demo of shell scripting using Bash Shell
If you work on terminal, something you traverse deep down in directories. Then for coming few directories up in path we have to execute command like this as shown below to get to the “python”
directory –

It is quite frustrating, so why not we can have a utility where we just have to type the name of directory and we can directly jump to that without executing “cd ../” command again and again. Save
the script as “jump.sh”
filter_none
edit
close
play_arrow
link
brightness_4
code
# !/bin/bash
# A simple bash script to move up to desired directory level directly
function jump()
{
# original value of Internal Field Separator
OLDIFS=$IFS
# setting field separator to "/"
IFS=/
# converting working path into array of directories in path
# eg. /my/path/is/like/this
# into [, my, path, is, like, this]
path_arr=($PWD)
# setting IFS to original value
IFS=$OLDIFS
local pos=-1
# ${path_arr[@]} gives all the values in path_arr
for dir in "${path_arr[@]}"
do
# find the number of directories to move up to
# reach at target directory
pos=$[$pos+1]
if [ "$1" = "$dir" ];then
# length of the path_arr
dir_in_path=${#path_arr[@]}
#current working directory
cwd=$PWD
limit=$[$dir_in_path-$pos-1]
for ((i=0; i<limit; i++))
do

cwd=$cwd/..
done
cd $cwd
break
fi
done
}

chevron_right
filter_none
For now we cannot execute our shell script because it do not have permissions. We have to make it executable by typing following command –
​
$ chmod -x path/to/our/file/jump.sh​

Now to make this available on every terminal session, we have to put this in “.bashrc” file.
“.bashrc” is a shell script that Bash shell runs whenever it is started interactively. The purpose of a .bashrc file is to provide a place where you can set up variables, functions and aliases, define our prompt and define
other settings that we want to use whenever we open a new terminal window.
Now open terminal and type following command –
​
$ echo “source ~/path/to/our/file/jump.sh”>> ~/.bashrc​

Now open you terminal and try out new “jump” functionality by typing following command​
$ jump dir_name​

just like below screenshot –

Resources for learning Bash Scripting
https://bash.cyberciti.biz/guide/The_bash_shell
http://tldp.org/LDP/abs/html/
References
https://en.wikipedia.org/wiki/Shell_script
https://en.wikipedia.org/wiki/Shell_(computing)

Source
https://www.geeksforgeeks.org/introduction-linux-shell-shell-scripting/
✍
Write a Testimonial

mindepth and maxdepth in Linux find() command for limiting search to a specific directory.
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ mindepth and maxdepth in Linux find() command for limiting search to a specific directory. - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
How to limit search a specified directory in Linux?
There is a command in Linux to search for files in a directory hierarchy known as ‘find’. It searches the directory tree rooted at each given starting-point by evaluating the given expression from left to right, according to
the rules of precedence, until the outcome is known (the left-hand side is false for and operations, true for or), at which point find moves on to the next file name. If no starting-point is specified, `.’ is assumed.
The find command by default travels down the entire directory tree recursively, which is time and resource consuming. However the depth of directory traversal can be specified(which are mindepth and maxdepth).
What are mindepth and maxdepth levels?
maxdepth levels : Descend at most levels (a non-negative integer) levels of directories below the starting-points. -maxdepth 0 means only apply the tests and actions to the starting-points themselves.
mindepth levels : Do not apply any tests or actions at levels less than levels (a non-negative integer). -mindepth 1 means process all files except the starting-points.
Given below some examples to illustrate how depth of the directory traversal can be specified using mindepth and maxdepth

Find the passwd file under all sub-directories starting from the root directory.
find / -name passwd

Find the passwd file under root and one level down. (i.e root — level 1, and one sub-directory — level 2)
find / -maxdepth 2 -name passwd

Find the passwd file under root and two levels down. (i.e root — level 1, and two sub-directories — level 2 and 3 )
find / -maxdepth 3 -name passwd

Find the password file between sub-directory level 2 and 4.
find / -mindepth 3 -maxdepth 5 -name passwd

There are two other ways to limit search a directory in linux :
1. grep
Grep searches the named input FILEs (or standard input if no files are named, or the file name – is given) for lines containing a match to the given PATTERN.By default, grep prints the matching lines.
Examples of grep :

​
You can search the current directory with grep as follows:​
​

​
​
To check whether a directory exists or not​

​
​
Find the directory under root directory.​

​
​
Find the directory under root and one levels down.​

​
​

​
​

​

2. ack Ack is designed as a replacement for 99% of the uses of grep. Ack searches the named input FILEs (or standard input if no files are named, or the file name – is given) for lines containing a match to the given
PATTERN. By default, ack prints the matching lines.
Ack can also list files that would be searched, without actually searching them, to let you take advantage of ack’s file-type filtering capabilities. Ack does not have a max-depth option
Examples of ack :
​
To check a particular directory under the root​

​
​

​
​

​
​

​

Reference : Linux manual page

Improved By : Akanksha_Rai

Source
https://www.geeksforgeeks.org/mindepth-maxdepth-linux-find-command-limiting-search-specific-directory/
✍
Write a Testimonial

Maximum number of Zombie process a system can handle
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Maximum number of Zombie process a system can handle - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Zombie Process or Defunct Process are those Process which has completed their execution by exit() system call but still has an entry in Process Table. It is a process in terminated state.
When child process is created in UNIX using fork() system call, then if somehow parent process were not available to reap child process from Process Table, then this situation arise. Basically, Zombie Process is neither
completely dead nor completely alive but it has having some state in between.
Since, there is an entry for all the process in process table, even for Zombie Processes. It is obvious that size of process table is Finite. So, if zombie process is created in large amount, then Process Table will get filled
up and program will stop without completing their task.

Here, our task is to find out Maximum Number of Zombie Process created so that Program will not stop its execution. Approach for this problem is to create a zombie process within a loop and count it until the
program does not stop the execution.
Below is the implementation in C of above idea :
filter_none
edit
close
play_arrow
link
brightness_4
code
// C program to find number of Zombie processes a
// system can handle.
#include<stdio.h>
#include<unistd.h>
int main()
{
int count = 0;
while (fork() > 0)

{
count++;
printf("%d\t", count);
}
}

chevron_right
filter_none
Output:
​

​

In the image, we can see after 11834, the increment of count get stopped. However, this is not a fixed number but it will come around it.
Also, it will depend upon system configuration and strength.

Improved By : Akanksha_Rai

Source
https://www.geeksforgeeks.org/maximum-number-zombie-process-system-can-handle/
✍
Write a Testimonial

Program for Priority CPU Scheduling | Set 1
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Program for Priority CPU Scheduling | Set 1 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Priority scheduling is one of the most common scheduling algorithms in batch systems. Each process is assigned a priority. Process with the highest priority is to be executed first and so on.
Processes with the same priority are executed on first come first served basis. Priority can be decided based on memory requirements, time requirements or any other resource requirement.
Implementation :
​
1- First input the processes with their burst time ​
and priority.​
2- Sort the processes, burst time and priority​
according to the priority.​
3- Now simply apply FCFS algorithm.​

Note: A major problem with priority scheduling is indefinite blocking or starvation. A solution to the problem of indefinite blockage of the low-priority process is aging. Aging is a technique of gradually increasing the
priority of processes that wait in the system for a long period of time.
C++
filter_none
edit
close
play_arrow
link
brightness_4
code
// C++ program for implementation of FCFS
// scheduling
#include<bits/stdc++.h>
using namespace std;
struct Process
{
int pid; // Process ID
int bt;
// CPU Burst time required
int priority; // Priority of this process
};
// Function to sort the Process acc. to priority
bool comparison(Process a, Process b)
{
return (a.priority > b.priority);
}
// Function to find the waiting time for all
// processes
void findWaitingTime(Process proc[], int n,
int wt[])
{
// waiting time for first process is 0
wt[0] = 0;
// calculating waiting time
for (int i = 1; i < n ; i++ )
wt[i] = proc[i-1].bt + wt[i-1] ;
}
// Function to calculate turn around time
void findTurnAroundTime( Process proc[], int n,
int wt[], int tat[])
{
// calculating turnaround time by adding
// bt[i] + wt[i]
for (int i = 0; i < n ; i++)
tat[i] = proc[i].bt + wt[i];
}
//Function to calculate average time
void findavgTime(Process proc[], int n)
{
int wt[n], tat[n], total_wt = 0, total_tat = 0;
//Function to find waiting time of all processes
findWaitingTime(proc, n, wt);
//Function to find turn around time for all processes
findTurnAroundTime(proc, n, wt, tat);
//Display processes along with all details
cout << "\nProcesses "<< " Burst time "
<< " Waiting time " << " Turn around time\n";
// Calculate total waiting time and total turn
// around time
for (int i=0; i<n; i++)
{
total_wt = total_wt + wt[i];
total_tat = total_tat + tat[i];
cout << "
" << proc[i].pid << "\t\t"
<< proc[i].bt << "\t
" << wt[i]
<< "\t\t " << tat[i] <<endl;
}
cout <<
<<
cout <<
<<

"\nAverage waiting time = "
(float)total_wt / (float)n;
"\nAverage turn around time = "
(float)total_tat / (float)n;

}
void priorityScheduling(Process proc[], int n)
{
// Sort processes by priority
sort(proc, proc + n, comparison);
cout<< "Order in which processes gets executed \n";
for (int i = 0 ; i < n; i++)
cout << proc[i].pid <<" " ;
findavgTime(proc, n);
}
// Driver code
int main()
{
Process proc[] = {{1, 10, 2}, {2, 5, 0}, {3, 8, 1}};
int n = sizeof proc / sizeof proc[0];
priorityScheduling(proc, n);
return 0;
}

chevron_right
filter_none
Python3
filter_none
edit
close
play_arrow
link
brightness_4
code
# Python3 program for implementation of
# Priority Scheduling
# Function to find the waiting time
# for all processes
def findWaitingTime(processes, n, wt):

wt[0] = 0
# calculating waiting time
for i in range(1, n):
wt[i] = processes[i - 1][1] + wt[i - 1]
# Function to calculate turn around time
def findTurnAroundTime(processes, n, wt, tat):
# Calculating turnaround time by
# adding bt[i] + wt[i]
for i in range(n):
tat[i] = processes[i][1] + wt[i]
# Function to calculate average waiting
# and turn-around times.
def findavgTime(processes, n):
wt = [0] * n
tat = [0] * n
# Function to find waiting time
# of all processes
findWaitingTime(processes, n, wt)
# Function to find turn around time
# for all processes
findTurnAroundTime(processes, n, wt, tat)
# Display processes along with all details
print("\nProcesses
Burst Time
Waiting",
"Time
Turn-Around Time")
total_wt = 0
total_tat = 0
for i in range(n):
total_wt = total_wt + wt[i]
total_tat = total_tat + tat[i]
print(" ", processes[i][0], "\t\t",
processes[i][1], "\t\t",
wt[i], "\t\t", tat[i])
print("\nAverage waiting time = %.5f "%(total_wt /n))
print("Average turn around time = ", total_tat / n)
def priorityScheduling(proc, n):
# Sort processes by priority
proc = sorted(proc, key = lambda proc:proc[2],
reverse = True);
print("Order in which processes gets executed")
for i in proc:
print(i[0], end = " ")
findavgTime(proc, n)
# Driver code
if __name__ =="__main__":
# Process id's
proc = [[1, 10, 1],
[2, 5, 0],
[3, 8, 1]]
n = 3
priorityScheduling(proc, n)
# This code is contributed
# Shubham Singh(SHUBHAMSINGH10)

chevron_right
filter_none
Output:
​
Order in which processes gets executed ​
1 3 2 ​
Processes Burst time Waiting time Turn around time​
1
10
0
10​
3
8
10
18​
2
5
18
23​
​
Average waiting time = 9.33333​
Average turn around time = 17​

In this post, the processes with arrival time 0 are discussed. In next set, we will be considering different arrival times to evaluate waiting times.

Improved By : SHUBHAMSINGH10

Source
https://www.geeksforgeeks.org/program-for-priority-cpu-scheduling-set-1/
✍
Write a Testimonial

Named Pipe or FIFO with example C program
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Named Pipe or FIFO with example C program - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
In computing, a named pipe (also known as a FIFO) is one of the methods for intern-process communication.
It is an extension to the traditional pipe concept on Unix. A traditional pipe is “unnamed” and lasts only as long as the process.
A named pipe, however, can last as long as the system is up, beyond the life of the process. It can be deleted if no longer used.
Usually a named pipe appears as a file and generally processes attach to it for inter-process communication. A FIFO file is a special kind of file on the local storage which allows two or more processes to
communicate with each other by reading/writing to/from this file.
A FIFO special file is entered into the filesystem by calling mkfifo() in C. Once we have created a FIFO special file in this way, any process can open it for reading or writing, in the same way as an ordinary file.
However, it has to be open at both ends simultaneously before you can proceed to do any input or output operations on it.
Creating a FIFO file: In order to create a FIFO file, a function calls i.e. mkfifo is used.
filter_none
edit

close
play_arrow
link
brightness_4
code
int mkfifo(const char *pathname, mode_t mode);

chevron_right
filter_none
mkfifo() makes a FIFO special file with name pathname. Here mode specifies the FIFO’s permissions. It is modified by the process’s umask in the usual way: the permissions of the created file are (mode & ~umask).

Using FIFO: As named pipe(FIFO) is a kind of file, we can use all the system calls associated with it i.e. open, read, write, close.
Example Programs to illustrate the named pipe: There are two programs that use the same FIFO. Program 1 writes first, then reads. The program 2 reads first, then writes. They both keep doing it until terminated.
Program 1(Writes first)
filter_none
edit
close
play_arrow
link
brightness_4
code
// C program to implement one side of FIFO
// This side writes first, then reads
#include <stdio.h>
#include <string.h>
#include <fcntl.h>
#include <sys/stat.h>
#include <sys/types.h>
#include <unistd.h>
int main()
{
int fd;
// FIFO file path
char * myfifo = "/tmp/myfifo";
// Creating the named file(FIFO)
// mkfifo(<pathname>, <permission>)
mkfifo(myfifo, 0666);
char arr1[80], arr2[80];
while (1)
{
// Open FIFO for write only
fd = open(myfifo, O_WRONLY);
// Take an input arr2ing from user.
// 80 is maximum length
fgets(arr2, 80, stdin);
// Write the input arr2ing on FIFO
// and close it
write(fd, arr2, strlen(arr2)+1);
close(fd);
// Open FIFO for Read only
fd = open(myfifo, O_RDONLY);
// Read from FIFO
read(fd, arr1, sizeof(arr1));
// Print the read message
printf("User2: %s\n", arr1);
close(fd);
}
return 0;
}

chevron_right
filter_none
Program 2(Reads First)
filter_none
edit
close
play_arrow
link
brightness_4
code
// C program to implement one side of FIFO
// This side reads first, then reads
#include <stdio.h>
#include <string.h>
#include <fcntl.h>
#include <sys/stat.h>
#include <sys/types.h>
#include <unistd.h>
int main()
{
int fd1;
// FIFO file path
char * myfifo = "/tmp/myfifo";
// Creating the named file(FIFO)
// mkfifo(<pathname>,<permission>)
mkfifo(myfifo, 0666);
char str1[80], str2[80];
while (1)
{
// First open in read only and read
fd1 = open(myfifo,O_RDONLY);
read(fd1, str1, 80);
// Print the read string and close
printf("User1: %s\n", str1);

close(fd1);
// Now open in write mode and write
// string taken from user.
fd1 = open(myfifo,O_WRONLY);
fgets(str2, 80, stdin);
write(fd1, str2, strlen(str2)+1);
close(fd1);
}
return 0;
}

chevron_right
filter_none
Output: Run the two programs simultaneously on two terminals.

Improved By : PaarmitaBhargava

Source
https://www.geeksforgeeks.org/named-pipe-fifo-example-c-program/
✍
Write a Testimonial

File Allocation Methods
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ File Allocation Methods - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
The allocation methods define how the files are stored in the disk blocks. There are three main disk space or file allocation methods.

The allocation methods define how the files are stored in the disk blocks. There are three main disk space or file allocation methods.
Contiguous Allocation
Linked Allocation
Indexed Allocation
The main idea behind these methods is to provide:
Efficient disk space utilization.
Fast access to the file blocks.
All the three methods have their own advantages and disadvantages as discussed below:
1. Contiguous Allocation
In this scheme, each file occupies a contiguous set of blocks on the disk. For example, if a file requires n blocks and is given a block b as the starting location, then the blocks assigned to the file will be: b, b+1, b+2,……
b+n-1. This means that given the starting block address and the length of the file (in terms of blocks required), we can determine the blocks occupied by the file.
The directory entry for a file with contiguous allocation contains
Address of starting block
Length of the allocated portion.
The file ‘mail’ in the following figure starts from the block 19 with length = 6 blocks. Therefore, it occupies 19, 20, 21, 22, 23, 24 blocks.

Advantages:
Both the Sequential and Direct Accesses are supported by this. For direct access, the address of the kth block of the file which starts at block b can easily be obtained as (b+k).
This is extremely fast since the number of seeks are minimal because of contiguous allocation of file blocks.

Disadvantages:
This method suffers from both internal and external fragmentation. This makes it inefficient in terms of memory utilization.
Increasing file size is difficult because it depends on the availability of contiguous memory at a particular instance.
2. Linked List Allocation
In this scheme, each file is a linked list of disk blocks which need not be contiguous. The disk blocks can be scattered anywhere on the disk.
The directory entry contains a pointer to the starting and the ending file block. Each block contains a pointer to the next block occupied by the file.
The file ‘jeep’ in following image shows how the blocks are randomly distributed. The last block (25) contains -1 indicating a null pointer and does not point to any other block.

Advantages:
This is very flexible in terms of file size. File size can be increased easily since the system does not have to look for a contiguous chunk of memory.
This method does not suffer from external fragmentation. This makes it relatively better in terms of memory utilization.
Disadvantages:
Because the file blocks are distributed randomly on the disk, a large number of seeks are needed to access every block individually. This makes linked allocation slower.
It does not support random or direct access. We can not directly access the blocks of a file. A block k of a file can be accessed by traversing k blocks sequentially (sequential access ) from the starting block of the file
via block pointers.
Pointers required in the linked allocation incur some extra overhead.
3. Indexed Allocation
In this scheme, a special block known as the Index block contains the pointers to all the blocks occupied by a file. Each file has its own index block. The ith entry in the index block contains the disk address of the ith file
block. The directory entry contains the address of the index block as shown in the image:

Advantages:
This supports direct access to the blocks occupied by the file and therefore provides fast access to the file blocks.
It overcomes the problem of external fragmentation.
Disadvantages:
The pointer overhead for indexed allocation is greater than linked allocation.
For very small files, say files that expand only 2-3 blocks, the indexed allocation would keep one entire block (index block) for the pointers which is inefficient in terms of memory utilization. However, in linked
allocation we lose the space of only 1 pointer per block.
For files that are very large, single index block may not be able to hold all the pointers.
Following mechanisms can be used to resolve this:
1. Linked scheme: This scheme links two or more index blocks together for holding the pointers. Every index block would then contain a pointer or the address to the next index block.
2. Multilevel index: In this policy, a first level index block is used to point to the second level index blocks which inturn points to the disk blocks occupied by the file. This can be extended to 3 or more levels
depending on the maximum file size.
3. Combined Scheme: In this scheme, a special block called the Inode (information Node) contains all the information about the file such as the name, size, authority, etc and the remaining space of Inode is used to
store the Disk Block addresses which contain the actual file as shown in the image below. The first few of these pointers in Inode point to the direct blocks i.e the pointers contain the addresses of the disk blocks
that contain data of the file. The next few pointers point to indirect blocks. Indirect blocks may be single indirect, double indirect or triple indirect. Single Indirect block is the disk block that does not contain the
file data but the disk address of the blocks that contain the file data. Similarly, double indirect blocks do not contain the file data but the disk address of the blocks that contain the address of the blocks containing
the file data.

Source
https://www.geeksforgeeks.org/file-allocation-methods/
✍
Write a Testimonial

What happens when we turn on computer?
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ What happens when we turn on computer? - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
A computer without a program running is just an inert hunk of electronics. The first thing a computer has to do when it is turned on is to start up a special program called an operating system. The operating system’s job is
to help other computer programs to work by handling the messy details of controlling the computer’s hardware.
An overview of the boot process

The boot process is something that happens every time you turn your computer on. You don’t really see it, because it happens so fast. You press the power button come back a few minutes later and Windows XP, or
Windows Vista, or whatever Operating System you use is all loaded.

The BIOS chip tells it to look in a fixed place, usually on the lowest-numbered hard disk (the boot disk) for a special program called a boot loader (under Linux the boot loader is called Grub or LILO). The boot loader is
pulled into memory and started. The boot loader’s job is to start the real operating system.

Functions of BIOS
POST (Power On Self Test) The Power On Self Test happens each time you turn your computer on. It sounds complicated and that’s because it kind of is. Your computer does so much when its turned on and this is just
part of that.
It initializes the various hardware devices. It is an important process so as to ensure that all the devices operate smoothly without any conflicts. BIOSes following ACPI create tables describing the devices in the computer.
The POST first checks the bios and then tests the CMOS RAM. If there is no problem with this then POST continues to check the CPU, hardware devices such as the Video Card, the secondary storage devices such as the
Hard Drive, Floppy Drives, Zip Drive or CD/DVD Drives. If some errors found then an error message is displayed on the screen or a number of beeps are heard. These beeps are known as POST beep codes.
Master Boot Record
The Master Boot Record (MBR) is a small program that starts when the computer is booting, in order to find the operating system (eg. Windows XP). This complicated process (called the Boot Process) starts with the
POST (Power On Self Test) and ends when the Bios searches for the MBR on the Hard Drive, which is generally located in the first sector, first head, first cylinder (cylinder 0, head 0, sector 1).
A typical structure looks like:

The bootstrap loader is stored in computer’s EPROM, ROM, or another non-volatile memory. When the computer is turned on or restarted, it first performs the power-on-self-test, also known as POST. If the POST is
successful and no issues are found, the bootstrap loader will load the operating system for the computer into memory. The computer will then be able to quickly access, load, and run the operating system.
init
init is the last step of the kernel boot sequence. It looks for the file /etc/inittab to see if there is an entry for initdefault. It is used to determine initial run-level of the system. A run-level is used to decide the initial state of
the operating system.
Some of the run levels are:
Level
0 –> System Halt
1 –> Single user mode
3 –> Full multiuser mode with network
5 –> Full multiuser mode with network and X display manager
6 –> Reboot
The above design of init is called SysV- pronounced as System five. Several other implementations of init have been written now. Some of the popular implementatios are systemd and upstart. Upstart is being used by
ubuntu since 2006. More details of the upstart can be found here.
The next step of init is to start up various daemons that support networking and other services. X server daemon is one of the most important daemon. It manages display, keyboard, and mouse. When X server daemon is
started you see a Graphical Interface and a login screen is displayed.
References :
http://www.tldp.org/HOWTO/Unix-and-Internet-Fundamentals-HOWTO/bootup.html
https://www.computerhope.com/jargon/b/bootload.htm
http://www.dewassoc.com/kbase/hard_drives/master_boot_record.htm

Improved By : ayushgangwar, DhananjayGhumare

Source
https://www.geeksforgeeks.org/what-happens-when-we-turn-on-computer/
✍
Write a Testimonial

Program for Banker’s Algorithm | Set 1 (Safety Algorithm)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Program for Banker's Algorithm | Set 1 (Safety Algorithm) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisite: Banker’s Algorithm
The banker’s algorithm is a resource allocation and deadlock avoidance algorithm that tests for safety by simulating the allocation for predetermined maximum possible amounts of all resources, then makes an “s-state”
check to test for possible activities, before deciding whether allocation should be allowed to continue.
Following Data structures are used to implement the Banker’s Algorithm:

Let ‘n’ be the number of processes in the system and ‘m’ be the number of resources types.
Available :
It is a 1-d array of size ‘m’ indicating the number of available resources of each type.

Available[ j ] = k means there are ‘k’ instances of resource type R j
Max :
It is a 2-d array of size ‘n*m’ that defines the maximum demand of each process in a system.
Max[ i, j ] = k means process Pi may request at most ‘k’ instances of resource type R j.
Allocation :
It is a 2-d array of size ‘n*m’ that defines the number of resources of each type currently allocated to each process.
Allocation[ i, j ] = k means process Pi is currently allocated ‘k’ instances of resource type R j
Need :
It is a 2-d array of size ‘n*m’ that indicates the remaining resource need of each process.
Need [ i, j ] = k means process Pi currently allocated ‘k’ instances of resource type R j
Need [ i, j ] = Max [ i, j ] – Allocation [ i, j ]
Allocationi specifies the resources currently allocated to process P i and Needi specifies the additional resources that process P i may still request to complete its task.
Banker’s algorithm consist of Safety algorithm and Resource request algorithm
Safety Algorithm
The algorithm for finding out whether or not a system is in a safe state can be described as follows:
1. Let Work and Finish be vectors of length ‘m’ and ‘n’ respectively.
Initialize: Work= Available
Finish [i]=false; for i=1,2,……,n
2. Find an i such that both
a) Finish [i]=false
b) Need_i<=work
if no such i exists goto step (4)
3. Work=Work + Allocation_i
Finish[i]= true
goto step(2)
4. If Finish[i]=true for all i,
then the system is in safe state.
Safe sequence is the sequence in which the processes can be safely executed.
In this post, implementation of Safety algorithm of Banker’s Algorithm is done.
C++
filter_none
edit
close
play_arrow
link
brightness_4
code
// C++ program to illustrate Banker's Algorithm
#include<iostream>
using namespace std;
// Number of processes
const int P = 5;
// Number of resources
const int R = 3;
// Function to find the need of each process
void calculateNeed(int need[P][R], int maxm[P][R],
int allot[P][R])
{
// Calculating Need of each P
for (int i = 0 ; i < P ; i++)
for (int j = 0 ; j < R ; j++)
// Need of instance = maxm instance //
allocated instance
need[i][j] = maxm[i][j] - allot[i][j];
}
// Function to find the system is in safe state or not
bool isSafe(int processes[], int avail[], int maxm[][R],
int allot[][R])
{
int need[P][R];
// Function to calculate need matrix
calculateNeed(need, maxm, allot);
// Mark all processes as infinish
bool finish[P] = {0};
// To store safe sequence
int safeSeq[P];
// Make a copy of available resources
int work[R];
for (int i = 0; i < R ; i++)
work[i] = avail[i];
// While all processes are not finished
// or system is not in safe state.
int count = 0;
while (count < P)
{
// Find a process which is not finish and
// whose needs can be satisfied with current
// work[] resources.
bool found = false;
for (int p = 0; p < P; p++)
{
// First check if a process is finished,
// if no, go for next condition
if (finish[p] == 0)
{
// Check if for all resources of
// current P need is less
// than work
int j;
for (j = 0; j < R; j++)
if (need[p][j] > work[j])
break;
// If all needs of p were satisfied.
if (j == R)
{
// Add the allocated resources of

// current P to the available/work
// resources i.e.free the resources
for (int k = 0 ; k < R ; k++)
work[k] += allot[p][k];
// Add this process to safe sequence.
safeSeq[count++] = p;
// Mark this p as finished
finish[p] = 1;
found = true;
}
}
}
// If we could not find a next process in safe
// sequence.
if (found == false)
{
cout << "System is not in safe state";
return false;
}
}
// If system is in safe state then
// safe sequence will be as below
cout << "System is in safe state.\nSafe"
" sequence is: ";
for (int i = 0; i < P ; i++)
cout << safeSeq[i] << " ";
return true;
}
// Driver code
int main()
{
int processes[] = {0, 1, 2, 3, 4};
// Available instances of resources
int avail[] = {3, 3, 2};
// Maximum R that can be allocated
// to processes
int maxm[][R] = {{7, 5, 3},
{3, 2, 2},
{9, 0, 2},
{2, 2, 2},
{4, 3, 3}};
// Resources allocated to processes
int allot[][R] = {{0, 1, 0},
{2, 0, 0},
{3, 0, 2},
{2, 1, 1},
{0, 0, 2}};
// Check system is in safe state or not
isSafe(processes, avail, maxm, allot);
return 0;
}

chevron_right
filter_none
Java
filter_none
edit
close
play_arrow
link
brightness_4
code
// Java program to illustrate Banker's Algorithm
import java.util.*;
class GFG
{
// Number of processes
static int P = 5;
// Number of resources
static int R = 3;
// Function to find the need of each process
static void calculateNeed(int need[][], int maxm[][],
int allot[][])
{
// Calculating Need of each P
for (int i = 0 ; i < P ; i++)
for (int j = 0 ; j < R ; j++)
// Need of instance = maxm instance //
allocated instance
need[i][j] = maxm[i][j] - allot[i][j];
}
// Function to find the system is in safe state or not
static boolean isSafe(int processes[], int avail[], int maxm[][],
int allot[][])
{
int [][]need = new int[P][R];
// Function to calculate need matrix
calculateNeed(need, maxm, allot);
// Mark all processes as infinish
boolean []finish = new boolean[P];
// To store safe sequence
int []safeSeq = new int[P];
// Make a copy of available resources
int []work = new int[R];
for (int i = 0; i < R ; i++)
work[i] = avail[i];
// While all processes are not finished
// or system is not in safe state.
int count = 0;
while (count < P)
{
// Find a process which is not finish and
// whose needs can be satisfied with current
// work[] resources.
boolean found = false;
for (int p = 0; p < P; p++)

{
// First check if a process is finished,
// if no, go for next condition
if (finish[p] == false)
{
// Check if for all resources of
// current P need is less
// than work
int j;
for (j = 0; j < R; j++)
if (need[p][j] > work[j])
break;
// If all needs of p were satisfied.
if (j == R)
{
// Add the allocated resources of
// current P to the available/work
// resources i.e.free the resources
for (int k = 0 ; k < R ; k++)
work[k] += allot[p][k];
// Add this process to safe sequence.
safeSeq[count++] = p;
// Mark this p as finished
finish[p] = true;
found = true;
}
}
}
// If we could not find a next process in safe
// sequence.
if (found == false)
{
System.out.print("System is not in safe state");
return false;
}
}
// If system is in safe state then
// safe sequence will be as below
System.out.print("System is in safe state.\nSafe"
+" sequence is: ");
for (int i = 0; i < P ; i++)
System.out.print(safeSeq[i] + " ");
return true;
}
// Driver code
public static void main(String[] args)
{
int processes[] = {0, 1, 2, 3, 4};
// Available instances of resources
int avail[] = {3, 3, 2};
// Maximum R that can be allocated
// to processes
int maxm[][] = {{7, 5, 3},
{3, 2, 2},
{9, 0, 2},
{2, 2, 2},
{4, 3, 3}};
// Resources allocated to processes
int allot[][] = {{0, 1, 0},
{2, 0, 0},
{3, 0, 2},
{2, 1, 1},
{0, 0, 2}};
// Check system is in safe state or not
isSafe(processes, avail, maxm, allot);
}
}
// This code has been contributed by 29AjayKumar

chevron_right
filter_none
Python3
filter_none
edit
close
play_arrow
link
brightness_4
code
# Python3 program to illustrate
# Banker's Algorithm
# Number of processes
P = 5
# Number of resources
R = 3
# Function to find the need of each process
def calculateNeed(need, maxm, allot):
# Calculating Need of each P
for i in range(P):
for j in range(R):
# Need of instance = maxm instance # allocated instance
need[i][j] = maxm[i][j] - allot[i][j]
# Function to find the system is in
# safe state or not
def isSafe(processes, avail, maxm, allot):
need = []
for i in range(P):
l = []
for j in range(R):
l.append(0)
need.append(l)
# Function to calculate need matrix
calculateNeed(need, maxm, allot)
# Mark all processes as infinish
finish = [0] * P
# To store safe sequence

safeSeq = [0] * P
# Make a copy of available resources
work = [0] * R
for i in range(R):
work[i] = avail[i]
# While all processes are not finished
# or system is not in safe state.
count = 0
while (count < P):
# Find a process which is not finish
# and whose needs can be satisfied
# with current work[] resources.
found = False
for p in range(P):
# First check if a process is finished,
# if no, go for next condition
if (finish[p] == 0):
# Check if for all resources
# of current P need is less
# than work
for j in range(R):
if (need[p][j] > work[j]):
break
# If all needs of p were satisfied.
if (j == R - 1):
# Add the allocated resources of
# current P to the available/work
# resources i.e.free the resources
for k in range(R):
work[k] += allot[p][k]
# Add this process to safe sequence.
safeSeq[count] = p
count += 1
# Mark this p as finished
finish[p] = 1
found = True
# If we could not find a next process
# in safe sequence.
if (found == False):
print("System is not in safe state")
return False
# If system is in safe state then
# safe sequence will be as below
print("System is in safe state.",
"\nSafe sequence is: ", end = " ")
print(*safeSeq)
return True
# Driver code
if __name__ =="__main__":
processes = [0, 1, 2, 3, 4]
# Available instances of resources
avail = [3, 3, 2]
# Maximum R that can be allocated
# to processes
maxm = [[7, 5, 3], [3, 2, 2],
[9, 0, 2], [2, 2, 2],
[4, 3, 3]]
# Resources allocated to processes
allot = [[0, 1, 0], [2, 0, 0],
[3, 0, 2], [2, 1, 1],
[0, 0, 2]]
# Check system is in safe state or not
isSafe(processes, avail, maxm, allot)
# This code is contributed by
# Shubham Singh(SHUBHAMSINGH10)

chevron_right
filter_none
C#
filter_none
edit
close
play_arrow
link
brightness_4
code
// C# program to illustrate Banker's Algorithm
using System;
class GFG
{
// Number of processes
static int P = 5;
// Number of resources
static int R = 3;
// Function to find the need of each process
static void calculateNeed(int [,]need, int [,]maxm,
int [,]allot)
{
// Calculating Need of each P
for (int i = 0 ; i < P ; i++)
for (int j = 0 ; j < R ; j++)
// Need of instance = maxm instance //
allocated instance
need[i,j] = maxm[i,j] - allot[i,j];
}
// Function to find the system is in safe state or not
static bool isSafe(int []processes, int []avail, int [,]maxm,
int [,]allot)
{
int [,]need = new int[P,R];
// Function to calculate need matrix
calculateNeed(need, maxm, allot);

// Mark all processes as infinish
bool []finish = new bool[P];
// To store safe sequence
int []safeSeq = new int[P];
// Make a copy of available resources
int []work = new int[R];
for (int i = 0; i < R ; i++)
work[i] = avail[i];
// While all processes are not finished
// or system is not in safe state.
int count = 0;
while (count < P)
{
// Find a process which is not finish and
// whose needs can be satisfied with current
// work[] resources.
bool found = false;
for (int p = 0; p < P; p++)
{
// First check if a process is finished,
// if no, go for next condition
if (finish[p] == false)
{
// Check if for all resources of
// current P need is less
// than work
int j;
for (j = 0; j < R; j++)
if (need[p,j] > work[j])
break;
// If all needs of p were satisfied.
if (j == R)
{
// Add the allocated resources of
// current P to the available/work
// resources i.e.free the resources
for (int k = 0 ; k < R ; k++)
work[k] += allot[p,k];
// Add this process to safe sequence.
safeSeq[count++] = p;
// Mark this p as finished
finish[p] = true;
found = true;
}
}
}
// If we could not find a next process in safe
// sequence.
if (found == false)
{
Console.Write("System is not in safe state");
return false;
}
}
// If system is in safe state then
// safe sequence will be as below
Console.Write("System is in safe state.\nSafe"
+" sequence is: ");
for (int i = 0; i < P ; i++)
Console.Write(safeSeq[i] + " ");
return true;
}
// Driver code
static public void Main ()
{
int []processes = {0, 1, 2, 3, 4};
// Available instances of resources
int []avail = {3, 3, 2};
// Maximum R that can be allocated
// to processes
int [,]maxm = {{7, 5, 3},
{3, 2, 2},
{9, 0, 2},
{2, 2, 2},
{4, 3, 3}};
// Resources allocated to processes
int [,]allot = {{0, 1, 0},
{2, 0, 0},
{3, 0, 2},
{2, 1, 1},
{0, 0, 2}};
// Check system is in safe state or not
isSafe(processes, avail, maxm, allot);
}
}
// This code has been contributed by ajit.

chevron_right
filter_none
Output:
​
System is in safe state.​
Safe sequence is: 1 3 4 0 2​

Illustration :
Considering a system with five processes P0 through P4 and three resources types A, B, C. Resource type A has 10 instances, B has 5 instances and type C has 7 instances. Suppose at time t0 following snapshot of the
system has been taken:

We must determine whether the new system state is safe. To do so, we need to execute Safety algorithm on the above given allocation chart.

Following is the resource allocation graph:

Executing safety algorithm shows that sequence < P1, P3, P4, P0, P2 > satisfies safety requirement.

Improved By : Prajwal Bhati, SHUBHAMSINGH10, 29AjayKumar, jit_t

Source
https://www.geeksforgeeks.org/program-bankers-algorithm-set-1-safety-algorithm/
✍
Write a Testimonial

Convoy Effect in Operating Systems
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Convoy Effect in Operating Systems - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisites : Basics of FCFS Scheduling (Program for FCFS Scheduling | Set 1, Program for FCFS Scheduling | Set 2 )
Convoy Effect is phenomenon associated with the First Come First Serve (FCFS) algorithm, in which the whole Operating System slows down due to few slow processes.

FCFS algorithm is non-preemptive in nature, that is, once CPU time has been allocated to a process, other processes can get CPU time only after the current process has finished. This property of FCFS scheduling leads to
the situation called Convoy Effect.
Suppose there is one CPU intensive (large burst time) process in the ready queue, and several other processes with relatively less burst times but are Input/Output (I/O) bound (Need I/O operations frequently).
Steps are as following below:
The I/O bound processes are first allocated CPU time. As they are less CPU intensive, they quickly get executed and goto I/O queues.
Now, the CPU intensive process is allocated CPU time. As its burst time is high, it takes time to complete.
While the CPU intensive process is being executed, the I/O bound processes complete their I/O operations and are moved back to ready queue.
However, the I/O bound processes are made to wait as the CPU intensive process still hasn’t finished. This leads to I/O devices being idle.

When the CPU intensive process gets over, it is sent to the I/O queue so that it can access an I/O device.
Meanwhile, the I/O bound processes get their required CPU time and move back to I/O queue.
However, they are made to wait because the CPU intensive process is still accessing an I/O device. As a result, the CPU is sitting idle now.
Hence in Convoy Effect, one slow process slows down the performance of the entire set of processes, and leads to wastage of CPU time and other devices.
To avoid Convoy Effect, preemptive scheduling algorithms like Round Robin Scheduling can be used – as the smaller processes don’t have to wait much for CPU time – making their execution faster and leading to less
resources sitting idle.
References –
A. Silberschatz, P. Galvin, G. Gagne, “Operating Systems Concepts (8th Edition)”, Wiley India Pvt. Ltd.

Improved By : anish3007, saurabhhjjain

Source
https://www.geeksforgeeks.org/convoy-effect-operating-systems/
✍
Write a Testimonial

Zombie Processes and their Prevention
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Zombie Processes and their Prevention - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Prerequisites: fork() in C, Zombie Process
Zombie state : When a process is created in UNIX using fork() system call, the address space of the Parent process is replicated. If the parent process calls wait() system call, then the execution of parent is suspended until
the child is terminated. At the termination of the child, a ‘SIGCHLD’ signal is generated which is delivered to the parent by the kernel. Parent, on receipt of ‘SIGCHLD’ reaps the status of the child from the process table.
Even though, the child is terminated, there is an entry in the process table corresponding to the child where the status is stored. When parent collects the status, this entry is deleted. Thus, all the traces of the child process
are removed from the system. If the parent decides not to wait for the child’s termination and it executes its subsequent task, then at the termination of the child, the exit status is not read. Hence, there remains an entry in
the process table even after the termination of the child. This state of the child process is known as the Zombie state.
filter_none
edit
close
play_arrow
link
brightness_4
code
// A C program to demonstrate working of
// fork() and process table entries.
#include<stdio.h>
#include<unistd.h>
#include<sys/wait.h>
#include<sys/types.h>
int main()
{
int i;
int pid = fork();
if (pid == 0)
{
for (i=0; i<20; i++)
printf("I am Child\n");
}
else
{
printf("I am Parent\n");
while(1);
}
}

chevron_right
filter_none
Output :

Now check the process table using the following command in the terminal
$ ps -eaf

Here the entry [a.out] defunct shows the zombie process.
Why do we need to prevent the creation of Zombie process?
There is one process table per system. The size of the process table is finite. If too many zombie processes are generated, then the process table will be full. That is, the system will not be able to generate any new process,
then the system will come to a standstill. Hence, we need to prevent the creation of zombie processes.
1. Using wait() system call : When the parent process calls wait(), after the creation of a child, it indicates that, it will wait for the child to complete and it will reap the exit status of the child. The parent process is
suspended(waits in a waiting queue) until the child is terminated. It must be understood that during this period, the parent process does nothing just waits.
filter_none
edit
close
play_arrow
link
brightness_4
code
// A C program to demonstrate working of
// fork()/wait() and Zombie processes
#include<stdio.h>
#include<unistd.h>
#include<sys/wait.h>
#include<sys/types.h>

int main()
{
int i;
int pid = fork();
if (pid==0)
{
for (i=0; i<20; i++)
printf("I am Child\n");
}
else
{
wait(NULL);
printf("I am Parent\n");
while(1);
}
}

chevron_right
filter_none
2. By ignoring the SIGCHLD signal : When a child is terminated, a corresponding SIGCHLD signal is delivered to the parent, if we call the ‘signal(SIGCHLD,SIG_IGN)’, then the SIGCHLD signal is ignored by the
system, and the child process entry is deleted from the process table. Thus, no zombie is created. However, in this case, the parent cannot know about the exit status of the child.
filter_none
edit
close
play_arrow
link
brightness_4
code
// A C program to demonstrate ignoring
// SIGCHLD signal to prevent Zombie processes
#include<stdio.h>
#include<unistd.h>
#include<sys/wait.h>
#include<sys/types.h>
int main()
{
int i;
int pid = fork();
if (pid == 0)
for (i=0; i<20; i++)
printf("I am Child\n");
else
{
signal(SIGCHLD,SIG_IGN);
printf("I am Parent\n");
while(1);
}
}

chevron_right
filter_none
3. By using a signal handler : The parent process installs a signal handler for the SIGCHLD signal. The signal handler calls wait() system call within it. In this senario, when the child terminated, the SIGCHLD is
delivered to the parent. On receipt of SIGCHLD, the corresponding handler is activated, which in turn calls the wait() system call. Hence, the parent collects the exit status almost immediately and the child entry in the
process table is cleared. Thus no zombie is created.
filter_none
edit
close
play_arrow
link
brightness_4
code
// A C program to demonstrate handling of
// SIGCHLD signal to prevent Zombie processes.
#include<stdio.h>
#include<unistd.h>
#include<sys/wait.h>
#include<sys/types.h>
void func(int signum)
{
wait(NULL);
}
int main()
{
int i;
int pid = fork();
if (pid == 0)
for (i=0; i<20; i++)
printf("I am Child\n");
else
{
signal(SIGCHLD, func);
printf("I am Parent\n");
while(1);
}
}

chevron_right
filter_none
Output:

Here no any [a.out] defunct i.e. no any Zombie process is created.

Source
https://www.geeksforgeeks.org/zombie-processes-prevention/
✍
Write a Testimonial

Program for FCFS CPU Scheduling | Set 2 (Processes with different arrival times)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Program for FCFS CPU Scheduling | Set 2 (Processes with different arrival times) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
We have already discussed FCFS Scheduling of processes with same arrival time. In this post, scenario when processes have different arrival times are discussed. Given n processes with their burst times and arrival times,
the task is to find average waiting time and average turn around time using FCFS scheduling algorithm.
FIFO simply queues processes in the order they arrive in the ready queue. Here, the process that comes first will be executed first and next process will start only after the previous gets fully executed.
1. Completion Time: Time at which process completes its execution.
2. Turn Around Time: Time Difference between completion time and arrival time. Turn Around Time = Completion Time – Arrival Time
3. Waiting Time(W.T): Time Difference between turn around time and burst time.
Waiting Time = Turn Around Time – Burst Time

​
Process
P0
P1
P2
P3

Wait Time : Service Time
0 - 0
5 - 1
8 - 2
16 - 3

=
=
=
=

Arrival Time​
0​
4​
6​
13​

​
Average Wait Time: (0 + 4 + 6 + 13) / 4 = 5.75​

Service Time : Service time means amount of time after which a process can start execution. It is summation of burst time of previous processes (Processes that came before)

Changes in code as compare to code of FCFS with same arrival time:
To find waiting time: Time taken by all processes before the current process to be started (i.e. burst time of all previous processes) – arrival time of current process
wait_time[i] = (bt[0] + bt[1] +…… bt[i-1] ) – arrival_time[i]
Implementation:
​
1- Input the processes along with their burst time(bt)​
and arrival time(at)​
2- Find waiting time for all other processes i.e. for​
a given process i:​
wt[i] = (bt[0] + bt[1] +...... bt[i-1]) - at[i] ​
3- Now find turn around time ​
= waiting_time + burst_time for all processes​
4- Average waiting time = ​
total_waiting_time / no_of_processes​
5- Average turn around time = ​
total_turn_around_time / no_of_processes​

C++
filter_none
edit
close
play_arrow
link
brightness_4
code
// C++ program for implementation of FCFS
// scheduling with different arrival time
#include<iostream>
using namespace std;
// Function to find the waiting time for all
// processes
void findWaitingTime(int processes[], int n, int bt[],
int wt[], int at[])
{
int service_time[n];
service_time[0] = 0;
wt[0] = 0;
// calculating waiting time
for (int i = 1; i < n ; i++)
{
// Add burst time of previous processes
service_time[i] = service_time[i-1] + bt[i-1];
// Find waiting time for current process =
// sum - at[i]
wt[i] = service_time[i] - at[i];
//
//
//
if

If waiting time for a process is in negative
that means it is already in the ready queue
before CPU becomes idle so its waiting time is 0
(wt[i] < 0)
wt[i] = 0;

}
}
// Function to calculate turn around time
void findTurnAroundTime(int processes[], int n, int bt[],
int wt[], int tat[])
{
// Calculating turnaround time by adding bt[i] + wt[i]
for (int i = 0; i < n ; i++)
tat[i] = bt[i] + wt[i];
}
// Function to calculate average waiting and turn-around
// times.
void findavgTime(int processes[], int n, int bt[], int at[])
{
int wt[n], tat[n];
// Function to find waiting time of all processes
findWaitingTime(processes, n, bt, wt, at);
// Function to find turn around time for all processes
findTurnAroundTime(processes, n, bt, wt, tat);
// Display processes along with all details
cout << "Processes " << " Burst Time " << " Arrival Time "
<< " Waiting Time " << " Turn-Around Time "
<< " Completion Time \n";
int total_wt = 0, total_tat = 0;
for (int i = 0 ; i < n ; i++)
{
total_wt = total_wt + wt[i];
total_tat = total_tat + tat[i];
int compl_time = tat[i] + at[i];
cout << " " << i+1 << "\t\t" << bt[i] << "\t\t"
<< at[i] << "\t\t" << wt[i] << "\t\t "
<< tat[i] << "\t\t " << compl_time << endl;
}
cout <<
<<
cout <<
<<

"Average waiting time = "
(float)total_wt / (float)n;
"\nAverage turn around time = "
(float)total_tat / (float)n;

}
// Driver code
int main()
{
// Process id's
int processes[] = {1, 2, 3};
int n = sizeof processes / sizeof processes[0];
// Burst time of all processes
int burst_time[] = {5, 9, 6};
// Arrival time of all processes
int arrival_time[] = {0, 3, 6};
findavgTime(processes, n, burst_time, arrival_time);
return 0;
}

chevron_right

filter_none
Java
filter_none
edit
close
play_arrow
link
brightness_4
code
// Java program for implementation of FCFS
// scheduling with different arrival time
public class GFG{
// Function to find the waiting time for all
// processes
static void findWaitingTime(int processes[], int n, int bt[], int wt[], int at[])
{
int service_time[] = new int[n];
service_time[0] = 0;
wt[0] = 0;
// calculating waiting time
for (int i = 1; i < n ; i++)
{
// Add burst time of previous processes
service_time[i] = service_time[i-1] + bt[i-1];
// Find waiting time for current process =
// sum - at[i]
wt[i] = service_time[i] - at[i];
//
//
//
if

If waiting time for a process is in negative
that means it is already in the ready queue
before CPU becomes idle so its waiting time is 0
(wt[i] < 0)
wt[i] = 0;

}
}
// Function to calculate turn around time
static void findTurnAroundTime(int processes[], int n, int bt[],
int wt[], int tat[])
{
// Calculating turnaround time by adding bt[i] + wt[i]
for (int i = 0; i < n ; i++)
tat[i] = bt[i] + wt[i];
}
// Function to calculate average waiting and turn-around
// times.
static void findavgTime(int processes[], int n, int bt[], int at[])
{
int wt[] = new int[n], tat[] = new int[n];
// Function to find waiting time of all processes
findWaitingTime(processes, n, bt, wt, at);
// Function to find turn around time for all processes
findTurnAroundTime(processes, n, bt, wt, tat);
// Display processes along with all details
System.out.print("Processes " + " Burst Time " + " Arrival Time "
+ " Waiting Time " + " Turn-Around Time "
+ " Completion Time \n");
int total_wt = 0, total_tat = 0;
for (int i = 0 ; i < n ; i++)
{
total_wt = total_wt + wt[i];
total_tat = total_tat + tat[i];
int compl_time = tat[i] + at[i];
System.out.println(i+1 + "\t\t" + bt[i] + "\t\t"
+ at[i] + "\t\t" + wt[i] + "\t\t "
+ tat[i] + "\t\t " + compl_time);
}
System.out.print("Average waiting time = "
+ (float)total_wt / (float)n);
System.out.print("\nAverage turn around time = "
+ (float)total_tat / (float)n);
}
// Driver code
public static void main(String args[]) {
// Process id's
int processes[] = {1, 2, 3};
int n = processes.length;
// Burst time of all processes
int burst_time[] = {5, 9, 6};
// Arrival time of all processes
int arrival_time[] = {0, 3, 6};
findavgTime(processes, n, burst_time, arrival_time);
}
}
/*This code is contributed by PrinciRaj1992*/

chevron_right
filter_none
Python3
filter_none
edit
close
play_arrow
link
brightness_4
code
# Python3 program for implementation of FCFS
# scheduling with different arrival time
# Function to find the waiting time
# for all processes
def findWaitingTime(processes, n, bt, wt, at):
service_time = [0] * n
service_time[0] = 0
wt[0] = 0

# calculating waiting time
for i in range(1, n):
# Add burst time of previous processes
service_time[i] = (service_time[i - 1] +
bt[i - 1])
# Find waiting time for current
# process = sum - at[i]
wt[i] = service_time[i] - at[i]
# If waiting time for a process is in
# negative that means it is already
# in the ready queue before CPU becomes
# idle so its waiting time is 0
if (wt[i] < 0):
wt[i] = 0
# Function to calculate turn around time
def findTurnAroundTime(processes, n, bt, wt, tat):
# Calculating turnaround time by
# adding bt[i] + wt[i]
for i in range(n):
tat[i] = bt[i] + wt[i]
# Function to calculate average waiting
# and turn-around times.
def findavgTime(processes, n, bt, at):
wt = [0] * n
tat = [0] * n
# Function to find waiting time
# of all processes
findWaitingTime(processes, n, bt, wt, at)
# Function to find turn around time for
# all processes
findTurnAroundTime(processes, n, bt, wt, tat)
# Display processes along with all details
print("Processes
Burst Time
Arrival Time
Waiting",
"Time
Turn-Around Time Completion Time \n")
total_wt = 0
total_tat = 0
for i in range(n):
total_wt = total_wt + wt[i]
total_tat = total_tat + tat[i]
compl_time = tat[i] + at[i]
print(" ", i + 1, "\t\t", bt[i], "\t\t", at[i],
"\t\t", wt[i], "\t\t ", tat[i], "\t\t ", compl_time)
print("Average waiting time = %.5f "%(total_wt /n))
print("\nAverage turn around time = ", total_tat / n)
# Driver code
if __name__ =="__main__":
# Process id's
processes = [1, 2, 3]
n = 3
# Burst time of all processes
burst_time = [5, 9, 6]
# Arrival time of all processes
arrival_time = [0, 3, 6]
findavgTime(processes, n, burst_time,
arrival_time)
# This code is contributed
# Shubham Singh(SHUBHAMSINGH10)

chevron_right
filter_none
C#
filter_none
edit
close
play_arrow
link
brightness_4
code
// C# program for implementation of FCFS
// scheduling with different arrival time
using System;
public class GFG{
// Function to find the waiting time for all
// processes
static void findWaitingTime(int []processes, int n, int []bt, int []wt, int []at)
{
int []service_time = new int[n];
service_time[0] = 0;
wt[0] = 0;
// calculating waiting time
for (int i = 1; i < n ; i++)
{
// Add burst time of previous processes
service_time[i] = service_time[i-1] + bt[i-1];
// Find waiting time for current process =
// sum - at[i]
wt[i] = service_time[i] - at[i];
//
//
//
if

If waiting time for a process is in negative
that means it is already in the ready queue
before CPU becomes idle so its waiting time is 0
(wt[i] < 0)
wt[i] = 0;

}
}
// Function to calculate turn around time
static void findTurnAroundTime(int []processes, int n, int[] bt,
int []wt, int[] tat)
{
// Calculating turnaround time by adding bt[i] + wt[i]
for (int i = 0; i < n ; i++)
tat[i] = bt[i] + wt[i];
}

// Function to calculate average waiting and turn-around
// times.
static void findavgTime(int []processes, int n, int []bt, int []at)
{
int []wt = new int[n]; int []tat = new int[n];
// Function to find waiting time of all processes
findWaitingTime(processes, n, bt, wt, at);
// Function to find turn around time for all processes
findTurnAroundTime(processes, n, bt, wt, tat);
// Display processes along with all details
Console.Write("Processes " + " Burst Time " + " Arrival Time "
+ " Waiting Time " + " Turn-Around Time "
+ " Completion Time \n");
int total_wt = 0, total_tat = 0;
for (int i = 0 ; i < n ; i++)
{
total_wt = total_wt + wt[i];
total_tat = total_tat + tat[i];
int compl_time = tat[i] + at[i];
Console.WriteLine(i+1 + "\t\t" + bt[i] + "\t\t"
+ at[i] + "\t\t" + wt[i] + "\t\t "
+ tat[i] + "\t\t " + compl_time);
}
Console.Write("Average waiting time = "
+ (float)total_wt / (float)n);
Console.Write("\nAverage turn around time = "
+ (float)total_tat / (float)n);
}
// Driver code
public static void Main(String []args) {
// Process id's
int []processes = {1, 2, 3};
int n = processes.Length;
// Burst time of all processes
int []burst_time = {5, 9, 6};
// Arrival time of all processes
int []arrival_time = {0, 3, 6};
findavgTime(processes, n, burst_time, arrival_time);
}
}
// This code is contributed by Princi Singh

chevron_right
filter_none
Output:
Processes Burst Time Arrival Time
1
5
0
0
2
9
3
2
3
6
6
8
Average waiting time = 3.33333​
Average turn around time = 10.0​

Waiting Time Turn-Around Time
5
5​
11
14​
14
20​

Completion Time ​

Improved By : princiraj1992, SHUBHAMSINGH10, soumya7, princi singh

Source
https://www.geeksforgeeks.org/program-for-fcfs-cpu-scheduling-set-2-processes-with-different-arrival-times/
✍
Write a Testimonial

Virtual Memory | Questions
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Virtual Memory | Questions - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Advantages
Large virtual memory.
More efficient use of memory.
Unconstrained multiprogramming. There is no limit on degree of multiprogramming.
Disadvantages
Number of tables and amount of processor overhead for handling page interrupts are greater than in the case of the simple paged management techniques.
Due to lack of an explicit constraint on a job’s address space size.
A way to control Thrashing

Set the lower and upper bounds of page fault rate for each process. Using the above step, establish ‘acceptable’ page fault rate.
If actual rate is lower than lower bound, decrease the number of frames
If actual rate is larger than upper bound, increase the number of frames.
Q1. Virtual memory is
(a) Large secondary memory
(b) Large main memory
(c) Illusion of large main memory
(d) None of the above
Answer: (c)
Explanation: Virtual memory is illusion of large main memory.
Q2. Thrashing occurs when

(a)When a page fault occurs
(b) Processes on system frequently access pages not memory
(c) Processes on system are in running state
(d) Processes on system are in waiting state
Answer: (b)
Explanation: Thrashing occurs when processes on system require more memory than it has. If processes do not have “enough” pages, the page fault rate is very high. This leads to:
– low CPU utilization
– operating system spends most of its time swapping to disk
The above situation is called thrashing
Q3. A computer system supports 32-bit virtual addresses as well as 32-bit physical addresses. Since the virtual address space is of the same size as the physical address space, the operating system designers decide to get
rid of the virtual memory entirely. Which one of the following is true?
(a) Efficient implementation of multi-user support is no longer possible
(b) The processor cache organization can be made more efficient now
(c) Hardware support for memory management is no longer needed
(d) CPU scheduling can be made more efficient now
Answer: (c)
Explanation: For supporting virtual memory, special hardware support is needed from Memory Management Unit. Since operating system designers decide to get rid of the virtual memory entirely, hardware support for
memory management is no longer needed.
This article is contributed by Mithlesh Upadhyay

Source
https://www.geeksforgeeks.org/virtual-memory-questions/
✍
Write a Testimonial

Program for Round Robin scheduling | Set 1
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Program for Round Robin scheduling | Set 1 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Round Robin is a CPU scheduling algorithm where each process is assigned a fixed time slot in a cyclic way.
It is simple, easy to implement, and starvation-free as all processes get fair share of CPU.
One of the most commonly used technique in CPU scheduling as a core.
It is preemptive as processes are assigned CPU only for a fixed slice of time at most.
The disadvantage of it is more overhead of context switching.
Illustration:

How to compute below times in Round Robin using a program?

1. Completion Time: Time at which process completes its execution.
2. Turn Around Time: Time Difference between completion time and arrival time. Turn Around Time = Completion Time – Arrival Time
3. Waiting Time(W.T): Time Difference between turn around time and burst time.
Waiting Time = Turn Around Time – Burst Time
In this post, we have assumed arrival times as 0, so turn around and completion times are same.
The tricky part is to compute waiting times. Once waiting times are computed, turn around times can be quickly computed.
Steps to find waiting times of all processes:
​
1- Create an array rem_bt[] to keep track of remaining​
burst time of processes. This array is initially a ​
copy of bt[] (burst times array)​
2- Create another array wt[] to store waiting times​
of processes. Initialize this array as 0.​
3- Initialize time : t = 0​
4- Keep traversing the all processes while all processes​
are not done. Do following for i'th process if it is​
not done yet.​
a- If rem_bt[i] > quantum​
(i) t = t + quantum​
(ii) bt_rem[i] -= quantum;​
c- Else // Last cycle for this process​
(i) t = t + bt_rem[i];​
(ii) wt[i] = t - bt[i]​
(ii) bt_rem[i] = 0; // This process is over​

Once we have waiting times, we can compute turn around time tat[i] of a process as sum of waiting and burst times, i.e., wt[i] + bt[i]

Below is implementation of above steps.
C/C++
filter_none
edit
close
play_arrow
link
brightness_4
code
// C++ program for implementation of RR scheduling
#include<iostream>
using namespace std;
// Function to find the waiting time for all
// processes
void findWaitingTime(int processes[], int n,
int bt[], int wt[], int quantum)
{
// Make a copy of burst times bt[] to store remaining
// burst times.
int rem_bt[n];
for (int i = 0 ; i < n ; i++)
rem_bt[i] = bt[i];
int t = 0; // Current time
// Keep traversing processes in round robin manner
// until all of them are not done.
while (1)
{
bool done = true;
// Traverse all processes one by one repeatedly
for (int i = 0 ; i < n; i++)
{
// If burst time of a process is greater than 0
// then only need to process further
if (rem_bt[i] > 0)
{
done = false; // There is a pending process
if (rem_bt[i] > quantum)
{
// Increase the value of t i.e. shows
// how much time a process has been processed
t += quantum;
// Decrease the burst_time of current process
// by quantum
rem_bt[i] -= quantum;
}
// If burst time is smaller than or equal to
// quantum. Last cycle for this process
else
{
// Increase the value of t i.e. shows
// how much time a process has been processed
t = t + rem_bt[i];
// Waiting time is current time minus time
// used by this process
wt[i] = t - bt[i];
// As the process gets fully executed
// make its remaining burst time = 0
rem_bt[i] = 0;
}
}
}
// If all processes are done
if (done == true)
break;
}
}
// Function to calculate turn around time
void findTurnAroundTime(int processes[], int n,
int bt[], int wt[], int tat[])
{
// calculating turnaround time by adding
// bt[i] + wt[i]
for (int i = 0; i < n ; i++)
tat[i] = bt[i] + wt[i];
}
// Function to calculate average time
void findavgTime(int processes[], int n, int bt[],
int quantum)
{
int wt[n], tat[n], total_wt = 0, total_tat = 0;
// Function to find waiting time of all processes
findWaitingTime(processes, n, bt, wt, quantum);
// Function to find turn around time for all processes
findTurnAroundTime(processes, n, bt, wt, tat);
// Display processes along with all details
cout << "Processes "<< " Burst time "
<< " Waiting time " << " Turn around time\n";
// Calculate total waiting time and total turn
// around time
for (int i=0; i<n; i++)
{
total_wt = total_wt + wt[i];
total_tat = total_tat + tat[i];
cout << " " << i+1 << "\t\t" << bt[i] <<"\t "
<< wt[i] <<"\t\t " << tat[i] <<endl;
}
cout <<
<<
cout <<
<<

"Average waiting time = "
(float)total_wt / (float)n;
"\nAverage turn around time = "
(float)total_tat / (float)n;

}
// Driver code
int main()
{
// process id's
int processes[] = { 1, 2, 3};
int n = sizeof processes / sizeof processes[0];
// Burst time of all processes
int burst_time[] = {10, 5, 8};
// Time quantum

int quantum = 2;
findavgTime(processes, n, burst_time, quantum);
return 0;
}

chevron_right
filter_none
Java
filter_none
edit
close
play_arrow
link
brightness_4
code
// Java program for implementation of RR scheduling
public class GFG
{
// Method to find the waiting time for all
// processes
static void findWaitingTime(int processes[], int n,
int bt[], int wt[], int quantum)
{
// Make a copy of burst times bt[] to store remaining
// burst times.
int rem_bt[] = new int[n];
for (int i = 0 ; i < n ; i++)
rem_bt[i] = bt[i];
int t = 0; // Current time
// Keep traversing processes in round robin manner
// until all of them are not done.
while(true)
{
boolean done = true;
// Traverse all processes one by one repeatedly
for (int i = 0 ; i < n; i++)
{
// If burst time of a process is greater than 0
// then only need to process further
if (rem_bt[i] > 0)
{
done = false; // There is a pending process
if (rem_bt[i] > quantum)
{
// Increase the value of t i.e. shows
// how much time a process has been processed
t += quantum;
// Decrease the burst_time of current process
// by quantum
rem_bt[i] -= quantum;
}
// If burst time is smaller than or equal to
// quantum. Last cycle for this process
else
{
// Increase the value of t i.e. shows
// how much time a process has been processed
t = t + rem_bt[i];
// Waiting time is current time minus time
// used by this process
wt[i] = t - bt[i];
// As the process gets fully executed
// make its remaining burst time = 0
rem_bt[i] = 0;
}
}
}
// If all processes are done
if (done == true)
break;
}
}
// Method to calculate turn around time
static void findTurnAroundTime(int processes[], int n,
int bt[], int wt[], int tat[])
{
// calculating turnaround time by adding
// bt[i] + wt[i]
for (int i = 0; i < n ; i++)
tat[i] = bt[i] + wt[i];
}
// Method to calculate average time
static void findavgTime(int processes[], int n, int bt[],
int quantum)
{
int wt[] = new int[n], tat[] = new int[n];
int total_wt = 0, total_tat = 0;
// Function to find waiting time of all processes
findWaitingTime(processes, n, bt, wt, quantum);
// Function to find turn around time for all processes
findTurnAroundTime(processes, n, bt, wt, tat);
// Display processes along with all details
System.out.println("Processes " + " Burst time " +
" Waiting time " + " Turn around time");
// Calculate total waiting time and total turn
// around time
for (int i=0; i<n; i++)
{
total_wt = total_wt + wt[i];
total_tat = total_tat + tat[i];
System.out.println(" " + (i+1) + "\t\t" + bt[i] +"\t " +
wt[i] +"\t\t " + tat[i]);
}
System.out.println("Average waiting time = " +
(float)total_wt / (float)n);
System.out.println("Average turn around time = " +
(float)total_tat / (float)n);
}
// Driver Method
public static void main(String[] args)
{

// process id's
int processes[] = { 1, 2, 3};
int n = processes.length;
// Burst time of all processes
int burst_time[] = {10, 5, 8};
// Time quantum
int quantum = 2;
findavgTime(processes, n, burst_time, quantum);
}
}

chevron_right
filter_none
Python3
filter_none
edit
close
play_arrow
link
brightness_4
code
# Python3 program for implementation of
# RR scheduling
# Function to find the waiting time
# for all processes
def findWaitingTime(processes, n, bt,
wt, quantum):
rem_bt = [0] * n
# Copy the burst time into rt[]
for i in range(n):
rem_bt[i] = bt[i]
t = 0 # Current time
# Keep traversing processes in round
# robin manner until all of them are
# not done.
while(1):
done = True
# Traverse all processes one by
# one repeatedly
for i in range(n):
# If burst time of
# than 0 then only
if (rem_bt[i] > 0)
done = False #

a process is greater
need to process further
:
There is a pending process

if (rem_bt[i] > quantum) :
# Increase the value of t i.e. shows
# how much time a process has been processed
t += quantum
# Decrease the burst_time of current
# process by quantum
rem_bt[i] -= quantum
# If burst time is smaller than or equal
# to quantum. Last cycle for this process
else:
# Increase the value of t i.e. shows
# how much time a process has been processed
t = t + rem_bt[i]
# Waiting time is current time minus
# time used by this process
wt[i] = t - bt[i]
# As the process gets fully executed
# make its remaining burst time = 0
rem_bt[i] = 0
# If all processes are done
if (done == True):
break
# Function to calculate turn around time
def findTurnAroundTime(processes, n, bt, wt, tat):
# Calculating turnaround time
for i in range(n):
tat[i] = bt[i] + wt[i]
# Function to calculate average waiting
# and turn-around times.
def findavgTime(processes, n, bt, quantum):
wt = [0] * n
tat = [0] * n
# Function to find waiting time
# of all processes
findWaitingTime(processes, n, bt,
wt, quantum)
# Function to find turn around time
# for all processes
findTurnAroundTime(processes, n, bt,
wt, tat)
# Display processes along with all details
print("Processes
Burst Time
Waiting",
"Time
Turn-Around Time")
total_wt = 0
total_tat = 0
for i in range(n):
total_wt = total_wt + wt[i]
total_tat = total_tat + tat[i]
print(" ", i + 1, "\t\t", bt[i],
"\t\t", wt[i], "\t\t", tat[i])
print("\nAverage waiting time = %.5f "%(total_wt /n) )
print("Average turn around time = %.5f "% (total_tat / n))
# Driver code
if __name__ =="__main__":
# Process id's
proc = [1, 2, 3]
n = 3
# Burst time of all processes

burst_time = [10, 5, 8]
# Time quantum
quantum = 2;
findavgTime(proc, n, burst_time, quantum)
# This code is contributed by
# Shubham Singh(SHUBHAMSINGH10)

chevron_right
filter_none
C#
filter_none
edit
close
play_arrow
link
brightness_4
code
// C# program for implementation of RR
// scheduling
using System;
public class GFG {
// Method to find the waiting time
// for all processes
static void findWaitingTime(int []processes,
int n, int []bt, int []wt, int quantum)
{
// Make a copy of burst times bt[] to
// store remaining burst times.
int []rem_bt = new int[n];
for (int i = 0 ; i < n ; i++)
rem_bt[i] = bt[i];
int t = 0; // Current time
// Keep traversing processes in round
// robin manner until all of them are
// not done.
while(true)
{
bool done = true;
// Traverse all processes one by
// one repeatedly
for (int i = 0 ; i < n; i++)
{
// If burst time of a process
// is greater than 0 then only
// need to process further
if (rem_bt[i] > 0)
{
// There is a pending process
done = false;
if (rem_bt[i] > quantum)
{
// Increase the value of t i.e.
// shows how much time a process
// has been processed
t += quantum;
// Decrease the burst_time of
// current process by quantum
rem_bt[i] -= quantum;
}
// If burst time is smaller than
// or equal to quantum. Last cycle
// for this process
else
{
// Increase the value of t i.e.
// shows how much time a process
// has been processed
t = t + rem_bt[i];
// Waiting time is current
// time minus time used by
// this process
wt[i] = t - bt[i];
// As the process gets fully
// executed make its remaining
// burst time = 0
rem_bt[i] = 0;
}
}
}
// If all processes are done
if (done == true)
break;
}
}
// Method to calculate turn around time
static void findTurnAroundTime(int []processes,
int n, int []bt, int []wt, int []tat)
{
// calculating turnaround time by adding
// bt[i] + wt[i]
for (int i = 0; i < n ; i++)
tat[i] = bt[i] + wt[i];
}
// Method to calculate average time
static void findavgTime(int []processes, int n,
int []bt, int quantum)
{
int []wt = new int[n];
int []tat = new int[n];
int total_wt = 0, total_tat = 0;
// Function to find waiting time of
// all processes
findWaitingTime(processes, n, bt, wt, quantum);
// Function to find turn around time
// for all processes
findTurnAroundTime(processes, n, bt, wt, tat);
// Display processes along with

// all details
Console.WriteLine("Processes " + " Burst time " +
" Waiting time " + " Turn around time");
// Calculate total waiting time and total turn
// around time
for (int i = 0; i < n; i++)
{
total_wt = total_wt + wt[i];
total_tat = total_tat + tat[i];
Console.WriteLine(" " + (i+1) + "\t\t" + bt[i]
+ "\t " + wt[i] +"\t\t " + tat[i]);
}
Console.WriteLine("Average waiting time = " +
(float)total_wt / (float)n);
Console.Write("Average turn around time = " +
(float)total_tat / (float)n);
}
// Driver Method
public static void Main()
{
// process id's
int []processes = { 1, 2, 3};
int n = processes.Length;
// Burst time of all processes
int []burst_time = {10, 5, 8};
// Time quantum
int quantum = 2;
findavgTime(processes, n, burst_time, quantum);
}
}
// This code is contributed by nitin mittal.

chevron_right
filter_none
Output:
​
Processes Burst time Waiting time
1
10
13
23​
2
5
10
15​
3
8
13
21​
Average waiting time = 12​
Average turn around time = 19.6667​

Turn around time​

Improved By : nitin mittal, SHUBHAMSINGH10

Source
https://www.geeksforgeeks.org/program-round-robin-scheduling-set-1/
✍
Write a Testimonial

Program for Shortest Job First (or SJF) CPU Scheduling | Set 1 (Non- preemptive)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Program for Shortest Job First (or SJF) CPU Scheduling | Set 1 (Non- preemptive) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Shortest job first (SJF) or shortest job next, is a scheduling policy that selects the waiting process with the smallest execution time to execute next. SJN is a non-preemptive algorithm.
Shortest Job first has the advantage of having a minimum average waiting time among all scheduling algorithms.
It is a Greedy Algorithm.
It may cause starvation if shorter processes keep coming. This problem can be solved using the concept of aging.
It is practically infeasible as Operating System may not know burst time and therefore may not sort them. While it is not possible to predict execution time, several methods can be used to estimate the execution time
for a job, such as a weighted average of previous execution times. SJF can be used in specialized environments where accurate estimates of running time are available.
Algorithm:
1. Sort all the process according to the arrival time.
2. Then select that process which has minimum arrival time and minimum Burst time.
3. After completion of process make a pool of process which after till the completion of previous process and select that process among the pool which is having minimum Burst time.

How to compute below times in SJF using a program?
1. Completion Time: Time at which process completes its execution.
2. Turn Around Time: Time Difference between completion time and arrival time. Turn Around Time = Completion Time – Arrival Time
3. Waiting Time(W.T): Time Difference between turn around time and burst time.
Waiting Time = Turn Around Time – Burst Time
In this post, we have assumed arrival times as 0, so turn around and completion times are same.
filter_none
edit
close
play_arrow
link
brightness_4
code
// C++ program to implement Shortest Job first with Arrival Time
#include<iostream>
using namespace std;
int mat[10][6];
void swap(int *a, int *b)
{
int temp = *a;
*a = *b;
*b = temp;
}
void arrangeArrival(int num, int mat[][6])
{
for(int i=0; i<num; i++)
{
for(int j=0; j<num-i-1; j++)
{
if(mat[j][1] > mat[j+1][1])
{
for(int k=0; k<5; k++)
{
swap(mat[j][k], mat[j+1][k]);
}
}
}
}
}
void completionTime(int num, int mat[][6])
{
int temp, val;
mat[0][3] = mat[0][1] + mat[0][2];
mat[0][5] = mat[0][3] - mat[0][1];
mat[0][4] = mat[0][5] - mat[0][2];
for(int i=1; i<num; i++)
{
temp = mat[i-1][3];
int low = mat[i][2];
for(int j=i; j<num; j++)
{
if(temp >= mat[j][1] && low >= mat[j][2])
{
low = mat[j][2];
val = j;
}
}
mat[val][3] = temp + mat[val][2];
mat[val][5] = mat[val][3] - mat[val][1];
mat[val][4] = mat[val][5] - mat[val][2];
for(int k=0; k<6; k++)
{
swap(mat[val][k], mat[i][k]);
}
}
}
int main()
{
int num, temp;
cout<<"Enter number of Process: ";
cin>>num;
cout<<"...Enter the process ID...\n";
for(int i=0; i<num; i++)
{
cout<<"...Process "<<i+1<<"...\n";
cout<<"Enter Process Id: ";
cin>>mat[i][0];
cout<<"Enter Arrival Time: ";
cin>>mat[i][1];
cout<<"Enter Burst Time: ";

cin>>mat[i][2];
}
cout<<"Before Arrange...\n";
cout<<"Process ID\tArrival Time\tBurst Time\n";
for(int i=0; i<num; i++)
{
cout<<mat[i][0]<<"\t\t"<<mat[i][1]<<"\t\t"<<mat[i][2]<<"\n";
}
arrangeArrival(num, mat);
completionTime(num, mat);
cout<<"Final Result...\n";
cout<<"Process ID\tArrival Time\tBurst Time\tWaiting Time\tTurnaround Time\n";
for(int i=0; i<num; i++)
{
cout<<mat[i][0]<<"\t\t"<<mat[i][1]<<"\t\t"<<mat[i][2]<<"\t\t"<<mat[i][4]<<"\t\t"<<mat[i][5]<<"\n";
}
}

chevron_right
filter_none
Output:
​
Process ID
Arrival Time
1
2
2
0
3
4
4
5
Final Result...​
Process ID
Arrival Time
2
0
3
4
1
2
4
5

Burst Time​
3​
4​
2​
4​
Burst Time
4
2
3
4

Waiting Time
0
0
4
4

Turnaround Time​
4​
2​
7​
8​

In Set-2 we will discuss the preemptive version of SJF i.e. Shortest Remaining Time First

Improved By : anish3007, msujawal

Source
https://www.geeksforgeeks.org/program-for-shortest-job-first-or-sjf-cpu-scheduling-set-1-non-preemptive/
✍
Write a Testimonial

Program for FCFS CPU Scheduling | Set 1
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Program for FCFS CPU Scheduling | Set 1 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Given n processes with their burst times, the task is to find average waiting time and average turn around time using FCFS scheduling algorithm.
First in, first out (FIFO), also known as first come, first served (FCFS), is the simplest scheduling algorithm. FIFO simply queues processes in the order that they arrive in the ready queue.
In this, the process that comes first will be executed first and next process starts only after the previous gets fully executed.
Here we are considering that arrival time for all processes is 0.
How to compute below times in Round Robin using a program?
1. Completion Time: Time at which process completes its execution.
2. Turn Around Time: Time Difference between completion time and arrival time. Turn Around Time = Completion Time – Arrival Time
3. Waiting Time(W.T): Time Difference between turn around time and burst time.
Waiting Time = Turn Around Time – Burst Time

In this post, we have assumed arrival times as 0, so turn around and completion times are same.

Implementation:
​
1234-

Input the processes along with their burst time (bt).​
Find waiting time (wt) for all processes.​
As first process that comes need not to wait so ​
waiting time for process 1 will be 0 i.e. wt[0] = 0.​
Find waiting time for all other processes i.e. for​
process i -> ​
wt[i] = bt[i-1] + wt[i-1] .​

567-

Find turnaround time = waiting_time + burst_time ​
for all processes.​
Find average waiting time = ​
total_waiting_time / no_of_processes.​
Similarly, find average turnaround time = ​
total_turn_around_time / no_of_processes.​

C++
filter_none
edit
close
play_arrow
link
brightness_4
code
// C++ program for implementation of FCFS
// scheduling
#include<iostream>
using namespace std;
// Function to find the waiting time for all
// processes
void findWaitingTime(int processes[], int n,
int bt[], int wt[])
{
// waiting time for first process is 0
wt[0] = 0;
// calculating waiting time
for (int i = 1; i < n ; i++ )
wt[i] = bt[i-1] + wt[i-1] ;
}
// Function to calculate turn around time
void findTurnAroundTime( int processes[], int n,
int bt[], int wt[], int tat[])
{
// calculating turnaround time by adding
// bt[i] + wt[i]
for (int i = 0; i < n ; i++)
tat[i] = bt[i] + wt[i];
}
//Function to calculate average time
void findavgTime( int processes[], int n, int bt[])
{
int wt[n], tat[n], total_wt = 0, total_tat = 0;
//Function to find waiting time of all processes
findWaitingTime(processes, n, bt, wt);
//Function to find turn around time for all processes
findTurnAroundTime(processes, n, bt, wt, tat);
//Display processes along with all details
cout << "Processes "<< " Burst time "
<< " Waiting time " << " Turn around time\n";
// Calculate total waiting time and total turn
// around time
for (int i=0; i<n; i++)
{
total_wt = total_wt + wt[i];
total_tat = total_tat + tat[i];
cout << "
" << i+1 << "\t\t" << bt[i] <<"\t
<< wt[i] <<"\t\t " << tat[i] <<endl;
}
cout <<
<<
cout <<
<<

"Average waiting time = "
(float)total_wt / (float)n;
"\nAverage turn around time = "
(float)total_tat / (float)n;

}
// Driver code
int main()
{
//process id's
int processes[] = { 1, 2, 3};
int n = sizeof processes / sizeof processes[0];
//Burst time of all processes
int burst_time[] = {10, 5, 8};
findavgTime(processes, n,
return 0;

burst_time);

}

chevron_right
filter_none
C
filter_none
edit
close
play_arrow
link
brightness_4
code
// C program for implementation of FCFS
// scheduling
#include<stdio.h>
// Function to find the waiting time for all
// processes
void findWaitingTime(int processes[], int n,
int bt[], int wt[])
{
// waiting time for first process is 0
wt[0] = 0;
// calculating waiting time
for (int i = 1; i < n ; i++ )
wt[i] = bt[i-1] + wt[i-1] ;
}
// Function to calculate turn around time
void findTurnAroundTime( int processes[], int n,
int bt[], int wt[], int tat[])
{
// calculating turnaround time by adding
// bt[i] + wt[i]
for (int i = 0; i < n ; i++)
tat[i] = bt[i] + wt[i];

"

}
//Function to calculate average time
void findavgTime( int processes[], int n, int bt[])
{
int wt[n], tat[n], total_wt = 0, total_tat = 0;
//Function to find waiting time of all processes
findWaitingTime(processes, n, bt, wt);
//Function to find turn around time for all processes
findTurnAroundTime(processes, n, bt, wt, tat);
//Display processes along with all details
printf("Processes
Burst time
Waiting time

Turn around time\n");

// Calculate total waiting time and total turn
// around time
for (int i=0; i<n; i++)
{
total_wt = total_wt + wt[i];
total_tat = total_tat + tat[i];
printf("
%d ",(i+1));
printf("
%d ", bt[i] );
printf("
%d",wt[i] );
printf("
%d\n",tat[i] );
}
int s=(float)total_wt / (float)n;
int t=(float)total_tat / (float)n;
printf("Average waiting time = %d",s);
printf("\n");
printf("Average turn around time = %d ",t);
}
// Driver code
int main()
{
//process id's
int processes[] = { 1, 2, 3};
int n = sizeof processes / sizeof processes[0];
//Burst time of all processes
int burst_time[] = {10, 5, 8};
findavgTime(processes, n, burst_time);
return 0;
}
// This code is contributed by Shivi_Aggarwal

chevron_right
filter_none
Java
filter_none
edit
close
play_arrow
link
brightness_4
code
// Java program for implementation of FCFS
// scheduling
import java.text.ParseException;
class GFG {
// Function to find the waiting time for all
// processes
static void findWaitingTime(int processes[], int n,
int bt[], int wt[]) {
// waiting time for first process is 0
wt[0] = 0;
// calculating waiting time
for (int i = 1; i < n; i++) {
wt[i] = bt[i - 1] + wt[i - 1];
}
}
// Function to calculate turn around time
static void findTurnAroundTime(int processes[], int n,
int bt[], int wt[], int tat[]) {
// calculating turnaround time by adding
// bt[i] + wt[i]
for (int i = 0; i < n; i++) {
tat[i] = bt[i] + wt[i];
}
}
//Function to calculate average time
static void findavgTime(int processes[], int n, int bt[]) {
int wt[] = new int[n], tat[] = new int[n];
int total_wt = 0, total_tat = 0;
//Function to find waiting time of all processes
findWaitingTime(processes, n, bt, wt);
//Function to find turn around time for all processes
findTurnAroundTime(processes, n, bt, wt, tat);
//Display processes along with all details
System.out.printf("Processes Burst time Waiting"
+" time Turn around time\n");
// Calculate total waiting time and total turn
// around time
for (int i = 0; i < n; i++) {
total_wt = total_wt + wt[i];
total_tat = total_tat + tat[i];
System.out.printf(" %d ", (i + 1));
System.out.printf("
%d ", bt[i]);
System.out.printf("
%d", wt[i]);
System.out.printf("
%d\n", tat[i]);
}
float s = (float)total_wt /(float) n;
int t = total_tat / n;
System.out.printf("Average waiting time = %f", s);
System.out.printf("\n");
System.out.printf("Average turn around time = %d ", t);
}
// Driver code
public static void main(String[] args) throws ParseException {
//process id's
int processes[] = {1, 2, 3};
int n = processes.length;
//Burst time of all processes
int burst_time[] = {10, 5, 8};

findavgTime(processes, n, burst_time);
}
}
// This code is contributed by 29ajaykumar

chevron_right
filter_none
Python 3
filter_none
edit
close
play_arrow
link
brightness_4
code
# Python3 program for implementation
# of FCFS scheduling
# Function to find the waiting
# time for all processes
def findWaitingTime(processes, n,
bt, wt):
# waiting time for
# first process is 0
wt[0] = 0
# calculating waiting time
for i in range(1, n ):
wt[i] = bt[i - 1] + wt[i - 1]
# Function to calculate turn
# around time
def findTurnAroundTime(processes, n,
bt, wt, tat):
# calculating turnaround
# time by adding bt[i] + wt[i]
for i in range(n):
tat[i] = bt[i] + wt[i]
# Function to calculate
# average time
def findavgTime( processes, n, bt):
wt = [0] * n
tat = [0] * n
total_wt = 0
total_tat = 0
# Function to find waiting
# time of all processes
findWaitingTime(processes, n, bt, wt)
# Function to find turn around
# time for all processes
findTurnAroundTime(processes, n,
bt, wt, tat)
# Display processes along
# with all details
print( "Processes Burst time " +
" Waiting time " +
" Turn around time")
# Calculate total waiting time
# and total turn around time
for i in range(n):
total_wt = total_wt + wt[i]
total_tat = total_tat + tat[i]
print(" " + str(i + 1) + "\t\t" +
str(bt[i]) + "\t " +
str(wt[i]) + "\t\t " +
str(tat[i]))
print( "Average waiting time = "+
str(total_wt / n))
print("Average turn around time = "+
str(total_tat / n))
# Driver code
if __name__ =="__main__":
# process id's
processes = [ 1, 2, 3]
n = len(processes)
# Burst time of all processes
burst_time = [10, 5, 8]
findavgTime(processes, n, burst_time)
# This code is contributed
# by ChitraNayal

chevron_right
filter_none
C#
filter_none
edit
close
play_arrow
link
brightness_4
code
// C# program for implementation of FCFS
// scheduling
using System;
class GFG
{
// Function to find the waiting time for all
// processes
static void findWaitingTime(int []processes, int n,
int []bt, int[] wt)
{

// waiting time for first process is 0
wt[0] = 0;
// calculating waiting time
for (int i = 1; i < n; i++)
{
wt[i] = bt[i - 1] + wt[i - 1];
}
}
// Function to calculate turn around time
static void findTurnAroundTime(int []processes, int n,
int []bt, int []wt, int []tat) {
// calculating turnaround time by adding
// bt[i] + wt[i]
for (int i = 0; i < n; i++)
{
tat[i] = bt[i] + wt[i];
}
}
// Function to calculate average time
static void findavgTime(int []processes, int n, int []bt)
{
int []wt = new int[n];
int []tat = new int[n];
int total_wt = 0, total_tat = 0;
//Function to find waiting time of all processes
findWaitingTime(processes, n, bt, wt);
//Function to find turn around time for all processes
findTurnAroundTime(processes, n, bt, wt, tat);
//Display processes along with all details
Console.Write("Processes Burst time Waiting"
+" time Turn around time\n");
// Calculate total waiting time and total turn
// around time
for (int i = 0; i < n; i++)
{
total_wt = total_wt + wt[i];
total_tat = total_tat + tat[i];
Console.Write(" {0} ", (i + 1));
Console.Write("
{0} ", bt[i]);
Console.Write("
{0}", wt[i]);
Console.Write("
{0}\n", tat[i]);
}
float s = (float)total_wt /(float) n;
int t = total_tat / n;
Console.Write("Average waiting time = {0}", s);
Console.Write("\n");
Console.Write("Average turn around time = {0} ", t);
}
// Driver code
public static void Main(String[] args)
{
// process id's
int []processes = {1, 2, 3};
int n = processes.Length;
// Burst time of all processes
int []burst_time = {10, 5, 8};
findavgTime(processes, n, burst_time);
}
}
// This code contributed by Rajput-Ji

chevron_right
filter_none
Output:
​
Processes Burst time Waiting time
1
10
0
10​
2
5
10
15​
3
8
15
23​
Average waiting time = 8.33333​
Average turn around time = 16​

Turn around time​

Important Points:
1. Non-preemptive
2. Average Waiting Time is not optimal
3. Cannot utilize resources in parallel : Results in Convoy effect (Consider a situation when many IO bound processes are there and one CPU bound process. The IO bound processes have to wait for CPU bound
process when CPU bound process acquires CPU. The IO bound process could have better taken CPU for some time, then used IO devices).
Source : http://web.cse.ohio-state.edu/~agrawal/660/Slides/jan18.pdf
In Set-2 we will be discussing the processes with different arrival time.

Improved By : Shivi_Aggarwal, chitranayal, 29AjayKumar, Rajput-Ji

Source
https://www.geeksforgeeks.org/program-for-fcfs-cpu-scheduling-set-1/
✍
Write a Testimonial

File Systems in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ File Systems in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
A file is a collection of related information that is recorded on secondary storage. Or file is a collection of logically related entities. From user’s perspective a file is the smallest allotment of logical secondary storage.

Attributes Types Operations
Name
Type
Size
Creation Data
Author
Last Modified
protection

File type

Doc
Exe
Jpg
Xis
C
Java
class

Create
Open
Read
Write
Append
Truncate
Delete
Close

Usual extension

Function

Executable
exe, com, bin
Read to run machine language program
Object
obj, o
Compiled, machine language not linked
Source Code C, java, pas, asm, a Source code in various languages
Batch
bat, sh
Commands to the command interpreter
Text
txt, doc
Textual data, documents
Word Processor wp, tex, rrf, doc
Various word processor formats
Archive
arc, zip, tar
Related files grouped into one compressed file
Multimedia
mpeg, mov, rm
For containing audio/video information
FILE DIRECTORIES:
Collection of files is a file directory. The directory contains information about the files, including attributes, location and ownership. Much of this information, especially that is concerned with storage, is managed by the
operating system. The directory is itself a file, accessible by various file management routines.
Information contained in a device directory are:
Name
Type
Address
Current length
Maximum length
Date last accessed
Date last updated
Owner id
Protection information
Operation performed on directory are:

Search for a file
Create a file
Delete a file
List a directory
Rename a file
Traverse the file system
Advantages of maintaining directories are:
Efficiency: A file can be located more quickly.
Naming: It becomes convenient for users as two users can have same name for different files or may have different name for same file.
Grouping: Logical grouping of files can be done by properties e.g. all java programs, all games etc.
SINGLE-LEVEL DIRECTORY
In this a single directory is maintained for all the users.
Naming problem: Users cannot have same name for two files.
Grouping problem: Users cannot group files according to their need.

TWO-LEVEL DIRECTORY
In this separate directories for each user is maintained.
Path name:Due to two levels there is a path name for every file to locate that file.
Now,we can have same file name for different user.
Searching is efficient in this method.

TREE-STRUCTURED DIRECTORY :
Directory is maintained in the form of a tree. Searching is efficient and also there is grouping capability. We have absolute or relative path name for a file.

FILE ALLOCATION METHODS
1. Continuous Allocation: A single continuous set of blocks is allocated to a file at the time of file creation. Thus, this is a pre-allocation strategy, using variable size portions. The file allocation table needs just a single
entry for each file, showing the starting block and the length of the file. This method is best from the point of view of the individual sequential file. Multiple blocks can be read in at a time to improve I/O performance for
sequential processing. It is also easy to retrieve a single block. For example, if a file starts at block b, and the ith block of the file is wanted, its location on secondary storage is simply b+i-1.

Disadvantage
External fragmentation will occur, making it difficult to find contiguous blocks of space of sufficient length. Compaction algorithm will be necessary to free up additional space on disk.
Also, with pre-allocation, it is necessary to declare the size of the file at the time of creation.
2. Linked Allocation(Non-contiguous allocation) : Allocation is on an individual block basis. Each block contains a pointer to the next block in the chain. Again the file table needs just a single entry for each file,
showing the starting block and the length of the file. Although pre-allocation is possible, it is more common simply to allocate blocks as needed. Any free block can be added to the chain. The blocks need not be
continuous. Increase in file size is always possible if free disk block is available. There is no external fragmentation because only one block at a time is needed but there can be internal fragmentation but it exists only in the
last disk block of file.
Disadvantage:
Internal fragmentation exists in last disk block of file.
There is an overhead of maintaining the pointer in every disk block.
If the pointer of any disk block is lost, the file will be truncated.
It supports only the sequencial access of files.
3. Indexed Allocation:
It addresses many of the problems of contiguous and chained allocation. In this case, the file allocation table contains a separate one-level index for each file: The index has one entry for each block allocated to the file.
Allocation may be on the basis of fixed-size blocks or variable-sized blocks. Allocation by blocks eliminates external fragmentation, whereas allocation by variable-size blocks improves locality. This allocation technique
supports both sequential and direct access to the file and thus is the most popular form of file allocation.

Disk Free Space Management
Just as the space that is allocated to files must be managed ,so the space that is not currently allocated to any file must be managed. To perform any of the file allocation techniques,it is necessary to know what blocks on
the disk are available. Thus we need a disk allocation table in addition to a file allocation table.The following are the approaches used for free space management.
1. Bit Tables : This method uses a vector containing one bit for each block on the disk. Each entry for a 0 corresponds to a free block and each 1 corresponds to a block in use.
For example: 00011010111100110001
In this vector every bit correspond to a particular block and 0 implies that, that particular block is free and 1 implies that the block is already occupied. A bit table has the advantage that it is relatively easy to find
one or a contiguous group of free blocks. Thus, a bit table works well with any of the file allocation methods. Another advantage is that it is as small as possible.
2. Free Block List : In this method, each block is assigned a number sequentially and the list of the numbers of all free blocks is maintained in a reserved block of the disk.

This article is contributed by Aakansha yadav

Improved By : raghvanimahesh

Source
https://www.geeksforgeeks.org/file-systems-in-operating-system/
✍
Write a Testimonial

Virtual Memory in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Virtual Memory in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Virtual Memory is a storage allocation scheme in which secondary memory can be addressed as though it were part of main memory. The addresses a program may use to reference memory are distinguished from the
addresses the memory system uses to identify physical storage sites, and program generated addresses are translated automatically to the corresponding machine addresses.
The size of virtual storage is limited by the addressing scheme of the computer system and amount of secondary memory is available not by the actual number of the main storage locations.
It is a technique that is implemented using both hardware and software. It maps memory addresses used by a program, called virtual addresses, into physical addresses in computer memory.
1. All memory references within a process are logical addresses that are dynamically translated into physical addresses at run time. This means that a process can be swapped in and out of main memory such that it
occupies different places in main memory at different times during the course of execution.
2. A process may be broken into number of pieces and these pieces need not be continuously located in the main memory during execution. The combination of dynamic run-time address translation and use of page or
segment table permits this.
If these characteristics are present then, it is not necessary that all the pages or segments are present in the main memory during execution. This means that the required pages need to be loaded into memory whenever
required. Virtual memory is implemented using Demand Paging or Demand Segmentation.

Demand Paging :
The process of loading the page into memory on demand (whenever page fault occurs) is known as demand paging.
The process includes the following steps :

1.
2.
3.
4.
5.
6.

If CPU try to refer a page that is currently not available in the main memory, it generates an interrupt indicating memory access fault.
The OS puts the interrupted process in a blocking state. For the execution to proceed the OS must bring the required page into the memory.
The OS will search for the required page in the logical address space.
The required page will be brought from logical address space to physical address space. The page replacement algorithms are used for the decision making of replacing the page in physical address space.
The page table will updated accordingly.
The signal will be sent to the CPU to continue the program execution and it will place the process back into ready state.

Hence whenever a page fault occurs these steps are followed by the operating system and the required page is brought into memory.
Advantages :
More processes may be maintained in the main memory: Because we are going to load only some of the pages of any particular process, there is room for more processes. This leads to more efficient utilization of
the processor because it is more likely that at least one of the more numerous processes will be in the ready state at any particular time.
A process may be larger than all of main memory: One of the most fundamental restrictions in programming is lifted. A process larger than the main memory can be executed because of demand paging. The OS
itself loads pages of a process in main memory as required.
It allows greater multiprogramming levels by using less of the available (primary) memory for each process.
Page Fault Service Time :
The time taken to service the page fault is called as page fault service time. The page fault service time includes the time taken to perform all the above six steps.
​
Let Main memory access time is: m​
Page fault service time is: s​
Page fault rate is : p​
Then, Effective memory access time = (p*s) + (1-p)*m​

Swapping:
Swapping a process out means removing all of its pages from memory, or marking them so that they will be removed by the normal page replacement process. Suspending a process ensures that it is not runnable while it is
swapped out. At some later time, the system swaps back the process from the secondary storage to main memory. When a process is busy swapping pages in and out then this situation is called thrashing.

Thrashing :

At any given time, only few pages of any process are in main memory and therefore more processes can be maintained in memory. Furthermore time is saved because unused pages are not swapped in and out of memory.
However, the OS must be clever about how it manages this scheme. In the steady state practically, all of main memory will be occupied with process’s pages, so that the processor and OS has direct access to as many
processes as possible. Thus when the OS brings one page in, it must throw another out. If it throws out a page just before it is used, then it will just have to get that page again almost immediately. Too much of this leads to
a condition called Thrashing. The system spends most of its time swapping pages rather than executing instructions. So a good page replacement algorithm is required.
In the given diagram, initial degree of multi programming upto some extent of point(lamda), the CPU utilization is very high and the system resources are utilized 100%. But if we further increase the degree of multi
programming the CPU utilization will drastically fall down and the system will spent more time only in the page replacement and the time taken to complete the execution of the process will increase. This situation in the
system is called as thrashing.
Causes of Thrashing :
1. High degree of multiprogramming : If the number of processes keeps on increasing in the memory than number of frames allocated to each process will be decreased. So, less number of frames will be available to
each process. Due to this, page fault will occur more frequently and more CPU time will be wasted in just swapping in and out of pages and the utilization will keep on decreasing.
For example:
Let free frames = 400
Case 1: Number of process = 100
Then, each process will get 4 frames.
Case 2: Number of process = 400
Each process will get 1 frame.
Case 2 is a condition of thrashing, as the number of processes are increased,frames per process are decreased. Hence CPU time will be consumed in just swapping pages.
2. Lacks of Frames:If a process has less number of frames then less pages of that process will be able to reside in memory and hence more frequent swapping in and out will be required. This may lead to thrashing.
Hence sufficient amount of frames must be allocated to each process in order to prevent thrashing.
Recovery of Thrashing :
Do not allow the system to go into thrashing by instructing the long term scheduler not to bring the processes into memory after the threshold.
If the system is already in thrashing then instruct the mid term schedular to suspend some of the processes so that we can recover the system from thrashing.
This article is contributed by Aakansha yadav

Improved By : nidhi_biet

Source
https://www.geeksforgeeks.org/virtual-memory-in-operating-system/
✍
Write a Testimonial

Inter Process Communication (IPC)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Inter Process Communication (IPC) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
A process can be of two type:
Independent process.
Co-operating process.
An independent process is not affected by the execution of other processes while a co-operating process can be affected by other executing processes. Though one can think that those processes, which are running
independently, will execute very efficiently but in practical, there are many situations when co-operative nature can be utilised for increasing computational speed, convenience and modularity. Inter process

communication (IPC) is a mechanism which allows processes to communicate each other and synchronize their actions. The communication between these processes can be seen as a method of co-operation between them.
Processes can communicate with each other using these two ways:
1. Shared Memory
2. Message passing
The Figure 1 below shows a basic structure of communication between processes via shared memory method and via message passing.

An operating system can implement both method of communication. First, we will discuss the shared memory method of communication and then message passing. Communication between processes using shared
memory requires processes to share some variable and it completely depends on how programmer will implement it. One way of communication using shared memory can be imagined like this: Suppose process1 and
process2 are executing simultaneously and they share some resources or use some information from other process, process1 generate information about certain computations or resources being used and keeps it as a record
in shared memory. When process2 need to use the shared information, it will check in the record stored in shared memory and take note of the information generated by process1 and act accordingly. Processes can use
shared memory for extracting information as a record from other process as well as for delivering any specific information to other process.
Let’s discuss an example of communication between processes using shared memory method.

i) Shared Memory Method
Ex: Producer-Consumer problem
There are two processes: Producer and Consumer. Producer produces some item and Consumer consumes that item. The two processes shares a common space or memory location known as buffer where the item
produced by Producer is stored and from where the Consumer consumes the item if needed. There are two version of this problem: first one is known as unbounded buffer problem in which Producer can keep on
producing items and there is no limit on size of buffer, the second one is known as bounded buffer problem in which producer can produce up to a certain amount of item and after that it starts waiting for consumer to
consume it. We will discuss the bounded buffer problem. First, the Producer and the Consumer will share some common memory, then producer will start producing items. If the total produced item is equal to the size of
buffer, producer will wait to get it consumed by the Consumer. Similarly, the consumer first check for the availability of the item and if no item is available, Consumer will wait for producer to produce it. If there are items available, consumer will consume it. The pseudo code are given
below:
Shared Data between the two Processes
filter_none
edit
close
play_arrow
link
brightness_4
code
#define buff_max 25
#define mod %
struct item{
// different member of the produced data
// or consumed data
--------}
//
//
//
//

An array is needed for holding the items.
This is the shared place which will be
access by both process
item shared_buff [ buff_max ];

// Two variables which will keep track of
// the indexes of the items produced by producer
// and consumer The free index points to
// the next free index. The full index points to
// the first full index.
int free_index = 0;
int full_index = 0;

chevron_right
filter_none
Producer Process Code
filter_none
edit
close
play_arrow
link
brightness_4
code
item nextProduced;
while(1){
// check if there is no space
// for production.
// if so keep waiting.
while((free_index+1) mod buff_max == full_index);
shared_buff[free_index] = nextProduced;
free_index = (free_index + 1) mod buff_max;
}

chevron_right
filter_none
Consumer Process Code
filter_none
edit
close

play_arrow
link
brightness_4
code
item nextConsumed;
while(1){
// check if there is an available
// item for consumption.
// if not keep on waiting for
// get them produced.
while((free_index == full_index);
nextConsumed = shared_buff[full_index];
full_index = (full_index + 1) mod buff_max;
}

chevron_right
filter_none
In the above code, The producer will start producing again when the (free_index+1) mod buff max will be free because if it it not free, this implies that there are still items that can be consumed by the Consumer so there
is no need to produce more. Similarly, if free index and full index points to the same index, this implies that there are no item to consume.
ii) Messaging Passing Method
Now, We will start our discussion for the communication between processes via message passing. In this method, processes communicate with each other without using any kind of shared memory. If two processes p1 and
p2 want to communicate with each other, they proceed as follow:
Establish a communication link (if a link already exists, no need to establish it again.)
Start exchanging messages using basic primitives.
We need at least two primitives:
– send(message, destinaion) or send(message)
– receive(message, host) or receive(message)

The message size can be of fixed size or of variable size. if it is of fixed size, it is easy for OS designer but complicated for programmer and if it is of variable size then it is easy for programmer but complicated for the OS
designer. A standard message can have two parts: header and body.
The header part is used for storing Message type, destination id, source id, message length and control information. The control information contains information like what to do if runs out of buffer space, sequence
number, priority. Generally, message is sent using FIFO style.

Message Passing through Communication Link.
Direct and Indirect Communication link
Now, We will start our discussion about the methods of implementing communication link. While implementing the link, there are some questions which need to be kept in mind like :
1.
2.
3.
4.
5.

How are links established?
Can a link be associated with more than two processes?
How many links can there be between every pair of communicating processes?
What is the capacity of a link? Is the size of a message that the link can accommodate fixed or variable?
Is a link unidirectional or bi-directional?

A link has some capacity that determines the number of messages that can reside in it temporarily for which Every link has a queue associated with it which can be either of zero capacity or of bounded capacity or of
unbounded capacity. In zero capacity, sender wait until receiver inform sender that it has received the message. In non-zero capacity cases, a process does not know whether a message has been received or not after the
send operation. For this, the sender must communicate to receiver explicitly. Implementation of the link depends on the situation, it can be either a Direct communication link or an In-directed communication link.
Direct Communication links are implemented when the processes use specific process identifier for the communication but it is hard to identify the sender ahead of time.
For example: the print server.
In-directed Communication is done via a shred mailbox (port), which consists of queue of messages. Sender keeps the message in mailbox and receiver picks them up.
Message Passing through Exchanging the Messages.
Synchronous and Asynchronous Message Passing:
A process that is blocked is one that is waiting for some event, such as a resource becoming available or the completion of an I/O operation. IPC is possible between the processes on same computer as well as on the
processes running on different computer i.e. in networked/distributed system. In both cases, the process may or may not be blocked while sending a message or attempting to receive a message so Message passing may be
blocking or non-blocking. Blocking is considered synchronous and blocking send means the sender will be blocked until the message is received by receiver. Similarly, blocking receive has the receiver block until a
message is available. Non-blocking is considered asynchronous and Non-blocking send has the sender sends the message and continue. Similarly, Non-blocking receive has the receiver receive a valid message or null.
After a careful analysis, we can come to a conclusion that, for a sender it is more natural to be non-blocking after message passing as there may be a need to send the message to different processes But the sender expect
acknowledgement from receiver in case the send fails. Similarly, it is more natural for a receiver to be blocking after issuing the receive as the information from the received message may be used for further execution but
at the same time, if the message send keep on failing, receiver will have to wait for indefinitely. That is why we also consider the other possibility of message passing. There are basically three most preferred
combinations:
Blocking send and blocking receive
Non-blocking send and Non-blocking receive
Non-blocking send and Blocking receive (Mostly used)
In Direct message passing, The process which want to communicate must explicitly name the recipient or sender of communication.
e.g. send(p1, message) means send the message to p1.
similarly, receive(p2, message) means receive the message from p2.
In this method of communication, the communication link get established automatically, which can be either unidirectional or bidirectional, but one link can be used between one pair of the sender and receiver and one pair
of sender and receiver should not possess more than one pair of link. Symmetry and asymmetry between the sending and receiving can also be implemented i.e. either both process will name each other for sending and
receiving the messages or only sender will name receiver for sending the message and there is no need for receiver for naming the sender for receiving the message.The problem with this method of communication is that
if the name of one process changes, this method will not work.
In Indirect message passing, processes uses mailboxes (also referred to as ports) for sending and receiving messages. Each mailbox has a unique id and processes can communicate only if they share a mailbox. Link
established only if processes share a common mailbox and a single link can be associated with many processes. Each pair of processes can share several communication links and these link may be unidirectional or bidirectional. Suppose two process want to communicate though Indirect message passing, the required operations are: create a mail box, use this mail box for sending and receiving messages, destroy the mail box. The
standard primitives used are : send(A, message) which means send the message to mailbox A. The primitive for the receiving the message also works in the same way e.g. received (A, message). There is a problem in this
mailbox implementation. Suppose there are more than two processes sharing the same mailbox and suppose the process p1 sends a message to the mailbox, which process will be the receiver? This can be solved by either
forcing that only two processes can share a single mailbox or enforcing that only one process is allowed to execute the receive at a given time or select any process randomly and notify the sender about the receiver. A
mailbox can be made private to a single sender/receiver pair and can also be shared between multiple sender/receiver pairs. Port is an implementation of such mailbox which can have multiple sender and single receiver. It
is used in client/server application (Here server is the receiver). The port is owned by the receiving process and created by OS on the request of the receiver process and can be destroyed either on request of the same
receiver process or when the receiver terminates itself. Enforcing that only one process is allowed to execute the receive can be done using the concept of mutual exclusion. Mutex mailbox is create which is shared by n
process. Sender is non-blocking and sends the message. The first process which executes the receive will enter in the critical section and all other processes will be blocking and will wait.

Now, lets discuss the Producer-Consumer problem using message passing concept. The producer place items (inside messages) in the mailbox and the consumer can consume item when at least one message present in the
mailbox. The code are given below:
Producer Code
filter_none
edit
close
play_arrow
link
brightness_4
code
void Producer(void){
int item;
Message m;
while(1){
receive(Consumer, &m);
item = produce();
build_message(&m , item ) ;
send(Consumer, &m);
}
}

chevron_right
filter_none
Consumer Code
filter_none
edit
close
play_arrow
link
brightness_4
code
void Consumer(void){
int item;
Message m;
while(1){
receive(Producer, &m);
item = extracted_item();
send(Producer, &m);
consume_item(item);
}
}

chevron_right
filter_none
Examples of IPC systems
1. Posix : uses shared memory method.
2. Mach : uses message passing
3. Windows XP : uses message passing using local procedural calls
Communication in client/server Architecture:
There are various mechanism:
Pipe
Socket
Remote Procedural calls (RPCs)
The above three methods will be discussed later article as all of them are quite conceptual and deserve their own separate articles.
References:
1. Operating System Concepts by Galvin et al.
2. Lecture notes/ppt of Ariel J. Frank, Bar-Ilan University
More Reference:
http://nptel.ac.in/courses/106108101/pdf/Lecture_Notes/Mod%207_LN.pdf
https://www.youtube.com/watch?v=lcRqHwIn5Dk

Improved By : ShubhamMaurya3, shubham_singh

Source
https://www.geeksforgeeks.org/inter-process-communication-ipc/
✍
Write a Testimonial

Peterson’s Algorithm for Mutual Exclusion | Set 2 (CPU Cycles and Memory Fence)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Peterson's Algorithm for Mutual Exclusion | Set 2 (CPU Cycles and Memory Fence) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Problem: Given 2 process i and j, you need to write a program that can guarantee mutual exclusion between the two without any additional hardware support.
We strongly recommend to refer below basic solution discussed in previous article.
Peterson’s Algorithm for Mutual Exclusion | Set 1
We would be resolving 2 issues in the previous algorithm.

Wastage of CPU clock cycles
In layman terms, when a thread was waiting for its turn, it ended in a long while loop which tested the condition millions of times per second thus doing unnecessary computation. There is a better way to wait, and it is
known as “yield”.
To understand what it does, we need to dig deep into how the Process scheduler works in Linux. The idea mentioned here is a simplified version of the scheduler, the actual implementation has lots of complications.
Consider the following example,
There are three processes, P1, P2 and P3. Process P3 is such that it has a while loop similar to the one in our code, doing not so useful computation, and it exists from the loop only when P2 finishes its execution. The
scheduler puts all of them in a round robin queue. Now, say the clock speed of processor is 1000000/sec, and it allocates 100 clocks to each process in each iteration. Then, first P1 will be run for 100 clocks (0.0001
seconds), then P2(0.0001 seconds) followed by P3(0.0001 seconds), now since there are no more processes, this cycle repeats untill P2 ends and then followed by P3’s execution and eventually its termination.
This is a complete waste of the 100 CPU clock cycles. To avoid this, we mutually give up the CPU time slice, i.e. yield, which essentially ends this time slice and the scheduler picks up the next process to run. Now, we
test our condition once, then we give up the CPU. Considering our test takes 25 clock cycles, we save 75% of our computation in a time slice. To put this graphically,

Considering the processor clock speed as 1MHz this is a lot of saving!.
Different distributions provide different function to achieve this functionality. Linux provides sched_yield().
filter_none
edit
close
play_arrow
link
brightness_4
code
void lock(int self)
{
flag[self] = 1;
turn = 1-self;
while (flag[1-self] == 1 &&
turn == 1-self)
// Only change is the addition of
// sched_yield() call
sched_yield();
}

chevron_right
filter_none
Memory fence.
The code in earlier tutorial might have worked on most systems, but is was not 100% correct. The logic was perfect, but most modern CPUs employ performance optimizations that can result in out-of-order execution. This
reordering of memory operations (loads and stores) normally goes unnoticed within a single thread of execution, but can cause unpredictable behaviour in concurrent programs.
Consider this example,
filter_none
edit
close
play_arrow
link
brightness_4
code
while (f == 0);
// Memory fence required here
print x;

chevron_right

filter_none
In the above example, the compiler considers the 2 statements as independent of each other and thus tries to increase the code efficiency by re-ordering them, which can lead to problems for concurrent programs. To avoid
this we place a memory fence to give hint to the compiler about the possible relationship between the statements across the barrier.
So the order of statements,

flag[self] = 1;
turn = 1-self;
while (turn condition check)
yield();
has to be exactly the same in order for the lock to work, otherwise it will end up in a deadlock condition.
To ensure this, compilers provide a instruction that prevent ordering of statements across this barrier. In case of gcc, its __sync_synchronize().
So the modified code becomes,
Full Implementation in C:
filter_none
edit
close
play_arrow
link
brightness_4
code
// Filename: peterson_yieldlock_memoryfence.c
// Use below command to compile:
// gcc -pthread peterson_yieldlock_memoryfence.c -o peterson_yieldlock_memoryfence
#include<stdio.h>
#include<pthread.h>
#include "mythreads.h"
int flag[2];
int turn;
const int MAX = 1e9;
int ans = 0;
void lock_init()
{
// Initialize lock by reseting the desire of
// both the threads to acquire the locks.
// And, giving turn to one of them.
flag[0] = flag[1] = 0;
turn = 0;
}
// Executed before entering critical section
void lock(int self)
{
// Set flag[self] = 1 saying you want
// to acquire lock
flag[self]=1;
// But, first give the other thread the
// chance to acquire lock
turn = 1-self;
// Memory fence to prevent the reordering
// of instructions beyond this barrier.
__sync_synchronize();
// Wait untill the other thread looses the
// desire to acquire lock or it is your
// turn to get the lock.
while (flag[1-self]==1 && turn==1-self)
// Yield to avoid wastage of resources.
sched_yield();
}
// Executed after leaving critical section
void unlock(int self)
{
// You do not desire to acquire lock in future.
// This will allow the other thread to acquire
// the lock.
flag[self]=0;
}
// A Sample function run by two threads created
// in main()
void* func(void *s)
{
int i = 0;
int self = (int *)s;
printf("Thread Entered: %d\n",self);
lock(self);
// Critical section (Only one thread
// can enter here at a time)
for (i=0; i<MAX; i++)
ans++;
unlock(self);
}
// Driver code
int main()
{
pthread_t p1, p2;
// Initialize the lock
lock_init();
// Create two threads (both run func)
Pthread_create(&p1, NULL, func, (void*)0);
Pthread_create(&p2, NULL, func, (void*)1);
// Wait for the threads to end.
Pthread_join(p1, NULL);
Pthread_join(p2, NULL);
printf("Actual Count: %d | Expected Count:"
" %d\n",ans,MAX*2);
return 0;
}

chevron_right
filter_none
filter_none

edit
close
play_arrow
link
brightness_4
code
// mythread.h (A wrapper header file with assert
// statements)
#ifndef __MYTHREADS_h__
#define __MYTHREADS_h__
#include <pthread.h>
#include <assert.h>
#include <sched.h>
void Pthread_mutex_lock(pthread_mutex_t *m)
{
int rc = pthread_mutex_lock(m);
assert(rc == 0);
}
void Pthread_mutex_unlock(pthread_mutex_t *m)
{
int rc = pthread_mutex_unlock(m);
assert(rc == 0);
}
void Pthread_create(pthread_t *thread, const pthread_attr_t *attr,
void *(*start_routine)(void*), void *arg)
{
int rc = pthread_create(thread, attr, start_routine, arg);
assert(rc == 0);
}
void Pthread_join(pthread_t thread, void **value_ptr)
{
int rc = pthread_join(thread, value_ptr);
assert(rc == 0);
}
#endif // __MYTHREADS_h__

chevron_right
filter_none
Output:
​
Thread Entered: 1​
Thread Entered: 0​
Actual Count: 2000000000 | Expected Count: 2000000000​

Source
https://www.geeksforgeeks.org/petersons-algorithm-for-mutual-exclusion-set-2-cpu-cycles-and-memory-fence/
✍
Write a Testimonial

Peterson’s Algorithm for Mutual Exclusion | Set 1 (Basic C implementation)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Peterson's Algorithm for Mutual Exclusion | Set 1 (Basic C implementation) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Problem: Given 2 process i and j, you need to write a program that can guarantee mutual exclusion between the two without any additional hardware support.
Solution: There can be multiple ways to solve this problem, but most of them require additional hardware support. The simplest and the most popular way to do this is by using Peterson Algorithm for mutual Exclusion. It
was developed by Peterson in 1981 though the initial work in this direction by done by Theodorus Jozef Dekker who came up with Dekker’s algorithm in 1960, which was later refined by Peterson and came to be known
as Peterson’s Algorithm.
Basically, Peterson’s algorithm provides guaranteed mutual exclusion by using only the shared memory. It uses two ideas in the algorithm,

1. Willingness to acquire lock.
2. Turn to acquire lock.
Prerequisite : Multithreading in C
Explanation:
The idea is that first a thread expresses its desire to acquire lock and sets flag[self] = 1 and then gives the other thread a chance to acquire the lock. If the thread desires to acquire the lock, then, it gets the lock and then
passes the chance to the 1st thread. If it does not desire to get the lock then the while loop breaks and the 1st thread gets the chance.
Implementation in C language
filter_none
edit
close
play_arrow
link
brightness_4
code
// Filename: peterson_spinlock.c
// Use below command to compile:
// gcc -pthread peterson_spinlock.c -o peterson_spinlock
#include <stdio.h>
#include <pthread.h>
#include"mythreads.h"
int flag[2];

int turn;
const int MAX = 1e9;
int ans = 0;
void lock_init()
{
// Initialize lock by reseting the desire of
// both the threads to acquire the locks.
// And, giving turn to one of them.
flag[0] = flag[1] = 0;
turn = 0;
}
// Executed before entering critical section
void lock(int self)
{
// Set flag[self] = 1 saying you want to acquire lock
flag[self] = 1;
// But, first give the other thread the chance to
// acquire lock
turn = 1-self;
// Wait until the other thread looses the desire
// to acquire lock or it is your turn to get the lock.
while (flag[1-self]==1 && turn==1-self) ;
}
// Executed after leaving critical section
void unlock(int self)
{
// You do not desire to acquire lock in future.
// This will allow the other thread to acquire
// the lock.
flag[self] = 0;
}
// A Sample function run by two threads created
// in main()
void* func(void *s)
{
int i = 0;
int self = (int *)s;
printf("Thread Entered: %d\n", self);
lock(self);
// Critical section (Only one thread
// can enter here at a time)
for (i=0; i<MAX; i++)
ans++;
unlock(self);
}
// Driver code
int main()
{
// Initialized the lock then fork 2 threads
pthread_t p1, p2;
lock_init();
// Create two threads (both run func)
pthread_create(&p1, NULL, func, (void*)0);
pthread_create(&p2, NULL, func, (void*)1);
// Wait for the threads to end.
pthread_join(p1, NULL);
pthread_join(p2, NULL);
printf("Actual Count: %d | Expected Count: %d\n",
ans, MAX*2);
return 0;
}

chevron_right
filter_none
filter_none
edit
close
play_arrow
link
brightness_4
code
// mythread.h (A wrapper header file with assert
// statements)
#ifndef __MYTHREADS_h__
#define __MYTHREADS_h__
#include <pthread.h>
#include <assert.h>
#include <sched.h>
void Pthread_mutex_lock(pthread_mutex_t *m)
{
int rc = pthread_mutex_lock(m);
assert(rc == 0);
}
void Pthread_mutex_unlock(pthread_mutex_t *m)
{
int rc = pthread_mutex_unlock(m);
assert(rc == 0);
}
void Pthread_create(pthread_t *thread, const pthread_attr_t *attr,
void *(*start_routine)(void*), void *arg)
{
int rc = pthread_create(thread, attr, start_routine, arg);
assert(rc == 0);
}
void Pthread_join(pthread_t thread, void **value_ptr)
{
int rc = pthread_join(thread, value_ptr);
assert(rc == 0);
}
#endif // __MYTHREADS_h__

chevron_right
filter_none
Output:
​
Thread Entered: 1​
Thread Entered: 0​

Actual Count: 2000000000 | Expected Count: 2000000000​

The produced output is 2*10 9 where 109 is incremented by both threads.

Source
https://www.geeksforgeeks.org/petersons-algorithm-for-mutual-exclusion-set-1/
✍
Write a Testimonial

Producer-Consumer solution using threads in Java
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Producer-Consumer solution using threads in Java - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
In computing, the producer-consumer problem (also known as the bounded-buffer problem) is a classic example of a multi-process synchronization problem. The problem describes two processes, the producer and the
consumer, which share a common, fixed-size buffer used as a queue.
The producer’s job is to generate data, put it into the buffer, and start again.
At the same time, the consumer is consuming the data (i.e. removing it from the buffer), one piece at a time.
Problem
To make sure that the producer won’t try to add data into the buffer if it’s full and that the consumer won’t try to remove data from an empty buffer.
Solution
The producer is to either go to sleep or discard data if the buffer is full. The next time the consumer removes an item from the buffer, it notifies the producer, who starts to fill the buffer again. In the same way, the
consumer can go to sleep if it finds the buffer to be empty. The next time the producer puts data into the buffer, it wakes up the sleeping consumer.
An inadequate solution could result in a deadlock where both processes are waiting to be awakened.

Recommended Reading- Multithreading in JAVA, Synchronized in JAVA, Inter-thread Communication
Implementation of Producer Consumer Class
A LinkedList list – to store list of jobs in queue.
A Variable Capacity – to check for if the list is full or not
A mechanism to control the insertion and extraction from this list so that we do not insert into list if it is full or remove from it if it is empty.
Note: It is recommended to test the below program on a offline IDE as infinite loops and sleep method may lead to it time out on any online IDE
filter_none
edit
close
play_arrow
link
brightness_4
code
// Java program to implement solution of producer
// consumer problem.
import java.util.LinkedList;
public class Threadexample {
public static void main(String[] args)
throws InterruptedException
{
// Object of a class that has both produce()
// and consume() methods
final PC pc = new PC();
// Create producer thread
Thread t1 = new Thread(new Runnable() {
@Override
public void run()
{
try {
pc.produce();
}
catch (InterruptedException e) {
e.printStackTrace();
}
}
});
// Create consumer thread
Thread t2 = new Thread(new Runnable() {
@Override
public void run()
{
try {
pc.consume();
}
catch (InterruptedException e) {
e.printStackTrace();
}
}
});
// Start both threads
t1.start();
t2.start();
// t1 finishes before t2
t1.join();
t2.join();
}
// This class has a list, producer (adds items to list
// and consumber (removes items).
public static class PC {
// Create a list shared by producer and consumer
// Size of list is 2.
LinkedList<Integer> list = new LinkedList<>();

int capacity = 2;
// Function called by producer thread
public void produce() throws InterruptedException
{
int value = 0;
while (true) {
synchronized (this)
{
// producer thread waits while list
// is full
while (list.size() == capacity)
wait();
System.out.println("Producer produced-"
+ value);
// to insert the jobs in the list
list.add(value++);
// notifies the consumer thread that
// now it can start consuming
notify();
// makes the working of program easier
// to understand
Thread.sleep(1000);
}
}
}
// Function called by consumer thread
public void consume() throws InterruptedException
{
while (true) {
synchronized (this)
{
// consumer thread waits while list
// is empty
while (list.size() == 0)
wait();
// to retrive the ifrst job in the list
int val = list.removeFirst();
System.out.println("Consumer consumed-"
+ val);
// Wake up producer thread
notify();
// and sleep
Thread.sleep(1000);
}
}
}
}
}

chevron_right
filter_none
Output:
​
Producer
Producer
Consumer
Consumer
Producer

produced-0​
produced-1​
consumed-0​
consumed-1​
produced-2​

Important Points
In PC class (A class that has both produce and consume methods), a linked list of jobs and a capacity of the list is added to check that producer does not produce if the list is full.
In Producer class, the value is initialized as 0.
Also, we have an infinite outer loop to insert values in the list. Inside this loop, we have a synchronized block so that only a producer or a consumer thread runs at a time.
An inner loop is there before adding the jobs to list that checks if the job list is full, the producer thread gives up the intrinsic lock on PC and goes on the waiting state.
If the list is empty, the control passes to below the loop and it adds a value in the list.
In the Consumer class, we again have an infinite loop to extract a value from the list.
Inside, we also have an inner loop which checks if the list is empty.
If it is empty then we make the consumer thread give up the lock on PC and passes the control to producer thread for producing more jobs.
If the list is not empty, we go round the loop and removes an item from the list.
In both the methods, we use notify at the end of all statements. The reason is simple, once you have something in list, you can have the consumer thread consume it, or if you have consumed something, you can have the
producer produce something.
sleep() at the end of both methods just make the output of program run in step wise manner and not display everything all at once so that you can see what actually is happening in the program.
Exercise :
Readers are advised to use if condition in place of inner loop for checking boundary conditions.
Try to make your program produce one item and immediately after that make the consumer consume it before any other item is produced by the producer.
Reference – https://en.wikipedia.org/wiki/Producer%E2%80%93consumer_problem

Improved By : Gaurav Yadav 6

Source
https://www.geeksforgeeks.org/producer-consumer-solution-using-threads-java/
✍
Write a Testimonial

Segmentation in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Segmentation in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
A process is divided into Segments. The chunks that a program is divided into which are not necessarily all of the same sizes are called segments. Segmentation gives user’s view of the process which paging does not give.
Here the user’s view is mapped to physical memory.

There are types of segmentation:
1. Virtual memory segmentation –
Each process is divided into a number of segments, not all of which are resident at any one point in time.
2. Simple segmentation –
Each process is divided into a number of segments, all of which are loaded into memory at run time, though not necessarily contiguously.
There is no simple relationship between logical addresses and physical addresses in segmentation. A table stores the information about all such segments and is called Segment Table.
Segment Table – It maps two-dimensional Logical address into one-dimensional Physical address. It’s each table entry has:
Base Address: It contains the starting physical address where the segments reside in memory.
Limit: It specifies the length of the segment.

Translation of Two dimensional Logical Address to one dimensional Physical Address.

Address generated by the CPU is divided into:
Segment number (s): Number of bits required to represent the segment.
Segment offset (d): Number of bits required to represent the size of the segment.
Advantages of Segmentation –
No Internal fragmentation.
Segment Table consumes less space in comparison to Page table in paging.
Disadvantage of Segmentation –
As processes are loaded and removed from the memory, the free memory space is broken into little pieces, causing External fragmentation.
This article has been contributed by Vikash Kumar. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above

Improved By : VaibhavRai3

Source

https://www.geeksforgeeks.org/segmentation-in-operating-system/
✍
Write a Testimonial

Last Minute Notes – Operating Systems
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Last Minute Notes – Operating Systems - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
See Last Minute Notes for all subjects all subjects here.
Operating Systems: It is the interface between the user and the computer hardware.
Types of Operating System (OS):

1. Batch OS – A set of similar jobs are stored in the main memory for execution. A job gets assigned to the CPU, only when the execution of the previous job completes.
2. Multiprogramming OS – The main memory consists of jobs waiting for CPU time. The OS selects one of the processes and assigns it to the CPU. Whenever the executing process needs to wait for any other
operation (like I/O), the OS selects another process from the job queue and assigns it to the CPU. This way, the CPU is never kept idle and the user gets the flavor of getting multiple tasks done at once.
3. Multitasking OS – Multitasking OS combines the benefits of Multiprogramming OS and CPU scheduling to perform quick switches between jobs. The switch is so quick that the user can interact with each program
as it runs
4. Time Sharing OS – Time-sharing systems require interaction with the user to instruct the OS to perform various tasks. The OS responds with an output. The instructions are usually given through an input device
like the keyboard.
5. Real Time OS – Real-Time OS are usually built for dedicated systems to accomplish a specific set of tasks within deadlines.

Threads:
A thread is a lightweight process and forms the basic unit of CPU utilization. A process can perform more than one task at the same time by including multiple threads.
A thread has its own program counter, register set, and stack
A thread shares resources with other threads of the same process the code section, the data section, files and signals.
A new thread, or a child process of a given process, can be introduced by using the fork() system call. A process with n fork() system calls generates 2n – 1 child processes.
There are two types of threads:
User threads
Kernel threads
Example: Java thread, POSIX threads.Example : Window Solaris.

User level thread
User threads are implemented by users.
OS doesn’t recognize user level threads.
Implementation of User threads is easy.
Context switch time is less.
Context switch requires no hardware support.
If one user level thread performs blocking operation then entire process will be blocked.

Kernel level thread
kernel threads are implemented by OS.
Kernel threads are recognized by OS.
Implementation of Kernel thread is complicated.
Context switch time is more.
Hardware support is needed.
If one kernel thread performs blocking operation then another thread can continue execution.

Process:
A process is a program under execution. The value of program counter (PC) indicates the address of the next instruction of the process being executed. Each process is represented by a Process Control Block (PCB).
Process Scheduling: Below are different times with respect to a process.
1.
2.
3.
4.

Arrival Time – Time at which the process arrives in the ready queue.
Completion Time – Time at which process completes its execution.
Burst Time – Time required by a process for CPU execution.
Turn Around Time – Time Difference between completion time and arrival time.
Turn Around Time = Completion Time - Arrival Time

5. Waiting Time (WT) – Time Difference between turn around time and burst time.
Waiting Time = Turn Around Time - Burst Time

Why do we need scheduling?
A typical process involves both I/O time and CPU time. In a uniprogramming system like MS-DOS, time spent waiting for I/O is wasted and CPU is free during this time. In multiprogramming systems, one process can
use CPU while another is waiting for I/O. This is possible only with process scheduling.
Objectives of Process Scheduling Algorithm:
Max CPU utilization (Keep CPU as busy as possible)
Fair allocation of CPU.
Max throughput (Number of processes that complete their execution per time unit)
Min turnaround time (Time taken by a process to finish execution)
Min waiting time (Time for which a process waits in ready queue)
Min response time (Time when a process produces first response)
Different Scheduling Algorithms:
1.
2.
3.
4.
5.

First Come First Serve (FCFS) : Simplest scheduling algorithm that schedules according to arrival times of processes.
Shortest Job First (SJF): Process which have the shortest burst time are scheduled first.
Shortest Remaining Time First (SRTF) : It is preemptive mode of SJF algorithm in which jobs are scheduled according to the shortest remaining time.
Round Robin (RR) Scheduling: Each process is assigned a fixed time, in cyclic way.
Priority Based scheduling (Non Preemptive): In this scheduling, processes are scheduled according to their priorities, i.e., highest priority process is schedule first. If priorities of two processes match, then
scheduling is according to the arrival time.
6. Highest Response Ratio Next (HRRN): In this scheduling, processes with highest response ratio is scheduled. This algorithm avoids starvation.
Response Ratio = (Waiting Time + Burst time) / Burst time

7. Multilevel Queue Scheduling (MLQ): According to the priority of process, processes are placed in the different queues. Generally high priority process are placed in the top level queue. Only after completion of
processes from top level queue, lower level queued processes are scheduled.
8. Multi level Feedback Queue (MLFQ) Scheduling: It allows the process to move in between queues. The idea is to separate processes according to the characteristics of their CPU bursts. If a process uses too much
CPU time, it is moved to a lower-priority queue.
Some useful facts about Scheduling Algorithms:

1.
2.
3.
4.

FCFS can cause long waiting times, especially when the first job takes too much CPU time.
Both SJF and Shortest Remaining time first algorithms may cause starvation. Consider a situation when a long process is there in the ready queue and shorter processes keep coming.
If time quantum for Round Robin scheduling is very large, then it behaves same as FCFS scheduling.
SJF is optimal in terms of average waiting time for a given set of processes. SJF gives minimum average waiting time, but problems with SJF is how to know/predict the time of next job.

The Critical Section Problem:
1. Critical Section – The portion of the code in the program where shared variables are accessed and/or updated.
2. Remainder Section – The remaining portion of the program excluding the Critical Section.
3. Race around Condition – The final output of the code depends on the order in which the variables are accessed. This is termed as the race around condition.
A solution for the critical section problem must satisfy the following three conditions:
1. Mutual Exclusion – If a process Pi is executing in its critical section, then no other process is allowed to enter into the critical section.
2. Progress – If no process is executing in the critical section, then the decision of a process to enter a critical section cannot be made by any other process that is executing in its remainder section. The selection of the
process cannot be postponed indefinitely.
3. Bounded Waiting – There exists a bound on the number of times other processes can enter into the critical section after a process has made request to access the critical section and before the requested is granted.
Synchronization Tools:
A Semaphore is an integer variable that is accessed only through two atomic operations, wait () and signal (). An atomic operation is executed in a single CPU time slice without any pre-emption. Semaphores are of two
types:
1. Counting Semaphore – A counting semaphore is an integer variable whose value can range over an unrestricted domain.
2. Mutex – Binary Semaphores are called Mutex. These can have only two values, 0 or 1. The operations wait () and signal () operate on these in a similar fashion.
Deadlock:
A situation where a set of processes are blocked because each process is holding a resource and waiting for another resource acquired by some other process. Deadlock can arise if following four conditions hold
simultaneously (Necessary Conditions):
1.
2.
3.
4.

Mutual Exclusion – One or more than one resource are non-sharable (Only one process can use at a time).
Hold and Wait – A process is holding at least one resource and waiting for resources.
No Preemption – A resource cannot be taken from a process unless the process releases the resource.
Circular Wait – A set of processes are waiting for each other in circular form.

Methods for handling deadlock: There are three ways to handle deadlock
1. Deadlock prevention or avoidance: The idea is to not let the system into deadlock state.
2. Deadlock detection and recovery : Let deadlock occur, then do preemption to handle it once occurred.
3. Ignore the problem all together – : If deadlock is very rare, then let it happen and reboot the system. This is the approach that both Windows and UNIX take.
Banker’s Algorithm:
This algorithm handles multiple instances of the same resource.
Memory Management:
These techniques allow the memory to be shared among multiple processes.
Overlays – The memory should contain only those instructions and data that are required at a given time.
Swapping – In multiprogramming, the instructions that have used the time slice are swapped out from the memory.
Memory Management Techniques:
(a) Single Partition Allocation Schemes –
The memory is divided into two parts. One part is kept to be used by the OS and the other is kept to be used by the users.
(b) Multiple Partition Schemes –

1. Fixed Partition – The memory is divided into fixed size partitions.
2. Variable Partition – The memory is divided into variable sized partitions.
Variable partition allocation schemes:
1. First Fit – The arriving process is allotted the first hole of memory in which it fits completely.
2. Best Fit – The arriving process is allotted the hole of memory in which it fits the best by leaving the minimum memory empty.
3. Worst Fit – The arriving process is allotted the hole of memory in which it leaves the maximum gap.
Note:
Best fit does not necessarily give the best results for memory allocation.
The cause of external fragmentation is the condition in Fixed partitioning and Variable partitioning saying that entire process should be allocated in a contiguous memory location.Therefore Paging is used.
1. Paging –
The physical memory is divided into equal sized frames. The main memory is divided into fixed size pages. The size of a physical memory frame is equal to the size of a virtual memory frame.
2. Segmentation –
Segmentation is implemented to give users view of memory. The logical address space is a collection of segments. Segmentation can be implemented with or without the use of paging.
Page Fault:
A page fault is a type of interrupt, raised by the hardware when a running program accesses a memory page that is mapped into the virtual address space, but not loaded in physical memory.
Page Replacement Algorithms:
1. First In First Out (FIFO) –
This is the simplest page replacement algorithm. In this algorithm, operating system keeps track of all pages in the memory in a queue, oldest page is in the front of the queue. When a page needs to be replaced page
in the front of the queue is selected for removal.
For example, consider page reference string 1, 3, 0, 3, 5, 6 and 3 page slots. Initially, all slots are empty, so when 1, 3, 0 came they are allocated to the empty slots —> 3 Page Faults. When 3 comes, it is already in
memory so —> 0 Page Faults. Then 5 comes, it is not available in memory so it replaces the oldest page slot i.e 1. —> 1 Page Fault. Finally, 6 comes, it is also not available in memory so it replaces the oldest page
slot i.e 3 —> 1 Page Fault.
Belady’s anomaly:
Belady’s anomaly proves that it is possible to have more page faults when increasing the number of page frames while using the First in First Out (FIFO) page replacement algorithm. For example, if we consider
reference string
3 2 1 0 3 2 4 3 2 1 0 4 and 3 slots, we get 9 total page faults, but if we increase slots to 4, we get 10 page faults.
2. Optimal Page replacement –
In this algorithm, pages are replaced which are not used for the longest duration of time in the future.
Let us consider page reference string 7 0 1 2 0 3 0 4 2 3 0 3 2 and 4 page slots. Initially, all slots are empty, so when 7 0 1 2 are allocated to the empty slots —> 4 Page faults. 0 is already there so —> 0 Page fault.
When 3 came it will take the place of 7 because it is not used for the longest duration of time in the future.—> 1 Page fault. 0 is already there so —> 0 Page fault. 4 will takes place of 1 —> 1 Page Fault. Now for
the further page reference string —> 0 Page fault because they are already available in the memory.
Optimal page replacement is perfect, but not possible in practice as an operating system cannot know future requests. The use of Optimal Page replacement is to set up a benchmark so that other replacement
algorithms can be analyzed against it.
3. Least Recently Used (LRU) –
In this algorithm, the page will be replaced which is least recently used.
Let say the page reference string 7 0 1 2 0 3 0 4 2 3 0 3 2 . Initially, we have 4-page slots empty. Initially, all slots are empty, so when 7 0 1 2 are allocated to the empty slots —> 4 Page faults. 0 is already their so —
> 0 Page fault. When 3 came it will take the place of 7 because it is least recently used —> 1 Page fault. 0 is already in memory so —> 0 Page fault. 4 will takes place of 1 —> 1 Page Fault. Now for the further page
reference string —> 0 Page fault because they are already available in the memory.

File System: A file is a collection of related information that is recorded on secondary storage. Or file is a collection of logically related entities.
File Directories: Collection of files is a file directory. The directory contains information about the files, including attributes, location and ownership. Much of this information, especially that is concerned with
storage, is managed by the operating system.
1. SINGLE-LEVEL DIRECTORY: In this a single directory is maintained for all the users
2. TWO-LEVEL DIRECTORY: Due to two levels there is a path name for every file to locate that file.
3. TREE-STRUCTURED DIRECTORY : Directory is maintained in the form of a tree. Searching is efficient and also there is grouping capability.

File Allocation Methods:
1. Continuous Allocation: A single continuous set of blocks is allocated to a file at the time of file creation.
2. Linked Allocation(Non-contiguous allocation): Allocation is on an individual block basis. Each block contains a pointer to the next block in the chain.
3. Indexed Allocation : It addresses many of the problems of contiguous and chained allocation. In this case, the file allocation table contains a separate one-level index for each file

Disk Scheduling:
Disk scheduling is done by operating systems to schedule I/O requests arriving for disk. Disk scheduling is also known as I/O scheduling.
1.
2.
3.
4.
5.

Seek Time: Seek time is the time taken to locate the disk arm to a specified track where the data is to be read or write.
Rotational Latency: Rotational Latency is the time taken by the desired sector of disk to rotate into a position so that it can access the read/write heads.
Transfer Time: Transfer time is the time to transfer the data. It depends on the rotating speed of the disk and number of bytes to be transferred.
Disk Access Time: Seek Time + Rotational Latency + Transfer Time
Disk Response Time: Response Time is the average of time spent by a request waiting to perform its I/O operation. Average Response time is the response time of the all requests.

Disk Scheduling Algorithms:
1. FCFS: FCFS is the simplest of all the Disk Scheduling Algorithms. In FCFS, the requests are addressed in the order they arrive in the disk queue.
2. SSTF: In SSTF (Shortest Seek Time First), requests having shortest seek time are executed first. So, the seek time of every request is calculated in advance in a queue and then they are scheduled according to
their calculated seek time. As a result, the request near the disk arm will get executed first.
3. SCAN: In SCAN algorithm the disk arm moves into a particular direction and services the requests coming in its path and after reaching the end of the disk, it reverses its direction and again services the
request arriving in its path. So, this algorithm works like an elevator and hence also known as elevator algorithm.
4. CSCAN: In SCAN algorithm, the disk arm again scans the path that has been scanned, after reversing its direction. So, it may be possible that too many requests are waiting at the other end or there may be
zero or few requests pending at the scanned area.
5. LOOK: It is similar to the SCAN disk scheduling algorithm except for the difference that the disk arm in spite of going to the end of the disk goes only to the last request to be serviced in front of the head and
then reverses its direction from there only. Thus it prevents the extra delay which occurred due to unnecessary traversal to the end of the disk.
6. CLOOK: As LOOK is similar to SCAN algorithm, in a similar way, CLOOK is similar to CSCAN disk scheduling algorithm. In CLOOK, the disk arm in spite of going to the end goes only to the last request
to be serviced in front of the head and then from there goes to the other end’s last request. Thus, it also prevents the extra delay which occurred due to unnecessary traversal to the end of the disk.

Improved By : AmeetPanwar, Evien, metadata, vishal9619, Akanksha_Rai

Source
https://www.geeksforgeeks.org/last-minute-notes-operating-systems/
✍
Write a Testimonial

What exactly Spooling is all about?
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ What exactly Spooling is all about? - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
SPOOL is an acronym for simultaneous peripheral operations on-line. It is a kind of buffering mechanism or a process in which data is temporarily held to be used and executed by a device, program or the system.
Data is sent to and stored in memory or other volatile storage until the program or computer requests it for execution.
In a computer system peripheral equipments, such as printers and punch card readers etc (batch processing), are very slow relative to the performance of the rest of the system. Getting input and output from the system was
quickly seen to be a bottleneck. Here comes the need for spool.
Spooling works like a typical request queue where data, instructions and processes from multiple sources are accumulated for execution later on. Generally, it is maintained on computer’s physical memory, buffers or the
I/O device-specific interrupts. The spool is processed in FIFO manner i.e. whatever first instruction is there in the queue will be popped and executed.

Applications/Implementations of Spool:
1) The most common can be found in I/O devices like keyboard printers and mouse. For example, In printer, the documents/files that are sent to the printer are first stored in the memory or the printer spooler. Once the
printer is ready, it fetches the data from the spool and prints it.
Even experienced a situation when suddenly for some seconds your mouse or keyboard stops working? Meanwhile, we usually click again and again here and there on the screen to check if its working or not. When it
actually starts working, what and wherever we pressed during its hang state gets executed very fast because all the instructions got stored in the respective device’s spool.
2) A batch processing system uses spooling to maintain a queue of ready-to-run jobs which can be started as soon as the system has the resources to process them.
3) Spooling is capable of overlapping I/O operation for one job with processor operations for another job. i.e. multiple processes can write documents to a print queue without waiting and resume with their work.
4) E-mail: an email is delivered by a MTA (Mail Transfer Agent) to a temporary storage area where it waits to be picked up by the MA (Mail User Agent)
5) Can also be used for generating Banner pages (these are the pages used in computerized printing in order to separate documents from each other and to identify e.g. the originator of the print request by username, an
account number or a bin for pickup. Such pages are used in office environments where many people share the small number of available resources).
About the Author:
Ekta is a very active contributor on Geeksforgeeks. Currently studying at Delhi Technological University.She has also made a Chrome extention for www.geeksquiz.com to practice MCQs randomly.She can be reached
at github.com/Ekta1994
If you also wish to showcase your blog here, please see GBlog for guest blog writing on GeeksforGeeks.

Source
https://www.geeksforgeeks.org/what-exactly-spooling-is-all-about/
✍
Write a Testimonial

Paging in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Paging in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Paging is a memory management scheme that eliminates the need for contiguous allocation of physical memory. This scheme permits the physical address space of a process to be non – contiguous.
Logical Address or Virtual Address (represented in bits): An address generated by the CPU
Logical Address Space or Virtual Address Space( represented in words or bytes): The set of all logical addresses generated by a program
Physical Address (represented in bits): An address actually available on memory unit
Physical Address Space (represented in words or bytes): The set of all physical addresses corresponding to the logical addresses
Example:
If Logical Address = 31 bit, then Logical Address Space = 231 words = 2 G words (1 G = 230)
If Logical Address Space = 128 M words = 27 * 220 words, then Logical Address = log2 227 = 27 bits
If Physical Address = 22 bit, then Physical Address Space = 222 words = 4 M words (1 M = 220)
If Physical Address Space = 16 M words = 24 * 220 words, then Physical Address = log2 224 = 24 bits
The mapping from virtual to physical address is done by the memory management unit (MMU) which is a hardware device and this mapping is known as paging technique.

The Physical Address Space is conceptually divided into a number of fixed-size blocks, called frames.
The Logical address Space is also splitted into fixed-size blocks, called pages.
Page Size = Frame Size
Let us consider an example:
Physical Address = 12 bits, then Physical Address Space = 4 K words
Logical Address = 13 bits, then Logical Address Space = 8 K words
Page size = frame size = 1 K words (assumption)

Address generated by CPU is divided into
Page number(p): Number of bits required to represent the pages in Logical Address Space or Page number
Page offset(d): Number of bits required to represent particular word in a page or page size of Logical Address Space or word number of a page or page offset.
Physical Address is divided into
Frame number(f): Number of bits required to represent the frame of Physical Address Space or Frame number.
Frame offset(d): Number of bits required to represent particular word in a frame or frame size of Physical Address Space or word number of a frame or frame offset.

The hardware implementation of page table can be done by using dedicated registers. But the usage of register for the page table is satisfactory only if page table is small. If page table contain large number of entries then
we can use TLB(translation Look-aside buffer), a special, small, fast look up hardware cache.
The TLB is associative, high speed memory.
Each entry in TLB consists of two parts: a tag and a value.
When this memory is used, then an item is compared with all tags simultaneously.If the item is found, then corresponding value is returned.

Main memory access time = m
If page table are kept in main memory,
Effective access time = m(for page table) + m(for particular page in page table)

Questions asked in GATE on Paging:
GATE CS 2001 Question 46

This article has been contributed by Vikash Kumar. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above

Source
https://www.geeksforgeeks.org/paging-in-operating-system/
✍
Write a Testimonial

Cache Memory in Computer Organization
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Cache Memory in Computer Organization - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Cache Memory is a special very high-speed memory. It is used to speed up and synchronizing with high-speed CPU. Cache memory is costlier than main memory or disk memory but economical than CPU registers.
Cache memory is an extremely fast memory type that acts as a buffer between RAM and the CPU. It holds frequently requested data and instructions so that they are immediately available to the CPU when needed.
Cache memory is used to reduce the average time to access data from the Main memory. The cache is a smaller and faster memory which stores copies of the data from frequently used main memory locations. There are
various different independent caches in a CPU, which store instructions and data.

Levels of memory:
Level 1 or Register –
It is a type of memory in which data is stored and accepted that are immediately stored in CPU. Most commonly used register is accumulator, Program counter, address register etc.
Level 2 or Cache memory –
It is the fastest memory which has faster access time where data is temporarily stored for faster access.
Level 3 or Main Memory –
It is memory on which computer works currently. It is small in size and once power is off data no longer stays in this memory.
Level 4 or Secondary Memory –
It is external memory which is not as fast as main memory but data stays permanently in this memory.

Cache Performance:
When the processor needs to read or write a location in main memory, it first checks for a corresponding entry in the cache.
If the processor finds that the memory location is in the cache, a cache hit has occurred and data is read from cache
If the processor does not find the memory location in the cache, a cache miss has occurred. For a cache miss, the cache allocates a new entry and copies in data from main memory, then the request is fulfilled from
the contents of the cache.
The performance of cache memory is frequently measured in terms of a quantity called Hit ratio.
Hit ratio = hit / (hit + miss) =

no. of hits/total accesses

We can improve Cache performance using higher cache block size, higher associativity, reduce miss rate, reduce miss penalty, and reduce Reduce the time to hit in the cache.
Cache Mapping:
There are three different types of mapping used for the purpose of cache memory which are as follows: Direct mapping, Associative mapping, and Set-Associative mapping. These are explained below.
1. Direct Mapping –
The simplest technique, known as direct mapping, maps each block of main memory into only one possible cache line. or
In Direct mapping, assigne each memory block to a specific line in the cache. If a line is previously taken up by a memory block when a new block needs to be loaded, the old block is trashed. An address space is
split into two parts index field and a tag field. The cache is used to store the tag field whereas the rest is stored in the main memory. Direct mapping`s performance is directly proportional to the Hit ratio.
i = j modulo m​
where​
i=cache line number​
j= main memory block number​
m=number of lines in the cache

For purposes of cache access, each main memory address can be viewed as consisting of three fields. The least significant w bits identify a unique word or byte within a block of main memory. In most contemporary
machines, the address is at the byte level. The remaining s bits specify one of the 2s blocks of main memory. The cache logic interprets these s bits as a tag of s-r bits (most significant portion) and a line field of r
bits. This latter field identifies one of the m=2 r lines of the cache.

2. Associative Mapping –
In this type of mapping, the associative memory is used to store content and addresses of the memory word. Any block can go into any line of the cache. This means that the word id bits are used to identify which
word in the block is needed, but the tag becomes all of the remaining bits. This enables the placement of any word at any place in the cache memory. It is considered to be the fastest and the most flexible mapping
form.

3. Set-associative Mapping –
This form of mapping is an enhanced form of direct mapping where the drawbacks of direct mapping are removed. Set associative addresses the problem of possible thrashing in the direct mapping method. It does
this by saying that instead of having exactly one line that a block can map to in the cache, we will group a few lines together creating a set. Then a block in memory can map to any one of the lines of a specific
set..Set-associative mapping allows that each word that is present in the cache can have two or more words in the main memory for the same index address. Set associative cache mapping combines the best of direct
and associative cache mapping techniques.
In this case, the cache consists of a number of sets, each of which consists of a number of lines. The relationships are
m = v * k​
i= j mod v​
​
where​
i=cache set number​
j=main memory block number​
v=number of sets​
m=number of lines in the cache number of sets ​
k=number of lines in each set

Application of Cache Memory –
1. Usually, the cache memory can store a reasonable number of blocks at any given time, but this number is small compared to the total number of blocks in the main memory.
2. The correspondence between the main memory blocks and those in the cache is specified by a mapping function.
Types of Cache –
Primary Cache –
A primary cache is always located on the processor chip. This cache is small and its access time is comparable to that of processor registers.
Secondary Cache –
Secondary cache is placed between the primary cache and the rest of the memory. It is referred to as the level 2 (L2) cache. Often, the Level 2 cache is also housed on the processor chip.
Locality of reference –
Since size of cache memory is less as compared to main memory. So to check which part of main memory should be given priority and loaded in cache is decided based on locality of reference.
Types of Locality of reference
1. Spatial Locality of reference
This says that there is a chance that element will be present in the close proximity to the reference point and next time if again searched then more close proximity to the point of reference.
2. Temporal Locality of reference
In this Least recently used algorithm will be used. Whenever there is page fault occurs within a word will not only load word in main memory but complete page fault will be loaded because spatial locality of
reference rule says that if you are referring any word next word will be referred in its register that’s why we load complete page table so the complete block will be loaded.

GATE Practice Questions –
Que-1: A computer has a 256 KByte, 4-way set associative, write back data cache with the block size of 32 Bytes. The processor sends 32-bit addresses to the cache controller. Each cache tag directory entry
contains, in addition, to address tag, 2 valid bits, 1 modified bit and 1 replacement bit. The number of bits in the tag field of an address is
(A)
(B)
(C)
(D)

11​
14​
16​
27

Answer: (C)
Explanation: https://www.geeksforgeeks.org/gate-gate-cs-2012-question-54/
Que-2: Consider the data given in previous question. The size of the cache tag directory is
(A)
(B)
(C)
(D)

160 Kbits​
136 bits​
40 Kbits​
32 bits

Answer: (A)
Explanation: https://www.geeksforgeeks.org/gate-gate-cs-2012-question-55/
Que-3: An 8KB direct-mapped write-back cache is organized as multiple blocks, each of size 32-bytes. The processor generates 32-bit addresses. The cache controller maintains the tag information for each cache
block comprising of the following.
1 Valid bit​
1 Modified bit

As many bits as the minimum needed to identify the memory block mapped in the cache. What is the total size of memory needed at the cache controller to store meta-data (tags) for the cache?
(A)
(B)
(C)
(D)

4864
6144
6656
5376

bits​
bits​
bits​
bits

Answer: (D)
Explanation: https://www.geeksforgeeks.org/gate-gate-cs-2011-question-43/
Article Contributed by Pooja Taneja and Vaishali Bhatia. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.

Improved By : VaibhavRai3, SarswatiPandey, shrutiss48

Source
https://www.geeksforgeeks.org/cache-memory-in-computer-organization/
✍
Write a Testimonial

Readers-Writers Problem | Set 1 (Introduction and Readers Preference Solution)
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Readers-Writers Problem | Set 1 (Introduction and Readers Preference Solution) - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Consider a situation where we have a file shared between many people.
If one of the people tries editing the file, no other person should be reading or writing at the same time, otherwise changes will not be visible to him/her.
However if some person is reading the file, then others may read it at the same time.
Precisely in OS we call this situation as the readers-writers problem
Problem parameters:

One set of data is shared among a number of processes
Once a writer is ready, it performs its write. Only one writer may write at a time
If a process is writing, no other process can read it
If at least one reader is reading, no other process can write
Readers may not write and only read
Solution when Reader has the Priority over Writer
Here priority means, no reader should wait if the share is currently opened for reading.
Three variables are used: mutex, wrt, readcnt to implement solution
1. semaphore mutex, wrt; // semaphore mutex is used to ensure mutual exclusion when readcnt is updated i.e. when any reader enters or exit from the critical section and semaphore wrt is used by both readers and
writers
2. int readcnt; // readcnt tells the number of processes performing read in the critical section, initially 0
Functions for sempahore :
– wait() : decrements the semaphore value.
– signal() : increments the semaphore value.
Writer process:
1. Writer requests the entry to critical section.
2. If allowed i.e. wait() gives a true value, it enters and performs the write. If not allowed, it keeps on waiting.
3. It exits the critical section.
do {​
// writer requests for critical section​
wait(wrt); ​
​
// performs the write​
​
// leaves the critical section​
signal(wrt);​
​
} while(true);

Reader process:
1. Reader requests the entry to critical section.
2. If allowed:
it increments the count of number of readers inside the critical section. If this reader is the first reader entering, it locks the wrt semaphore to restrict the entry of writers if any reader is inside.
It then, signals mutex as any other reader is allowed to enter while others are already reading.
After performing reading, it exits the critical section. When exiting, it checks if no more reader is inside, it signals the semaphore “wrt” as now, writer can enter the critical section.
3. If not allowed, it keeps on waiting.
do {​
​
// Reader wants to enter the critical section​
wait(mutex);​
​
// The number of readers has now increased by 1​
readcnt++;
​
​
// there is atleast one reader in the critical section​
// this ensure no writer can enter if there is even one reader​
// thus we give preference to readers here​
if (readcnt==1)
​
wait(wrt);
​
​

// other readers can enter while this current reader is inside ​
// the critical section​
signal(mutex);
​
​
// current reader performs reading here​
wait(mutex);
// a reader wants to leave​
​
readcnt--;​
​
// that is, no reader is left in the critical section,​
if (readcnt == 0) ​
signal(wrt);
// writers can enter​
​
signal(mutex); // reader leaves​
​
} while(true);

Thus, the semaphore ‘wrt‘ is queued on both readers and writers in a manner such that preference is given to readers if writers are also there. Thus, no reader is waiting simply because a writer has requested to enter the
critical section.
Article contributed by Ekta Goel. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above

Improved By : Ankit Kumar Singh 2

Source
https://www.geeksforgeeks.org/readers-writers-problem-set-1-introduction-and-readers-preference-solution/
✍
Write a Testimonial

Disk Scheduling Algorithms
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Disk Scheduling Algorithms - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Disk scheduling is done by operating systems to schedule I/O requests arriving for the disk. Disk scheduling is also known as I/O scheduling.
Disk scheduling is important because:
Multiple I/O requests may arrive by different processes and only one I/O request can be served at a time by the disk controller. Thus other I/O requests need to wait in the waiting queue and need to be scheduled.
Two or more request may be far from each other so can result in greater disk arm movement.
Hard drives are one of the slowest parts of the computer system and thus need to be accessed in an efficient manner.
There are many Disk Scheduling Algorithms but before discussing them let’s have a quick look at some of the important terms:
Seek Time:Seek time is the time taken to locate the disk arm to a specified track where the data is to be read or write. So the disk scheduling algorithm that gives minimum average seek time is better.
Rotational Latency: Rotational Latency is the time taken by the desired sector of disk to rotate into a position so that it can access the read/write heads. So the disk scheduling algorithm that gives minimum
rotational latency is better.
Transfer Time: Transfer time is the time to transfer the data. It depends on the rotating speed of the disk and number of bytes to be transferred.
Disk Access Time: Disk Access Time is:

​
Disk Access Time = Seek Time + ​
Rotational Latency + ​
Transfer Time

Disk Response Time: Response Time is the average of time spent by a request waiting to perform its I/O operation. Average Response time is the response time of the all requests. Variance Response Time is
measure of how individual request are serviced with respect to average response time. So the disk scheduling algorithm that gives minimum variance response time is better.
Disk Scheduling Algorithms
1. FCFS: FCFS is the simplest of all the Disk Scheduling Algorithms. In FCFS, the requests are addressed in the order they arrive in the disk queue.
Advantages:
Every request gets a fair chance
No indefinite postponement
Disadvantages:
Does not try to optimize seek time
May not provide the best possible service
2. SSTF: In SSTF (Shortest Seek Time First), requests having shortest seek time are executed first. So, the seek time of every request is calculated in advance in the queue and then they are scheduled according to their
calculated seek time. As a result, the request near the disk arm will get executed first. SSTF is certainly an improvement over FCFS as it decreases the average response time and increases the throughput of system.
Advantages:
Average Response Time decreases
Throughput increases
Disadvantages:
Overhead to calculate seek time in advance
Can cause Starvation for a request if it has higher seek time as compared to incoming requests
High variance of response time as SSTF favours only some requests
3. SCAN: In SCAN algorithm the disk arm moves into a particular direction and services the requests coming in its path and after reaching the end of disk, it reverses its direction and again services the request arriving
in its path. So, this algorithm works as an elevator and hence also known as elevator algorithm. As a result, the requests at the midrange are serviced more and those arriving behind the disk arm will have to wait.

Advantages:
High throughput
Low variance of response time
Average response time
Disadvantages:
Long waiting time for requests for locations just visited by disk arm
4. CSCAN: In SCAN algorithm, the disk arm again scans the path that has been scanned, after reversing its direction. So, it may be possible that too many requests are waiting at the other end or there may be zero or
few requests pending at the scanned area.
These situations are avoided in CSCAN algorithm in which the disk arm instead of reversing its direction goes to the other end of the disk and starts servicing the requests from there. So, the disk arm moves in a circular
fashion and this algorithm is also similar to SCAN algorithm and hence it is known as C-SCAN (Circular SCAN).
Advantages:
Provides more uniform wait time compared to SCAN
5. LOOK: It is similar to the SCAN disk scheduling algorithm except for the difference that the disk arm in spite of going to the end of the disk goes only to the last request to be serviced in front of the head and then
reverses its direction from there only. Thus it prevents the extra delay which occurred due to unnecessary traversal to the end of the disk.
6. CLOOK: As LOOK is similar to SCAN algorithm, in similar way, CLOOK is similar to CSCAN disk scheduling algorithm. In CLOOK, the disk arm in spite of going to the end goes only to the last request to be
serviced in front of the head and then from there goes to the other end’s last request. Thus, it also prevents the extra delay which occurred due to unnecessary traversal to the end of the disk.
Each algorithm is unique in its own way. Overall Performance depends on the number and type of requests.
Note:Average Rotational latency is generally taken as 1/2(Rotational latency).
Exercise
1) Suppose a disk has 201 cylinders, numbered from 0 to 200. At some time the disk arm is at cylinder 100, and there is a queue of disk access requests for cylinders 30, 85, 90, 100, 105, 110, 135 and 145. If Shortest-Seek
Time First (SSTF) is being used for scheduling the disk access, the request for cylinder 90 is serviced after servicing ____________ number of requests. (GATE CS 2014
(A) 1
(B) 2
(C) 3
(D) 4
See this for solution.
2) Consider an operating system capable of loading and executing a single sequential user process at a time. The disk head scheduling algorithm used is First Come First Served (FCFS). If FCFS is replaced by Shortest
Seek Time First (SSTF), claimed by the vendor to give 50% better benchmark results, what is the expected improvement in the I/O performance of user programs? (GATE CS 2004)
(A) 50%
(B) 40%
(C) 25%
(D) 0%
See this for solution.
3) Suppose the following disk request sequence (track numbers) for a disk with 100 tracks is given: 45, 20, 90, 10, 50, 60, 80, 25, 70. Assume that the initial position of the R/W head is on track 50. The additional distance
that will be traversed by the R/W head when the Shortest Seek Time First (SSTF) algorithm is used compared to the SCAN (Elevator) algorithm (assuming that SCAN algorithm moves towards 100 when it starts
execution) is _________ tracks
(A) 8
(B) 9
(C) 10
(D) 11
See this for solution.
4) Consider a typical disk that rotates at 15000 rotations per minute (RPM) and has a transfer rate of 50 × 10^6 bytes/sec. If the average seek time of the disk is twice the average rotational delay and the controller’s
transfer time is 10 times the disk transfer time, the average time (in milliseconds) to read or write a 512 byte sector of the disk is _____________
See this for solution.
This article is contributed by Ankit Mittal. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.

Improved By : VaibhavRai3, Majorssn

Source
https://www.geeksforgeeks.org/disk-scheduling-algorithms/
✍
Write a Testimonial

Difference between Priority Inversion and Priority Inheritance
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between Priority Inversion and Priority Inheritance - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Both of these concepts come under Priority scheduling in Operating System. But are they same ?
In one line, Priority Inversion is a problem while Priority Inheritance is a solution. Literally, Priority Inversion means that priority of tasks get inverted and Priority Inheritance means that priority of tasks get inherited.
Both of these phenomena happen in priority scheduling. Basically, in Priority Inversion, higher priority task (H) ends up waiting for middle priority task (M) when H is sharing critical section with lower priority task (L)
and L is already in critical section. Effectively, H waiting for M results in inverted priority i.e. Priority Inversion. One of the solution for this problem is Priority Inheritance. In Priority Inheritance, when L is in critical
section, L inherits priority of H at the time when H starts pending for critical section. By doing so, M doesn’t interrupt L and H doesn’t wait for M to finish. Please note that inheriting of priority is done temporarily i.e. L
goes back to its old priority when L comes out of critical section.
More details on these can be found here.

Please do Like/Tweet/G+1 if you find the above useful. Also, please do leave us comment for further clarification or info. We would love to help and learn

Source
https://www.geeksforgeeks.org/difference-between-priority-inversion-and-priority-inheritance/
✍
Write a Testimonial

Priority Inversion : What the heck !
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Priority Inversion : What the heck ! - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Let us first put ‘priority inversion’ in the context of the Big Picture i.e. where does this come from.
In Operating System, one of the important concepts is Task Scheduling. There are several Scheduling methods such as First Come First Serve, Round Robin, Priority-based scheduling, etc. Each scheduling method has its
pros and cons. As you might have guessed, Priority Inversion comes under Priority-based Scheduling. Basically, it’s a problem which arises sometimes when Priority-based scheduling is used by OS. In Priority-based
scheduling, different tasks are given different priority so that higher priority tasks can intervene in lower priority tasks if possible.
So, in priority-based scheduling, if lower priority task (L) is running and if a higher priority task (H) also needs to run, the lower priority task (L) would be preempted by higher priority task (H). Now, suppose both lower
and higher priority tasks need to share a common resource (say access to the same file or device) to achieve their respective work. In this case, since there are resource sharing and task synchronization is needed, several
methods/techniques can be used for handling such scenarios. For sake of our topic on Priority Inversion, let us mention a synchronization method say mutex. Just to recap on the mutex, a task acquires mutex before
entering critical section (CS) and releases mutex after exiting critical section (CS). While running in CS, task access this common resource. More details on this can be referred here. Now, say both L and H shares a
common Critical Section (CS) i.e. the same mutex is needed for this CS.

Coming to our discussion of priority inversion, let us examine some scenarios.
1) L is running but not in CS; H needs to run; H preempts L; H starts running; H relinquishes or releases control; L resumes and starts running
2) L is running in CS; H needs to run but not in CS; H preempts L; H starts running; H relinquishes control; L resumes and starts running.
3) L is running in CS; H also needs to run in CS; H waits for L to come out of CS; L comes out of CS; H enters CS and starts running
Please note that the above scenarios don’t show the problem of any Priority Inversion (not even scenario 3). Basically, so long as lower priority task isn’t running in shared CS, higher priority task can preempt it. But if L is
running in shared CS and H also needs to run in CS, H waits until L comes out of CS. The idea is that CS should be small enough so that it doesn’t result in H waiting for a long time while L was in CS. That’s why writing
a CS code requires careful consideration. In any of the above scenarios, priority inversion (i.e. reversal of priority) didn’t occur because the tasks are running as per the design.
Now let us add another task of middle priority say M. Now the task priorities are in the order of L < M < H. In our example, M doesn’t share the same Critical Section (CS). In this case, the following sequence of task
running would result in ‘Priority Inversion’ problem.
4) L is running in CS ; H also needs to run in CS ; H waits for L to come out of CS ; M interrupts L and starts running ; M runs till completion and relinquishes control ; L resumes and starts running till the end of CS ; H
enters CS and starts running.
Note that neither L nor H share CS with M.
Here, we can see that running of M has delayed the running of both L and H. Precisely speaking, H is of higher priority and doesn’t share CS with M; but H had to wait for M. This is where Priority based scheduling didn’t
work as expected because priorities of M and H got inverted in spite of not sharing any CS. This problem is called Priority Inversion. This is what the heck was Priority Inversion! In a system with priority based
scheduling, higher priority tasks can face this problem and it can result in unexpected behavior/result. In general purpose OS, it can result in slower performance. In RTOS, it can result in more severe outcomes. The most
famous ‘Priority Inversion’ problem was what happened at Mars Pathfinder.
If we have a problem, there has to be solution for this. For Priority Inversion as well, there’re different solutions such as Priority Inheritance, etc. This is going to be our next article
But for the inpatients, this can be referred for time being.

Improved By : Akanksha_Rai

Source
https://www.geeksforgeeks.org/priority-inversion-what-the-heck/
✍
Write a Testimonial

Multi Threading Models in Process Management
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Multi Threading Models in Process Management - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Many operating systems support kernel thread and user thread in a combined way. Example of such system is Solaris. Multi threading model are of three types.
Many to many model.​
Many to one model.​
one to one model.

Many to Many Model
In this model, we have multiple user threads multiplex to same or lesser number of kernel level threads. Number of kernel level threads are specific to the machine, advantage of this model is if a user thread is blocked we
can schedule others user thread to other kernel thread. Thus, System doesn’t block if a particular thread is blocked.

Many to One Model
In this model, we have multiple user threads mapped to one kernel thread. In this model when a user thread makes a blocking system call entire process blocks. As we have only one kernel thread and only one user thread
can access kernel at a time, so multiple threads are not able access multiprocessor at the same time.

One to One Model
In this model, one to one relationship between kernel and user thread. In this model multiple thread can run on multiple processor. Problem with this model is that creating a user thread requires the corresponding kernel
thread.

Source
https://www.geeksforgeeks.org/multi-threading-models-in-process-management/
✍
Write a Testimonial

Page Replacement Algorithms in Operating Systems
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Page Replacement Algorithms in Operating Systems - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
In an operating system that uses paging for memory management, a page replacement algorithm is needed to decide which page needs to be replaced when new page comes in.
Page Fault – A page fault happens when a running program accesses a memory page that is mapped into the virtual address space, but not loaded in physical memory.
Since actual physical memory is much smaller than virtual memory, page faults happen. In case of page fault, Operating System might have to replace one of the existing pages with the newly needed page. Different page
replacement algorithms suggest different ways to decide which page to replace. The target for all algorithms is to reduce the number of page faults.

Page Replacement Algorithms :
First In First Out (FIFO) –
This is the simplest page replacement algorithm. In this algorithm, the operating system keeps track of all pages in the memory in a queue, the oldest page is in the front of the queue. When a page needs to be
replaced page in the front of the queue is selected for removal.
Example-1Consider page reference string 1, 3, 0, 3, 5, 6 with 3 page frames.Find number of page faults.

Initially all slots are empty, so when 1, 3, 0 came they are allocated to the empty slots —> 3 Page Faults.
when 3 comes, it is already in memory so —> 0 Page Faults.
Then 5 comes, it is not available in memory so it replaces the oldest page slot i.e 1. —>1 Page Fault.
6 comes, it is also not available in memory so it replaces the oldest page slot i.e 3 —>1 Page Fault.
Finally when 3 come it is not avilable so it replaces 0 1 page fault
Belady’s anomaly – Belady’s anomaly proves that it is possible to have more page faults when increasing the number of page frames while using the First in First Out (FIFO) page replacement algorithm. For
example, if we consider reference string 3, 2, 1, 0, 3, 2, 4, 3, 2, 1, 0, 4 and 3 slots, we get 9 total page faults, but if we increase slots to 4, we get 10 page faults.
Optimal Page replacement –
In this algorithm, pages are replaced which would not be used for the longest duration of time in the future.
Example-2:Consider the page references 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, with 4 page frame. Find number of page fault.

Initially all slots are empty, so when 7 0 1 2 are allocated to the empty slots —> 4 Page faults
0 is already there so —> 0 Page fault.
when 3 came it will take the place of 7 because it is not used for the longest duration of time in the future.—>1 Page fault.
0 is already there so —> 0 Page fault..
4 will takes place of 1 —> 1 Page Fault.
Now for the further page reference string —> 0 Page fault because they are already available in the memory.
Optimal page replacement is perfect, but not possible in practice as the operating system cannot know future requests. The use of Optimal Page replacement is to set up a benchmark so that other replacement
algorithms can be analyzed against it.

Least Recently Used –
In this algorithm page will be replaced which is least recently used.
Example-3Consider the page reference string 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2 with 4 page frames.Find number of page faults.

Initially all slots are empty, so when 7 0 1 2 are allocated to the empty slots —> 4 Page faults
0 is already their so —> 0 Page fault.
when 3 came it will take the place of 7 because it is least recently used —>1 Page fault
0 is already in memory so —> 0 Page fault.
4 will takes place of 1 —> 1 Page Fault
Now for the further page reference string —> 0 Page fault because they are already available in the memory.
GATE CS Corner Questions
Practicing the following questions will help you test your knowledge. All questions have been asked in GATE in previous years or in GATE Mock Tests. It is highly recommended that you practice them.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.

Memory Management | Question 1
Memory Management | Question 10
GATE CS 2014 (Set-1), Question 65
GATE CS 2012, Question 40
GATE CS 2007, Question 56
GATE CS 2007, Question 82
GATE CS 2007, Question 83
GATE CS 2014 (Set-3), Question 65
GATE CS 2002 Question 23
GATE CS 2001, Question 21
GATE CS 2010, Question 24

Reference –
Bélády’s anomaly
This article has been improved by RajshreeSrivastava. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above

Improved By : Abdul Mohsin, PratikRoy, VaibhavRai3

Source
https://www.geeksforgeeks.org/page-replacement-algorithms-in-operating-systems/
✍
Write a Testimonial

Thread in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Thread in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
What is a Thread?
A thread is a path of execution within a process. A process can contain multiple threads.
Why Multithreading?
A thread is also known as lightweight process. The idea is to achieve parallelism by dividing a process into multiple threads. For example, in a browser, multiple tabs can be different threads. MS Word uses multiple
threads: one thread to format the text, another thread to process inputs, etc. More advantages of multithreading are discussed below
Process vs Thread?
The primary difference is that threads within the same process run in a shared memory space, while processes run in separate memory spaces.
Threads are not independent of one another like processes are, and as a result threads share with other threads their code section, data section, and OS resources (like open files and signals). But, like process, a thread has
its own program counter (PC), register set, and stack space.
Advantages of Thread over Process
1. Responsiveness: If the process is divided into multiple threads, if one thread completes its execution, then its output can be immediately returned.
2. Faster context switch: Context switch time between threads is lower compared to process context switch. Process context switching requires more overhead from the CPU.
3. Effective utilization of multiprocessor system: If we have multiple threads in a single process, then we can schedule multiple threads on multiple processor. This will make process execution faster.

4. Resource sharing: Resources like code, data, and files can be shared among all threads within a process.
Note: stack and registers can’t be shared among the threads. Each thread has its own stack and registers.
5. Communication: Communication between multiple threads is easier, as the threads shares common address space. while in process we have to follow some specific communication technique for communication between
two process.
6. Enhanced throughput of the system: If a process is divided into multiple threads, and each thread function is considered as one job, then the number of jobs completed per unit of time is increased, thus increasing the
throughput of the system.
Types of Threads
There are two types of threads.
User Level Thread
Kernel Level Thread
Refer User Thread vs Kernel Thread for more details.
Below are previous years’ gate questions on threads:
http://quiz.geeksforgeeks.org/gate-gate-cs-2011-question-16/
http://quiz.geeksforgeeks.org/gate-gate-cs-2007-question-17/
http://quiz.geeksforgeeks.org/gate-gate-cs-2014-set-1-question-30/
Reference:
Multithreading in C

Improved By : chrismaher37

Source
https://www.geeksforgeeks.org/thread-in-operating-system/
✍
Write a Testimonial

Partition Allocation Methods in Memory Management
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Partition Allocation Methods in Memory Management - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
In the operating system, the following are four common memory management techniques.
Single contiguous allocation: Simplest allocation method used by MS-DOS. All memory (except some reserved for OS) is available to a process.
Partitioned allocation: Memory is divided in different blocks or partitions.Each process is allocated accroding to the requirment.

Paged memory management: Memory is divided into fixed sized units called page frames, used in a virtual memory environment.
Segmented memory management: Memory is divided in different segments (a segment is a logical grouping of the process’ data or code).In this management, allocated memory doesn’t have to be contiguous.
Most of the operating systems (for example Windows and Linux) use Segmentation with Paging. A process is divided into segments and individual segments have pages.
In Partition Allocation, when there is more than one partition freely available to accommodate a process’s request, a partition must be selected. To choose a particular partition, a partition allocation method is needed. A
partition allocation method is considered better if it avoids internal fragmentation.

Below are the various partition allocation schemes :
1. First Fit: In the first fit, the partition is allocated which is first sufficient block from the top of Main Memory.
2. Best Fit Allocate the process to the partition which is the first smallest sufficient partition among the free available partition.
3. Worst Fit Allocate the process to the partition which is the largest sufficient among the freely available partitions available in the main memory.
4. Next Fit Next fit is similar to the first fit but it will search for the first sufficient partition from the last allocation point.
Is Best-Fit really best?
Although best fit minimizes the wastage space, it consumes a lot of processor time for searching the block which is close to the required size. Also, Best-fit may perform poorer than other algorithms in some cases. For
example, see below exercise.

Exercise: Consider the requests from processes in given order 300K, 25K, 125K and 50K. Let there be two blocks of memory available of size 150K followed by a block size 350K.
Which of the following partition allocation schemes can satisfy above requests?
A) Best fit but not first fit.
B) First fit but not best fit.
C) Both First fit & Best fit.
D) neither first fit nor best fit.
Solution: Let us try all options.
Best Fit:
300K is allocated from block of size 350K. 50 is left in the block.
25K is allocated from the remaining 50K block. 25K is left in the block.
125K is allocated from 150 K block. 25K is left in this block also.
50K can’t be allocated even if there is 25K + 25K space available.
First Fit:
300K request is allocated from 350K block, 50K is left out.
25K is be allocated from 150K block, 125K is left out.
Then 125K and 50K are allocated to remaining left out partitions.
So, first fit can handle requests.
So option B is the correct choice.

Improved By : VaibhavRai3

Source
https://www.geeksforgeeks.org/partition-allocation-methods-in-memory-management/
✍
Write a Testimonial

Difference between User Level thread and Kernel Level thread
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Difference between User Level thread and Kernel Level thread - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
User level thread
User thread are implemented by users.
OS doesn’t recognized user level threads.
Implementation of User threads is easy.
Context switch time is less.
Context switch requires no hardware support.
If one user level thread perform blocking operation then entire process will be blocked.
Example : Java thread, POSIX threads.

Kernel level thread
kernel threads are implemented by OS.
Kernel threads are recognized by OS.
Implementation of Kernel thread is complicated.
Context switch time is more.
Hardware support is needed.
If one kernel thread perform blocking operation then another thread can continue execution.
Example : Window Solaris.

Below is the Previous Year Gate Question
http://quiz.geeksforgeeks.org/gate-gate-cs-2007-question-17/
References:
http://www.cs.iit.edu/~cs561/cs450/ChilkuriDineshThreads/dinesh%27s%20files/User%20and%20Kernel%20Level%20Threads.html

Source
https://www.geeksforgeeks.org/difference-between-user-level-thread-and-kernel-level-thread/
✍
Write a Testimonial

Monitors in Process Synchronization
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Monitors in Process Synchronization - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
The monitor is one of the ways to achieve Process synchronization. The monitor is supported by programming languages to achieve mutual exclusion between processes. For example Java Synchronized methods. Java
provides wait() and notify() constructs.
1. It is the collection of condition variables and procedures combined together in a special kind of module or a package.
2. The processes running outside the monitor can’t access the internal variable of the monitor but can call procedures of the monitor.

3. Only one process at a time can execute code inside monitors.
Syntax:

Condition Variables:
Two different operations are performed on the condition variables of the monitor.
​
Wait.​
signal.​

let say we have 2 condition variables
condition x, y; // Declaring variable

Wait operation
x.wait() : Process performing wait operation on any condition variable are suspended. The suspended processes are placed in block queue of that condition variable.
Note: Each condition variable has its unique block queue.

Signal operation
x.signal(): When a process performs signal operation on condition variable, one of the blocked processes is given chance.
If (x block queue empty)​
// Ignore signal​
else​
// Resume a process from block queue.

Advantages of Monitor:
Monitors have the advantage of making parallel programming easier and less error prone than using techniques such as semaphore.
Disadvantages of Monitor:
Monitors have to be implemented as part of the programming language . The compiler must generate code for them. This gives the compiler the additional burden of having to know what operating system facilities are
available to control access to critical sections in concurrent processes. Some languages that do support monitors are Java,C#,Visual Basic,Ada and concurrent Euclid.

Improved By : shivanshukumarsingh1

Source
https://www.geeksforgeeks.org/monitors-in-process-synchronization/
✍
Write a Testimonial

Deadlock Detection And Recovery
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Deadlock Detection And Recovery - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
In the previous post, we have discussed Deadlock Prevention and Avoidance. In this post, Deadlock Detection and Recovery technique to handle deadlock is discussed.
Deadlock Detection
1. If resources have single instance:
In this case for Deadlock detection we can run an algorithm to check for cycle in the Resource Allocation Graph. Presence of cycle in the graph is the sufficient condition for deadlock.

In the above diagram, resource 1 and resource 2 have single instances. There is a cycle R1 → P1 → R2 → P2. So, Deadlock is Confirmed.
2. If there are multiple instances of resources:
Detection of the cycle is necessary but not sufficient condition for deadlock detection, in this case, the system may or may not be in deadlock varies according to different situations.
Deadlock Recovery
A traditional operating system such as Windows doesn’t deal with deadlock recovery as it is time and space consuming process. Real-time operating systems use Deadlock recovery.
Recovery method
1. Killing the process: killing all the process involved in the deadlock. Killing process one by one. After killing each process check for deadlock again keep repeating the process till system recover from deadlock.
2. Resource Preemption: Resources are preempted from the processes involved in the deadlock, preempted resources are allocated to other processes so that there is a possibility of recovering the system from
deadlock. In this case, the system goes into starvation.
See Quiz on Deadlock.

Improved By : VaibhavRai3

Source
https://www.geeksforgeeks.org/deadlock-detection-recovery/
✍
Write a Testimonial

Deadlock Prevention And Avoidance
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Deadlock Prevention And Avoidance - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Deadlock Characteristics
As discussed in the previous post, deadlock has following characteristics.
1.
2.
3.
4.

Mutual Exclusion
Hold and Wait
No preemption
Circular wait

Deadlock Prevention
We can prevent Deadlock by eliminating any of the above four conditions.

Eliminate Mutual Exclusion
It is not possible to dis-satisfy the mutual exclusion because some resources, such as the tape drive and printer, are inherently non-shareable.

Eliminate Hold and wait
1. Allocate all required resources to the process before the start of its execution, this way hold and wait condition is eliminated but it will lead to low device utilization. for example, if a process requires printer at a later
time and we have allocated printer before the start of its execution printer will remain blocked till it has completed its execution.
2. The process will make a new request for resources after releasing the current set of resources. This solution may lead to starvation.

Eliminate No Preemption
Preempt resources from the process when resources required by other high priority processes.

Eliminate Circular Wait
Each resource will be assigned with a numerical number. A process can request the resources increasing/decreasing. order of numbering.
For Example, if P1 process is allocated R5 resources, now next time if P1 ask for R4, R3 lesser than R5 such request will not be granted, only request for resources more than R5 will be granted.

Deadlock Avoidance
Deadlock avoidance can be done with Banker’s Algorithm.
Banker’s Algorithm
Bankers’s Algorithm is resource allocation and deadlock avoidance algorithm which test all the request made by processes for resources, it checks for the safe state, if after granting request system remains in the safe state
it allows the request and if there is no safe state it doesn’t allow the request made by the process.
Inputs to Banker’s Algorithm:
1. Max need of resources by each process.
2. Currently allocated resources by each process.
3. Max free available resources in the system.
The request will only be granted under the below condition:
1. If the request made by the process is less than equal to max need to that process.
2. If the request made by the process is less than equal to the freely available resource in the system.
Example:
Total resources in system:​
A B C D​
6 5 7 6
Available system resources are:​
A B C D​
3 1 1 2
Processes
A B C
P1 1 2 2
P2 1 0 3
P3 1 2 1

(currently allocated resources):​
D​
1​
3​
0

Processes (maximum resources):​
A B C D​
P1 3 3 2 2​

P2
P3

1 2 3 4​
1 3 5 0

Need = maximum resources - currently allocated resources.​
Processes (need resources):​
A B C D​
P1 2 1 0 1​
P2 0 2 0 1​
P3 0 1 4 0

Note:Deadlock prevention is more strict that Deadlock Avoidance.
Following are Gate Previous Year Question
http://quiz.geeksforgeeks.org/gate-gate-cs-2014-set-1-question-41/
http://quiz.geeksforgeeks.org/gate-gate-cs-2014-set-3-question-41/
http://quiz.geeksforgeeks.org/gate-gate-cs-2010-question-46/
References
https://en.wikipedia.org/wiki/Banker’s_algorithm

Improved By : VaibhavRai3

Source
https://www.geeksforgeeks.org/deadlock-prevention/
✍
Write a Testimonial

Introduction of Deadlock in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Introduction of Deadlock in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
A process in operating systems uses different resources and uses resources in following way.
1) Requests a resource
2) Use the resource
2) Releases the resource
Deadlock is a situation where a set of processes are blocked because each process is holding a resource and waiting for another resource acquired by some other process.
Consider an example when two trains are coming toward each other on same track and there is only one track, none of the trains can move once they are in front of each other. Similar situation occurs in operating systems
when there are two or more processes hold some resources and wait for resources held by other(s). For example, in the below diagram, Process 1 is holding Resource 1 and waiting for resource 2 which is acquired by
process 2, and process 2 is waiting for resource 1.

Deadlock can arise if following four conditions hold simultaneously (Necessary Conditions)
Mutual Exclusion: One or more than one resource are non-sharable (Only one process can use at a time)
Hold and Wait: A process is holding at least one resource and waiting for resources.
No Preemption: A resource cannot be taken from a process unless the process releases the resource.
Circular Wait: A set of processes are waiting for each other in circular form.
Methods for handling deadlock
There are three ways to handle deadlock
1) Deadlock prevention or avoidance: The idea is to not let the system into deadlock state.
One can zoom into each category individually, Prevention is done by negating one of above mentioned necessary conditions for deadlock.
Avoidance is kind of futuristic in nature. By using strategy of “Avoidance”, we have to make an assumption. We need to ensure that all information about resources which process WILL need are known to us prior to
execution of the process. We use Banker’s algorithm (Which is in-turn a gift from Dijkstra) in order to avoid deadlock.
2) Deadlock detection and recovery: Let deadlock occur, then do preemption to handle it once occurred.
3) Ignore the problem all together: If deadlock is very rare, then let it happen and reboot the system. This is the approach that both Windows and UNIX take.
Exercise:
1) Suppose n processes, P1, …. Pn share m identical resource units, which can be reserved and released one at a time. The maximum resource requirement of process Pi is Si, where Si > 0. Which one of the following is a
sufficient condition for ensuring that deadlock does not occur? (GATE CS 2005)

(A) A
(B) B
(C) C
(D) D

For solution, see Question 4 of https://www.geeksforgeeks.org/operating-systems-set-16/
See QUIZ ON DEADLOCK for more questions.
References:
http://www2.latech.edu/~box/os/ch07.pdf
http://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/7_Deadlocks.html

Improved By : BJP_supporting_Leftist

Source
https://www.geeksforgeeks.org/introduction-of-deadlock-in-operating-system/
✍
Write a Testimonial

Introduction of Process Synchronization
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Introduction of Process Synchronization - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
On the basis of synchronization, processes are categorized as one of the following two types:
Independent Process : Execution of one process does not affects the execution of other processes.
Cooperative Process : Execution of one process affects the execution of other processes.
Process synchronization problem arises in the case of Cooperative process also because resources are shared in Cooperative processes.
Race Condition
When more than one processes are executing the same code or accessing the same memory or any shared variable in that condition there is a possibility that the output or the value of the shared variable is wrong so for that
all the processes doing race to say that my output is correct this condition known as
race condition.
Several processes access and process the manipulations over the same data concurrently, then the outcome depends on the particular order in which the access takes place.
Critical Section Problem
Critical section is a code segment that can be accessed by only one process at a time. Critical section contains shared variables which need to be synchronized to maintain consistency of data variables.

In the entry section, the process requests for entry in the Critical Section.

Any solution to the critical section problem must satisfy three requirements:
Mutual Exclusion : If a process is executing in its critical section, then no other process is allowed to execute in the critical section.
Progress : If no process is executing in the critical section and other processes are waiting outside the critical section, then only those processes that are not executing in their remainder section can participate in
deciding which will enter in the critical section next, and the selection can not be postponed indefinitely.
Bounded Waiting : A bound must exist on the number of times that other processes are allowed to enter their critical sections after a process has made a request to enter its critical section and before that request is
granted.
Peterson’s Solution
Peterson’s Solution is a classical software based solution to the critical section problem.
In Peterson’s solution, we have two shared variables:
boolean flag[i] :Initialized to FALSE, initially no one is interested in entering the critical section
int turn : The process whose turn is to enter the critical section.

Peterson’s Solution preserves all three conditions :
Mutual Exclusion is assured as only one process can access the critical section at any time.
Progress is also assured, as a process outside the critical section does not block other processes from entering the critical section.
Bounded Waiting is preserved as every process gets a fair chance.
Disadvantages of Peterson’s Solution
It involves Busy waiting
It is limited to 2 processes.

TestAndSet
TestAndSet is a hardware solution to the synchronization problem. In TestAndSet, we have a shared lock variable which can take either of the two values, 0 or 1.
0 Unlock​
1 Lock​

Before entering into the critical section, a process inquires about the lock. If it is locked, it keeps on waiting till it becomes free and if it is not locked, it takes the lock and executes the critical section.
In TestAndSet, Mutual exclusion and progress are preserved but bounded waiting cannot be preserved.
Question : The enter_CS() and leave_CS() functions to implement critical section of a process are realized using test-and-set instruction as follows:

​
int TestAndSet(int &lock) {​
int initial = lock;​
lock = 1;​
return initial;​
}​
​
void enter_CS(X)​
{​
while test-and-set(X) ;​
}​
​
void leave_CS(X)​
{​
X = 0;​
}​

In the above solution, X is a memory location associated with the CS and is initialized to 0. Now, consider the following statements:
I. The above solution to CS problem is deadlock-free
II. The solution is starvation free.
III. The processes enter CS in FIFO order.
IV. More than one process can enter CS at the same time.
Which of the above statements is TRUE?
(A) I
(B) II and III
(C) II and IV
(D) IV
Click here for the Solution.
true
Semaphores
A Semaphore is an integer variable, which can be accessed only through two operations wait() and signal().
There are two types of semaphores : Binary Semaphores and Counting Semaphores
Binary Semaphores : They can only be either 0 or 1. They are also known as mutex locks, as the locks can provide mutual exclusion. All the processes can share the same mutex semaphore that is initialized to 1.
Then, a process has to wait until the lock becomes 0. Then, the process can make the mutex semaphore 1 and start its critical section. When it completes its critical section, it can reset the value of mutex semaphore
to 0 and some other process can enter its critical section.
Counting Semaphores: They can have any value and are not restricted over a certain domain. They can be used to control access to a resource that has a limitation on the number of simultaneous accesses. The
semaphore can be initialized to the number of instances of the resource. Whenever a process wants to use that resource, it checks if the number of remaining instances is more than zero, i.e., the process has an
instance available. Then, the process can enter its critical section thereby decreasing the value of the counting semaphore by 1. After the process is over with the use of the instance of the resource, it can leave the
critical section thereby adding 1 to the number of available instances of the resource.
true

References
www.csee.wvu.edu/~jdmooney/classes/cs550/notes/tech/mutex/Peterson.html
http://iit.qau.edu.pk/books/OS_8th_Edition.pdf
Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above

Improved By : AmishRanjan, vaishnavipandey, shreyashagrawal, heart_bleed

Source
https://www.geeksforgeeks.org/introduction-of-process-synchronization/
✍
Write a Testimonial

CPU Scheduling in Operating Systems
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ CPU Scheduling in Operating Systems - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Scheduling of processes/work is done to finish the work on time.
Below are different time with respect to a process.
Arrival Time: Time at which the process arrives in the ready queue.
Completion Time: Time at which process completes its execution.
Burst Time: Time required by a process for CPU execution.
Turn Around Time: Time Difference between completion time and arrival time.
Turn Around Time = Completion Time – Arrival Time

Waiting Time(W.T): Time Difference between turn around time and burst time.
Waiting Time = Turn Around Time – Burst Time

Why do we need scheduling?
A typical process involves both I/O time and CPU time. In a uni programming system like MS-DOS, time spent waiting for I/O is wasted and CPU is free during this time. In multi programming systems, one process can
use CPU while another is waiting for I/O. This is possible only with process scheduling.

Objectives of Process Scheduling Algorithm
Max CPU utilization [Keep CPU as busy as possible]
Fair allocation of CPU.
Max throughput [Number of processes that complete their execution per time unit]
Min turnaround time [Time taken by a process to finish execution]
Min waiting time [Time a process waits in ready queue]
Min response time [Time when a process produces first response]

First Come First Serve (FCFS): Simplest scheduling algorithm that schedules according to arrival times of processes. First come first serve scheduling algorithm states that the process that requests the CPU first is
allocated the CPU first. It is implemented by using the FIFO queue. When a process enters the ready queue, its PCB is linked onto the tail of the queue. When the CPU is free, it is allocated to the process at the head of the
queue. The running process is then removed from the queue. FCFS is a non-preemptive scheduling algorithm.
Note:First come first serve suffers from convoy effect.
Shortest Job First (SJF): Process which have the shortest burst time are scheduled first.If two processes have the same bust time then FCFS is used to break the tie. It is a non-preemptive scheduling algorithm.

Longest Job First (LJF): It is similar to SJF scheduling algorithm. But, in this scheduling algorithm, we give priority to the process having the longest burst time. This is non-preemptive in nature i.e., when any process
starts executing, can’t be interrupted before complete execution.
Shortest Remaining Time First (SRTF): It is preemptive mode of SJF algorithm in which jobs are schedule according to shortest remaining time.
Longest Remaining Time First (LRTF): It is preemptive mode of LJF algorithm in which we give priority to the process having largest burst time remaining.
Round Robin Scheduling: Each process is assigned a fixed time(Time Quantum/Time Slice) in cyclic way.It is designed especially for the time-sharing system. The ready queue is treated as a circular queue. The CPU
scheduler goes around the ready queue, allocating the CPU to each process for a time interval of up to 1-time quantum. To implement Round Robin scheduling, we keep the ready queue as a FIFO queue of processes. New
processes are added to the tail of the ready queue. The CPU scheduler picks the first process from the ready queue, sets a timer to interrupt after 1-time quantum, and dispatches the process. One of two things will then
happen. The process may have a CPU burst of less than 1-time quantum. In this case, the process itself will release the CPU voluntarily. The scheduler will then proceed to the next process in the ready queue. Otherwise,
if the CPU burst of the currently running process is longer than 1-time quantum, the timer will go off and will cause an interrupt to the operating system. A context switch will be executed, and the process will be put at
the tail of the ready queue. The CPU scheduler will then select the next process in the ready queue.
Priority Based scheduling (Non-Preemptive): In this scheduling, processes are scheduled according to their priorities, i.e., highest priority process is scheduled first. If priorities of two processes match, then schedule
according to arrival time. Here starvation of process is possible.
Highest Response Ratio Next (HRRN): In this scheduling, processes with highest response ratio is scheduled. This algorithm avoids starvation.
Response Ratio = (Waiting Time + Burst time) / Burst time

Multilevel Queue Scheduling: According to the priority of process, processes are placed in the different queues. Generally high priority process are placed in the top level queue. Only after completion of processes from
top level queue, lower level queued processes are scheduled. It can suffer from starvation.
Multi level Feedback Queue Scheduling: It allows the process to move in between queues. The idea is to separate processes according to the characteristics of their CPU bursts. If a process uses too much CPU time, it is

moved to a lower-priority queue.

Some useful facts about Scheduling Algorithms:

1. FCFS can cause long waiting times, especially when the first job takes too much CPU time.
2. Both SJF and Shortest Remaining time first algorithms may cause starvation. Consider a situation when the long process is there in the ready queue and shorter processes keep coming.
3. If time quantum for Round Robin scheduling is very large, then it behaves same as FCFS scheduling.
4. SJF is optimal in terms of average waiting time for a given set of processes,i.e., average waiting time is minimum with this scheduling, but problems are, how to know/predict the time of next job.

Exercise:
1. Consider a system which requires 40-time units of burst time. The Multilevel feedback queue scheduling is used and time quantum is 2 unit for the top queue and is incremented by 5 unit at each level, then in what
queue the process will terminate the execution?

2. Which of the following is false about SJF?
S1: It causes minimum average waiting time
S2: It can cause starvation
(A) Only S1
(B) Only S2
(C) Both S1 and S2
(D) Neither S1 nor S2
Answer (D)
S1 is true SJF will always give minimum average waiting time.
S2 is true SJF can cause starvation.

3. Consider the following table of arrival time and burst time for three processes P0, P1 and P2. (GATE-CS-2011)
Process
P0
P1
P2

Arrival time
0 ms
1 ms
2 ms

Burst Time​
9 ms​
4 ms​
9 ms

The pre-emptive shortest job first scheduling algorithm is used. Scheduling is carried out only at arrival or completion of processes. What is the average waiting time for the three processes?
(A) 5.0 ms
(B) 4.33 ms
(C) 6.33
(D) 7.33
Solution :
Answer: – (A)
Process P0 is allocated processor at 0 ms as there is no other process in the ready queue. P0 is preempted after 1 ms as P1 arrives at 1 ms and burst time for P1 is less than remaining time of P0. P1 runs for 4ms. P2
arrived at 2 ms but P1 continued as burst time of P2 is longer than P1. After P1 completes, P0 is scheduled again as the remaining time for P0 is less than the burst time of P2.
P0 waits for 4 ms, P1 waits for 0 ms and P2 waits for 11 ms. So average waiting time is (0+4+11)/3 = 5.

4. Consider the following set of processes, with the arrival times and the CPU-burst times given in milliseconds (GATE-CS-2004)
Process
P1
P2
P3
P4

Arrival Time
0
1
2
4

Burst Time​
5​
3​
3​
1

What is the average turnaround time for these processes with the preemptive shortest remaining processing time first (SRPT) algorithm ?
(A) 5.50
(B) 5.75
(C) 6.00
(D) 6.25
Answer (A)
Solution:
The following is Gantt Chart of execution
P1
1

P2
4

P4
5

P3
8

P1
12

Turn Around Time = Completion Time – Arrival Time
Avg Turn Around Time = (12 + 3 + 6+ 1)/4 = 5.50

5. An operating system uses the Shortest Remaining Time first (SRTF) process scheduling algorithm. Consider the arrival times and execution times for the following processes:
Process
P1
P2
P3
P4

Execution time
20
25
10
15

Arrival time​
0​
15​
30​
45

What is the total waiting time for process P2?
(A) 5
(B) 15
(C) 40
(D) 55
Answer (B)
At time 0, P1 is the only process, P1 runs for 15 time units.
At time 15, P2 arrives, but P1 has the shortest remaining time. So P1 continues for 5 more time units.
At time 20, P2 is the only process. So it runs for 10 time units
At time 30, P3 is the shortest remaining time process. So it runs for 10 time units
At time 40, P2 runs as it is the only process. P2 runs for 5 time units.
At time 45, P3 arrives, but P2 has the shortest remaining time. So P2 continues for 10 more time units.
P2 completes its ececution at time 55
Total waiting time for P2 = Complition time - (Arrival time + Execution time)​
= 55 - (15 + 25)​
= 15

Please refer Quiz on CPU Scheduling for more questions.

References:
http://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/5_CPU_Scheduling.html
http://codex.cs.yale.edu/avi/os-book/OS8/os8c/slide-dir/PDF-dir/ch5.pdf

Improved By : Rohit_ranjan, VaibhavRai3, Vaibhav_Negi, shreyashagrawal

Source
https://www.geeksforgeeks.org/cpu-scheduling-in-operating-systems/
✍
Write a Testimonial

Introduction of Process Management
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Introduction of Process Management - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Program vs Process
A process is a program in execution. For example, when we write a program in C or C++ and compile it, the compiler creates binary code. The original code and binary code are both programs. When we actually run the
binary code, it becomes a process.
A process is an ‘active’ entity, as opposed to a program, which is considered to be a ‘passive’ entity. A single program can create many processes when run multiple times; for example, when we open a .exe or binary file
multiple times, multiple instances begin (multiple processes are created).
What does a process look like in memory?

Text Section:A Process, sometimes known as the Text Section, also includes the current activity represented by the value of the Program Counter.
Stack: The Stack contains the temporary data, such as function parameters, returns addresses, and local variables.
Data Section: Contains the global variable.
Heap Section: Dynamically allocated memory to process during its run time.
Refer this for more details on sections.

Attributes or Characteristics of a Process
A process has following attributes.
1. Process Id:
A unique identifier assigned by the operating system​
2. Process State: Can be ready, running, etc.​
3. CPU registers: Like the Program Counter (CPU registers must be saved and ​
restored when a process is swapped in and out of CPU)​
5. Accounts information:​
6. I/O status information: For example, devices allocated to the process, ​
open files, etc​
8. CPU scheduling information: For example, Priority (Different processes ​
may have different priorities, for example​
a short process may be assigned a low priority​
in the shortest job first scheduling)

All of the above attributes of a process are also known as thecontext of the process.
Every process has its own program control block(PCB), i.e each process will have a unique PCB. All of the above attributes are part of the PCB.

States of Process:
A process is in one of the following states:
1. New: Newly Created Process (or) being-created process.​
​
2. Ready: After creation process moves to Ready state, i.e. the ​
process is ready for execution.​
​
3. Run: Currently running process in CPU (only one process at​
a time can be under execution in a single processor).​
​
4. Wait (or Block): When a process requests I/O access.​
​
5. Complete (or Terminated): The process completed its execution.​
​
6. Suspended Ready: When the ready queue becomes full, some processes ​
are moved to suspended ready state​
​
7. Suspended Block: When waiting queue becomes full.

Context Switching
The process of saving the context of one process and loading the context of another process is known as Context Switching. In simple terms, it is like loading and unloading the process from running state to ready state.
When does context switching happen?
1. When a high-priority process comes to ready state (i.e. with higher priority than the running process)
2. An Interrupt occurs
3. User and kernel mode switch (It is not necessary though)
4. Preemptive CPU scheduling used.
Context Switch vs Mode Switch
A mode switch occurs when CPU privilege level is changed, for example when a system call is made or a fault occurs. The kernel works in more a privileged mode than a standard user task. If a user process wants to
access things which are only accessible to the kernel, a mode switch must occur. The currently executing process need not be changed during a mode switch.
A mode switch typically occurs for a process context switch to occur. Only the kernel can cause a context switch.
CPU-Bound vs I/O-Bound Processes:
A CPU-bound process requires more CPU time or spends more time in the running state.
An I/O-bound process requires more I/O time and less CPU time. An I/O-bound process spends more time in the waiting state.

Exercise:
1. Which of the following need not necessarily be saved on a context switch between processes? (GATE-CS-2000)
(A) General purpose registers
(B) Translation lookaside buffer
(C) Program counter
(D) All of the above
Answer (B)
Explanation:
In a process context switch, the state of the first process must be saved somehow, so that when the scheduler gets back to the execution of the first process, it can restore this state and continue. The state of the process
includes all the registers that the process may be using, especially the program counter, plus any other operating system-specific data that may be necessary. A translation look-aside buffer (TLB) is a CPU cache that
memory management hardware uses to improve virtual address translation speed. A TLB has a fixed number of slots that contain page table entries, which map virtual addresses to physical addresses. On a context switch,
some TLB entries can become invalid, since the virtual-to-physical mapping is different. The simplest strategy to deal with this is to completely flush the TLB.
2. The time taken to switch between user and kernel modes of execution is t1 while the time taken to switch between two processes is t2. Which of the following is TRUE? (GATE-CS-2011)
(A) t1 > t2
(B) t1 = t2
(C) t1 < t2
(D) nothing can be said about the relation between t1 and t2.
Answer: (C)
Explanation: Process switching involves mode switch. Context switching can occur only in kernel mode.
Quiz on Process Management
References:
http://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/3_Processes.html
http://cs.nyu.edu/courses/spring11/G22.2250-001/lectures/lecture-04.html

Improved By : Nilesh Singh 2, chrismaher37

Source
https://www.geeksforgeeks.org/introduction-of-process-management/
✍
Write a Testimonial

Process Schedulers in Operating System
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Process Schedulers in Operating System - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
There are three types of process scheduler.
1. Long Term or job scheduler It brings the new process to the ‘Ready State’. It controls Degree of Multi-programming, i.e., number of process present in ready state at any point of time.It is important that the longterm scheduler make a careful selection of both IO and CPU bound process.

2. Short term or CPU scheduler: It is responsible for selecting one process from ready state for scheduling it on the running state. Note: Short-term scheduler only selects the process to schedule it doesn’t load the
process on running.
Dispatcher is responsible for loading the process selected by Short-term scheduler on the CPU (Ready to Running State) Context switching is done by dispatcher only. A dispatcher does the following:
1. Switching context.
2. Switching to user mode.
3. Jumping to the proper location in the newly loaded program.
3. Medium-term scheduler It is responsible for suspending and resuming the process. It mainly does swapping (moving processes from main memory to disk and vice versa). Swapping may be necessary to improve the
process mix or because a change in memory requirements has overcommitted available memory, requiring memory to be freed up.

Improved By : magbene, VaibhavRai3

Source
https://www.geeksforgeeks.org/process-schedulers-in-operating-system/
✍
Write a Testimonial

GATE | GATE-CS-2015 (Set 2) | Question 65
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ GATE | GATE-CS-2015 (Set 2) | Question 65 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
A computer system implements a 40 bit virtual address, page size of 8 kilobytes, and a 128-entry translation look-aside buffer (TLB) organized into 32 sets each having four ways. Assume that the TLB tag does not store
any process id. The
minimum length of the TLB tag in bits is _________
(A) 20
(B) 10
(C) 11
(D) 22
Answer: (D)
Explanation:
​
Total virtual address size = 40​
​
Since there are 32 sets, set offset = 5​
​
Since page size is 8kilobytes, word offset = 13​
​
Minimum tag size = 40 - 5- 13 = 22

Quiz of this Question

Source
https://www.geeksforgeeks.org/gate-gate-cs-2015-set-2-question-35/
✍
Write a Testimonial

Commonly Asked Operating Systems Interview Questions | Set 1
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Commonly Asked Operating Systems Interview Questions | Set 1 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
What is a process and process table? What are different states of process
A process is an instance of program in execution. For example a Web Browser is a process, a shell (or command prompt) is a process.
The operating system is responsible for managing all the processes that are running on a computer and allocated each process a certain amount of time to use the processor. In addition, the operating system also allocates
various other resources that processes will need such as computer memory or disks. To keep track of the state of all the processes, the operating system maintains a table known as the process table. Inside this table, every
process is listed along with the resources the processes is using and the current state of the process.
Processes can be in one of three states: running, ready, or waiting. The running state means that the process has all the resources it need for execution and it has been given permission by the operating system to use the
processor. Only one process can be in the running state at any given time. The remaining processes are either in a waiting state (i.e., waiting for some external event to occur such as user input or a disk access) or a ready
state (i.e., waiting for permission to use the processor). In a real operating system, the waiting and ready states are implemented as queues which hold the processes in these states. The animation below shows a simple
representation of the life cycle of a process (Source: http://courses.cs.vt.edu/csonline/OS/Lessons/Processes/index.html)
What is a Thread? What are the differences between process and thread?
A thread is a single sequence stream within in a process. Because threads have some of the properties of processes, they are sometimes called lightweight processes. Threads are popular way to improve application through
parallelism. For example, in a browser, multiple tabs can be different threads. MS word uses multiple threads, one thread to format the text, other thread to process inputs, etc.
A thread has its own program counter (PC), a register set, and a stack space. Threads are not independent of one other like processes as a result threads shares with other threads their code section, data section and OS
resources like open files and signals. See http://www.personal.kent.edu/~rmuhamma/OpSystems/Myos/threads.htm for more details.
What are the benefits of multithreaded programming?
It makes the system more responsive and enables resource sharing. It leads to the use of multiprocess architectur.It is more economical and preferred.

What are the different scheduling algorithms
First-Come, First-Served (FCFS) Scheduling.
Shortest-Job-Next (SJN) Scheduling.
Priority Scheduling.
Shortest Remaining Time.
Round Robin(RR) Scheduling.
Multiple-Level Queues Scheduling.
What is deadlock?
Deadlock is a situation when two or more processes wait for each other to finish and none of them ever finish. Consider an example when two trains are coming toward each other on same track and there is only one track,
none of the trains can move once they are in front of each other. Similar situation occurs in operating systems when there are two or more processes hold some resources and wait for resources held by other(s).
What are the necessary conditions for deadlock?
Mutual Exclusion: There is a resource that cannot be shared.
Hold and Wait: A process is holding at least one resource and waiting for another resource which is with some other process.
No Preemption: The operating system is not allowed to take a resource back from a process until process gives it back.
Circular Wait: A set of processes are waiting for each other in circular form.

What is Virtual Memory? How is it implemented?
Virtual memory creates an illusion that each user has one or more contiguous address spaces, each beginning at address zero. The sizes of such virtual address spaces is generally very high.
The idea of virtual memory is to use disk space to extend the RAM. Running processes don’t need to care whether the memory is from RAM or disk. The illusion of such a large amount of memory is created by
subdividing the virtual memory into smaller pieces, which can be loaded into physical memory whenever they are needed by a process.
What is Thrashing?
Thrashing is a situation when the performance of a computer degrades or collapses. Thrashing occurs when a system spends more time processing page faults than executing transactions. While processing page faults is
necessary to in order to appreciate the benefits of virtual memory, thrashing has a negative affect on the system. As the page fault rate increases, more transactions need processing from the paging device. The queue at the
paging device increases, resulting in increased service time for a page fault (Source: http://cs.gmu.edu/cne/modules/vm/blue/thrash.html)
What is Belady’s Anomaly?
Bélády’s anomaly is an anomaly with some page replacement policies where increasing the number of page frames results in an increase in the number of page faults. It occurs with First in First Out page replacement is
used. See the wiki page for an example and more details.
Differences between mutex and semphore?
See https://www.geeksforgeeks.org/mutex-vs-semaphore/
Practice Quizzes on Operating System topics
Last Minute Notes – OS
OS articles
Last Minute Notes – Operating Systems
We will soon be covering more Operating System questions. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.

Improved By : shiwanisinha

Source
https://www.geeksforgeeks.org/commonly-asked-operating-systems-interview-questions-set-1/
✍
Write a Testimonial

Working with Shared Libraries | Set 2
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Working with Shared Libraries | Set 2 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
We have covered basic information about shared libraries in the previous post. In the current article we will learn how to create shared libraries on Linux.
Prior to that we need to understand how a program is loaded into memory, various (basic) steps involved in the process.
Let us see a typical “Hello World” program in C. Simple Hello World program screen image is given below.

We were compiling our code using the command “gcc -o sample shared.c” When we compile our code, the compiler won’t resolve implementation of the function printf(). It only verifies the syntactical checking. The
tool chain leaves a stub in our application which will be filled by dynamic linker. Since printf is standard function the compiler implicitly invoking its shared library. More details down.
We are using ldd to list dependencies of our program binary image. In the screen image, we can see our sample program depends on three binary files namely, linux-vdso.so.1, libc.so.6 and /lib64/ld-linux-x86-64.so.2.
The file VDSO is fast implementation of system call interface and some other stuff, it is not our focus (on some older systems you may see different file name in liue of *.vsdo.*). Ignore this file. We have interest in the
other two files.
The file libc.so.6 is C implementation of various standard functions. It is the file where we see printf definition needed for our Hello World. It is the shared library needed to be loaded into memory to run our Hello World
program.
The third file /lib64/ld-linux-x86-64.so.2 is infact an executable that runs when an application is invoked. When we invoke the program on bash terminal, typically the bash forks itself and replaces its address space with
image of program to run (so called fork-exec pair). The kernel verifies whether the libc.so.6 resides in the memory. If not, it will load the file into memory and does the relocation of libc.so.6 symbols. It then invokes the
dynamic linker (/lib64/ld-linux-x86-64.so.2) to resolve unresolved symbols of application code (printf in the present case). Then the control transfers to our program main. (I have intensionally omitted many details in the
process, our focus is to understand basic details).
Creating our own shared library:
Let us work with simple shared library on Linux. Create a file library.c with the following content.

The file library.c defines a function signum which will be used by our application code. Compile the file library.c file using the following command.

gcc -shared -fPIC -o liblibrary.so library.c
The flag -shared instructs the compiler that we are building a shared library. The flag -fPIC is to generate position independent code (ignore for now). The command generates a shared library liblibrary.so in the current
working directory. We have our shared object file (shared library name in Linux) ready to use.
Create another file application.c with the following content.

In the file application.c we are invoking the function signum which was defined in a shared library. Compile the application.c file using the following command.
gcc application.c -L /home/geetanjali/coding/ -llibrary -o sample
The flag -llibrary instructs the compiler to look for symbol definitions that are not available in the current code (signum function in our case). The option -L is hint to the compiler to look in the directory followed by the
option for any shared libraries (during link time only). The command generates an executable named as “sample“.
If you invoke the executable, the dynamic linker will not be able to find the required shared library. By default it won’t look into current working directory. You have to explicitly instruct the tool chain to provide proper
paths. The dynamic linker searches standard paths available in the LD_LIBRARY_PATH and also searches in system cache (for details explore the command ldconfig). We have to add our working directory to
the LD_LIBRARY_PATH environment variable. The following command does the same.
export LD_LIBRARY_PATH=/home/geetanjali/coding/:$LD_LIBRARY_PATH
You can now invoke our executable as shown.

./sample
Sample output on my system is shown below.

Note: The path /home/geetanjali/coding/ is working directory path on my machine. You need to use your working directory path where ever it is being used in the above commands.
Stay tuned, we haven’t even explored 1/3rd of shared library concepts. More advanced concepts in the later articles.
Exercise:
It is workbook like article. You won’t gain much unless you practice and do some research.
1. Create similar example and write your won function in the shared library. Invoke the function in another application.
2. Is (Are) there any other tool(s) which can list dependent libraries?
3. What is position independent code (PIC)?
4. What is system cache in the current context? How does the directory /etc/ld.so.conf.d/* related in the current context?
— Venki. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.

Source
https://www.geeksforgeeks.org/working-with-shared-libraries-set-2/
✍
Write a Testimonial

Working with Shared Libraries | Set 1
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Working with Shared Libraries | Set 1 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
This article is not for those algo geeks. If you are interested in systems related stuff, just read on…
Shared libraries are useful in sharing code which is common across many applications. For example, it is more economic to pack all the code related to TCP/IP implementation in a shared library. However, data can’t be
shared as every application needs its own set of data. Applications like, browser, ftp, telnet, etc… make use of the shared ‘network’ library to elevate specific functionality.
Every operating system has its own representation and tool-set to create shared libraries. More or less the concepts are same. On Windows every object file (*.obj, *.dll, *.ocx, *.sys, *.exe etc…) follow a format called
Portalbe Executable. Even shared libraries (called as Dynamic Linked Libraries or DLL in short) are also represented in PE format. The tool-set that is used to create these libraries need to understand the binary format.
Linux variants follow a format called Executable and Linkable Format (ELF). The ELF files are position independent (PIC) format. Shared libraries in Linux are referred as shared objects (generally with extension *.so).
These are similar to DLLs in Windows platform. Even shared object files follow the ELF binary format.

Remember, the file extensions (*.dll, *.so, *.a, *.lib, etc…) are just for programmer convenience. They don’t have any significance. All these are binary files. You can name them as you wish. Yet ensure you provide
absolute paths in building applications.
In general, when we compile an application the steps are simple. Compile, Link and Load. However, it is not simple. These steps are more versatile on modern operating systems.
When you link your application against static library, the code is part of your application. There is no dependency. Even though it causes the application size to increase, it has its own advantages. The primary one is speed
as there will be no symbol (a program entity) resolution at runtime. Since every piece of code part of the binary image, such applications are independent of version mismatch issues. However, the cost is on fixing an issue
in library code. If there is any bug in library code, entire application need to be recompiled and shipped to the client. In case of dynamic libraries, fixing or upgrading the libraries is easy. You just need to ship the updated
shared libraries. The application need not to recompile, it only need to re-run. You can design a mechanism where we don’t need to restart the application.
When we link an application against a shared library, the linker leaves some stubs (unresolved symbols) to be filled at application loading time. These stubs need to be filled by a tool called, dynamic linker at run time or at
application loading time. Again loading of a library is of two types, static loading and dynamic loading. Don’t confuse between static loading vs static linking and dynamic loading vs dynamic linking.
For example, you have built an application that depends on libstdc++.so which is a shared object (dynamic libary). How does the application become aware of required shared libraries? (If you are interested, explore the
tools tdump from Borland tool set, objdump or nm or readelf tools on Linux).
Static loading:
In static loading, all of those dependent shared libraries are loaded into memory even before the application starts execution. If loading of any shared library fails, the application won’t run.
A dynamic loader examines application’s dependency on shared libraries. If these libraries are already loaded into the memory, the library address space is mapped to application virtual address space (VAS) and the
dynamic linker does relocation of unresolved symbols.
If these libraries are not loaded into memory (perhaps your application might be first to invoke the shared library), the loader searches in standard library paths and loads them into memory, then maps and resolves
symbols. Again loading is big process, if you are interested write your own loader :).
While resolving the symbols, if the dynamic linker not able to find any symbol (may be due to older version of shared library), the application can’t be started.
Dynamic Loading:
As the name indicates, dynamic loading is about loading of library on demand.
For example, if you want a small functionality from a shared library. Why should it be loaded at the application load time and sit in the memory? You can invoke loading of these shared libraries dynamically when

you need their functionality. This is called dynamic loading. In this case, the programmer aware of situation ‘when should the library be loaded’. The tool-set and relevant kernel provides API to support dynamic
loading, and querying of symbols in the shared library.
More details in later articles.
Note: If you come across terms like loadable modules or equivalent terms, don’t mix them with shared libraries. They are different from shared libraries The kernels provide framework to support loadable modules.
Working with Shared Libraries | Set 2
Exercise:
1. Assuming you have understood the concepts, How do you design an application (e.g. Banking) which can upgrade to new shared libraries without re-running the application.
— Venki. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.

Source
https://www.geeksforgeeks.org/working-with-shared-libraries-set-1/
✍
Write a Testimonial

Static and Dynamic Libraries | Set 1
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Static and Dynamic Libraries | Set 1 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
When a C program is compiled, the compiler generates object code. After generating the object code, the compiler also invokes linker. One of the main tasks for linker is to make code of library functions (eg printf(),
scanf(), sqrt(), ..etc) available to your program. A linker can accomplish this task in two ways, by copying the code of library function to your object code, or by making some arrangements so that the complete code of
library functions is not copied, but made available at run-time.
Static Linking and Static Libraries is the result of the linker making copy of all used library functions to the executable file. Static Linking creates larger binary files, and need more space on disk and main memory.
Examples of static libraries (libraries which are statically linked) are, .a files in Linux and .lib files in Windows.
Steps to create a static library Let us create and use a Static Library in UNIX or UNIX like OS.
1. Create a C file that contains functions in your library.

filter_none
edit
close
play_arrow
link
brightness_4
code
/* Filename: lib_mylib.c */
#include <stdio.h>
void fun(void)
{
printf("fun() called from a static library");
}

chevron_right
filter_none
We have created only one file for simplicity. We can also create multiple files in a library.
2. Create a header file for the library
filter_none
edit
close
play_arrow
link
brightness_4
code
/* Filename: lib_mylib.h */
void fun(void);

chevron_right
filter_none
3. Compile library files.
gcc -c lib_mylib.c -o lib_mylib.o

4. Create static library. This step is to bundle multiple object files in one static library (see ar for details). The output of this step is static library.
ar rcs lib_mylib.a lib_mylib.o

5. Now our static library is ready to use. At this point we could just copy lib_mylib.a somewhere else to use it. For demo purposes, let us keep the library in the current directory.
Let us create a driver program that uses above created static library.
1. Create a C file with main function
filter_none
edit
close
play_arrow
link
brightness_4
code

/* filename: driver.c
#include "lib_mylib.h"
void main()
{
fun();
}

*/

chevron_right
filter_none
2. Compile the driver program.
gcc -c driver.c -o driver.o

3. Link the compiled driver program to the static library. Note that -L. is used to tell that the static library is in current folder (See this for details of -L and -l options).
gcc -o driver driver.o -L. -l_mylib

4. Run the driver program
./driver ​
fun() called from a static library

Following are some important points about static libraries.
1. For a static library, the actual code is extracted from the library by the linker and used to build the final executable at the point you compile/build your application.

2. Each process gets its own copy of the code and data. Where as in case of dynamic libraries it is only code shared, data is specific to each process. For static libraries memory footprints are larger. For example, if all the
window system tools were statically linked, several tens of megabytes of RAM would be wasted for a typical user, and the user would be slowed down by a lot of paging.
3. Since library code is connected at compile time, the final executable has no dependencies on the library at run time i.e. no additional run-time loading costs, it means that you don’t need to carry along a copy of the
library that is being used and you have everything under your control and there is no dependency.
4. In static libraries, once everything is bundled into your application, you don’t have to worry that the client will have the right library (and version) available on their system.
5. One drawback of static libraries is, for any change(up-gradation) in the static libraries, you have to recompile the main program every time.
6. One major advantage of static libraries being preferred even now “is speed”. There will be no dynamic querying of symbols in static libraries. Many production line software use static libraries even today.
Dynamic linking and Dynamic Libraries Dynamic Linking doesn’t require the code to be copied, it is done by just placing name of the library in the binary file. The actual linking happens when the program is run,
when both the binary file and the library are in memory. Examples of Dynamic libraries (libraries which are linked at run-time) are, .so in Linux and .dll in Windows.
We will soon be covering more points on Dynamic Libraries and steps to create them.
This article is compiled by Abhijit Saha and reviewed by GeeksforGeeks team. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.

Improved By : aakash nandi, Akanksha_Rai

Source
https://www.geeksforgeeks.org/static-vs-dynamic-libraries/
✍
Write a Testimonial

Operating Systems | Input Output Systems | Question 5
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Input Output Systems | Question 5 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Which of the following is major part of time taken when accessing data on the disk?
(A) Settle time
(B) Rotational latency
(C) Seek time
(D) Waiting time
Answer: (C)
Explanation: Seek time is time taken by the head to travel to the track of the disk where the data to be accessed is stored.
Quiz of this Question

Source
https://www.geeksforgeeks.org/operating-systems-iinput-output-systems-question-1/
✍
Write a Testimonial

Operating Systems | CPU Scheduling | Question 2
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | CPU Scheduling | Question 2 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Consider three processes, all arriving at time zero, with total execution time of 10, 20 and 30 units, respectively. Each process spends the first 20% of execution time doing I/O, the next 70% of time doing computation, and
the last 10% of time doing I/O again. The operating system uses a shortest remaining compute time first scheduling algorithm and schedules a new process either when the running process gets blocked on I/O or when the
running process finishes its compute burst. Assume that all I/O operations can be overlapped as much as possible. For what percentage of time does the CPU remain idle?
(A) 0%
(B) 10.6%

(C) 30.0%
(D) 89.4%

Answer: (B)
Explanation: Let three processes be p0, p1 and p2. Their execution time is 10, 20 and 30 respectively. p0 spends first 2 time units in I/O, 7 units of CPU time and finally 1 unit in I/O. p1 spends first 4 units in I/O, 14 units
of CPU time and finally 2 units in I/O. p2 spends first 6 units in I/O, 21 units of CPU time and finally 3 units in I/O.
idle
0

p0

p1

2

p2

9

23

44

idle​
47​

Total time spent = 47
Idle time = 2 + 3 = 5
Percentage of idle time = (5/47)*100 = 10.6 %
Quiz of this Question

Source
https://www.geeksforgeeks.org/operating-systems-process-synchronization-question-4/
✍
Write a Testimonial

Operating Systems | CPU Scheduling | Question 1
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | CPU Scheduling | Question 1 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Consider three processes (process id 0, 1, 2 respectively) with compute time bursts 2, 4 and 8 time units. All processes arrive at time zero. Consider the longest remaining time first (LRTF) scheduling algorithm. In LRTF
ties are broken by giving priority to the process with the lowest process id. The average turn around time is:
(A) 13 units
(B) 14 units
(C) 15 units
(D) 16 units
Answer: (A)
Explanation: Let the processes be p0, p1 and p2. These processes will be executed in following order.
p2
0

p1
4

p2
5

p1
6

p2
7

p0
8

p1
9

p2
10

p0
11

p1
12

p2​
13

14 ​

Turn around time of a process is total time between submission of the process and its completion.
Turn around time of p0 = 12 (12-0)
Turn around time of p1 = 13 (13-0)
Turn around time of p2 = 14 (14-0)
Average turn around time is (12+13+14)/3 = 13.
Quiz of this Question

Source
https://www.geeksforgeeks.org/operating-systems-process-synchronization-question-3/
✍
Write a Testimonial

Operating Systems | Process Management | Question 6
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Process Management | Question 6 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Consider the following code fragment:
filter_none
edit
close
play_arrow
link
brightness_4
code
if (fork() == 0)
{ a = a + 5; printf("%d,%d\n", a, &a); }
else { a = a –5; printf("%d, %d\n", a, &a); }

chevron_right
filter_none
Let u, v be the values printed by the parent process, and x, y be the values printed by the child process. Which one of the following is TRUE?
(A) u = x + 10 and v = y
(B) u = x + 10 and v != y
(C) u + 10 = x and v = y
(D) u + 10 = x and v != y

Answer: (C)
Explanation: fork() returns 0 in child process and process ID of child process in parent process.
In Child (x), a = a + 5
In Parent (u), a = a – 5;
Therefore x = u + 10.
The physical addresses of ‘a’ in parent and child must be different. But our program accesses virtual addresses (assuming we are running on an OS that uses virtual memory). The child process gets an exact copy of parent
process and virtual address of ‘a’ doesn’t change in child process. Therefore, we get same addresses in both parent and child. See this run for example.
Quiz of this Question

Improved By : ArthurDayne05

Source
https://www.geeksforgeeks.org/operating-systems-process-synchronization-question-2/
✍
Write a Testimonial

Operating Systems | Memory Management | Question 1
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Memory Management | Question 1 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Which of the following page replacement algorithms suffers from Belady’s anomaly?
(A) FIFO
(B) LRU
(C) Optimal Page Replacement
(D) Both LRU and FIFO
Answer: (A)
Explanation: Belady’s anomaly proves that it is possible to have more page faults when increasing the number of page frames while using the First in First Out (FIFO) page replacement algorithm.
See the example given on Wiki Page.
Quiz of this Question

Source
https://www.geeksforgeeks.org/operating-systems-memory-management-question-1/
✍
Write a Testimonial

Operating Systems | Set 17
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Set 17 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Following question has been asked in GATE 2012 CS exam.
Fetch_And_Add(X,i) is an atomic Read-Modify-Write instruction that reads the value of memory location X, increments it by the value i, and returns the old value of X. It is used in the pseudocode shown
below to implement a busy-wait lock. L is an unsigned integer shared variable initialized to 0. The value of 0 corresponds to lock being available, while any non-zero value corresponds to the lock being not
available.
​
AcquireLock(L){​
while (Fetch_And_Add(L,1))​
L = 1;​
}​
ReleaseLock(L){​
L = 0;​
}

This implementation
(A) fails as L can overflow
(B) fails as L can take on a non-zero value when the lock is actually available
(C) works correctly but may starve some processes
(D) works correctly without starvation

Answer (B)
Take closer look the below while loop.
​
while (Fetch_And_Add(L,1))​
L = 1; // A waiting process can be here just after ​
// the lock is released, and can make L = 1.​

Consider a situation where a process has just released the lock and made L = 0. Let there be one more process waiting for the lock, means executing the AcquireLock() function. Just after the L was made 0, let the waiting
processes executed the line L = 1. Now, the lock is available and L = 1. Since L is 1, the waiting process (and any other future coming processes) can not come out of the while loop.
The above problem can be resolved by changing the AcuireLock() to following.
​
AcquireLock(L){​
while (Fetch_And_Add(L,1))​
{ // Do Nothing }​

}​

Please see GATE Corner for all previous year paper/solutions/explanations, syllabus, important dates, notes, etc.

Source
https://www.geeksforgeeks.org/operating-systems-set-17/
✍
Write a Testimonial

Operating Systems | Set 16
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Set 16 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Following questions have been asked in GATE CS 2005 exam.
1) Normally user programs are prevented from handling I/O directly by I/O instructions in them. For CPUs having explicit I/O instructions, such I/O protection is ensured by having the I/O instructions
privileged. In a CPU with memory mapped I/O, there is no explicit I/O instruction. Which one of the following is true for a CPU with memory mapped I/O?
(a) I/O protection is ensured by operating system routine(s)
(b) I/O protection is ensured by a hardware trap
(c) I/O protection is ensured during system configuration
(d) I/O protection is not possible
Answwer (a)
Memory mapped I/O means, accessing I/O via general memory access as opposed to specialized IO instructions. An example,

​
unsigned int volatile const *pMappedAddress const = (unsigned int *)0x100;​

So, the programmer can directly access any memory location directly. To prevent such an access, the OS (kernel) will divide the address space into kernel space and user space. An user application can easily access user
application. To access kernel space, we need system calls (traps).
Thanks to Venki for providing the above explanation.
2) What is the swap space in the disk used for?
(a) Saving temporary html pages
(b) Saving process data
(c) Storing the super-block
(d) Storing device drivers
Answer (b)
Swap space is typically used to store process data. See this for more details.
3) Increasing the RAM of a computer typically improves performance because:
(a) Virtual memory increases
(b) Larger RAMs are faster
(c) Fewer page faults occur
(d) Fewer segmentation faults occur
Answer (c)
4) Suppose n processes, P1, …. Pn share m identical resource units, which can be reserved and released one at a time. The maximum resource requirement of process Pi is Si, where Si > 0. Which one of the
following is a sufficient condition for ensuring that deadlock does not occur?

Answer (c)
In the extreme condition, all processes acquire Si-1 resources and need 1 more resource. So following condition must be true to make sure that deadlock never occurs.
< m The above expression can be written as following.

< (m + n)

5) Consider the following code fragment:
​
if (fork() == 0)​
{ a = a + 5; printf(“%d,%d\n”, a, &a); }​
else { a = a –5; printf(“%d, %d\n”, a, &a); }

Let u, v be the values printed by the parent process, and x, y be the values printed by the child process. Which one of the following is TRUE?
(a) u = x + 10 and v = y
(b) u = x + 10 and v != y
(c) u + 10 = x and v = y
(d) u + 10 = x and v != y
Answer (c)
fork() returns 0 in child process and process ID of child process in parent process.

In Child (x), a = a + 5
In Parent (u), a = a – 5;
Therefore x = u + 10.
The physical addresses of ‘a’ in parent and child must be different. But our program accesses virtual addresses (assuming we are running on an OS that uses virtual memory). The child process gets an exact copy of parent
process and virtual address of ‘a’ doesn’t change in child process. Therefore, we get same addresses in both parent and child. See this run for example.
Thanks to Smart Pointer for providing the above explanation.
Please see GATE Corner for all previous year paper/solutions/explanations, syllabus, important dates, notes, etc.

Source
https://www.geeksforgeeks.org/operating-systems-set-16/
✍
Write a Testimonial

Operating Systems | Set 15
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Set 15 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Following questions have been asked in GATE CS 2006 exam.
1) Consider three processes (process id 0, 1, 2 respectively) with compute time bursts 2, 4 and 8 time units. All processes arrive at time zero. Consider the longest remaining time first (LRTF) scheduling
algorithm. In LRTF ties are broken by giving priority to the process with the lowest process id. The average turn around time is:
(A) 13 units
(B) 14 units
(C) 15 units
(D) 16 units
Answer (A)
Let the processes be p0, p1 and p2. These processes will be executed in following order.

​
p2
0

p1
4

p2
5

p1
6

p2
7

p0
8

p1
9

p2
10

p0
11

p1
12

p2​
13

14 ​

Turn around time of a process is total time between submission of the process and its completion.
Turn around time of p0 = 12 (12-0)
Turn around time of p1 = 13 (13-0)
Turn around time of p2 = 14 (14-0)
Average turn around time is (12+13+14)/3 = 13.
2) Consider three processes, all arriving at time zero, with total execution time of 10, 20 and 30 units, respectively. Each process spends the first 20% of execution time doing I/O, the next 70% of time doing
computation, and the last 10% of time doing I/O again. The operating system uses a shortest remaining compute time first scheduling algorithm and schedules a new process either when the running process
gets blocked on I/O or when the running process finishes its compute burst. Assume that all I/O operations can be overlapped as much as possible. For what percentage of time does the CPU remain idle?(A)
0%
(B) 10.6%
(C) 30.0%
(D) 89.4%
Answer (B)
Let three processes be p0, p1 and p2. Their execution time is 10, 20 and 30 respectively. p0 spends first 2 time units in I/O, 7 units of CPU time and finally 1 unit in I/O. p1 spends first 4 units in I/O, 14 units of CPU time
and finally 2 units in I/O. p2 spends first 6 units in I/O, 21 units of CPU time and finally 3 units in I/O.
​
idle
p0
p1
p2
idle​
0
2
9
23
44
47​
​
Total time spent = 47​
Idle time = 2 + 3 = 5​
Percentage of idle time = (5/47)*100 = 10.6 %​

3) The atomic fetch-and-set x, y instruction unconditionally sets the memory location x to 1 and fetches the old value of x in y without allowing any intervening access to the memory location x. consider the
following implementation of P and V functions on a binary semaphore .
​
void P (binary_semaphore *s) {​
unsigned y;​
unsigned *x = &(s->value);​
do {​
fetch-and-set x, y;​
} while (y);​
}​
​
void V (binary_semaphore *s) {​
S->value = 0;​
}

Which one of the following is true?
(A) The implementation may not work if context switching is disabled in P.
(B) Instead of using fetch-and-set, a pair of normal load/store can be used

(C) The implementation of V is wrong
(D) The code does not implement a binary semaphore
Answer (A)
Let us talk about the operation P(). It stores the value of s in x, then it fetches the old value of x, stores it in y and sets x as 1. The while loop of a process will continue forever if some other process doesn’t execute V() and
sets the value of s as 0. If context switching is disabled in P, the while loop will run forever as no other process will be able to execute V().
4) Consider the following snapshot of a system running n processes. Process i is holding Xi instances of a resource R, 1 <= i <= n. currently, all instances of R are occupied. Further, for all i, process i has placed
a request for an additional Yi instances while holding the Xi instances it already has. There are exactly two processes p and q such that Yp = Yq = 0. Which one of the following can serve as a necessary
condition to guarantee that the system is not approaching a deadlock?
(A) min (Xp, Xq) < max (Yk) where k != p and k != q (B) Xp + Xq >= min (Yk) where k != p and k != q
(C) max (Xp, Xq) > 1
(D) min (Xp, Xq) > 1
Answer (B)
Since both p and q don’t need additional resources, they both can finish and release Xp + Xq resources without asking for any additional resource. If the resources released by p and q are sufficient for another process
waiting for Yk resources, then system is not approaching deadlock.
Please see GATE Corner for all previous year paper/solutions/explanations, syllabus, important dates, notes, etc.

Source
https://www.geeksforgeeks.org/operating-systems-set-15/
✍
Write a Testimonial

Operating Systems | Set 14
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Set 14 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Following questions have been asked in GATE CS 2006 exam.
1) Consider three CPU-intensive processes, which require 10, 20 and 30 time units and arrive at times 0, 2 and 6, respectively. How many context switches are needed if the operating system implements a
shortest remaining time first scheduling algorithm? Do not count the context switches at time zero and at the end.
(A) 1
(B) 2
(C) 3
(D) 4
Answer (B)
Let three process be P0, P1 and P2 with arrival times 0, 2 and 6 respectively and CPU burst times 10, 20 and 30 respectively. At time 0, P0 is the only available process so it runs. At time 2, P1 arrives, but P0 has the
shortest remaining time, so it continues. At time 6, P2 arrives, but P0 has the shortest remaining time, so it continues. At time 10, P1 is scheduled as it is the shortest remaining time process. At time 30, P2 is scheduled.
Only two context switches are needed. P0 to P1 and P1 to P2.

2) A computer system supports 32-bit virtual addresses as well as 32-bit physical addresses. Since the virtual address space is of the same size as the physical address space, the operating system designers
decide to get rid of the virtual memory entirely. Which one of the following is true?
(A) Efficient implementation of multi-user support is no longer possible
(B) The processor cache organization can be made more efficient now
(C) Hardware support for memory management is no longer needed
(D) CPU scheduling can be made more efficient now
Answer (C)
For supporting virtual memory, special hardware support is needed from Memory Management Unit. Since operating system designers decide to get rid of the virtual memory entirely, hardware support for memory
management is no longer needed
3) A CPU generates 32-bit virtual addresses. The page size is 4 KB. The processor has a translation look-aside buffer (TLB) which can hold a total of 128 page table entries and is 4-way set associative. The
minimum size of the TLB tag is:
(A) 11 bits
(B) 13 bits
(C) 15 bits
(D) 20 bits
Answer (C)
Size of a page = 4KB = 2^12
Total number of bits needed to address a page frame = 32 – 12 = 20
If there are ‘n’ cache lines in a set, the cache placement is called n-way set associative. Since TLB is 4 way set associative and can hold total 128 (2^7) page table entries, number of sets in cache = 2^7/4 = 2^5. So 5 bits
are needed to address a set, and 15 (20 – 5) bits are needed for tag.
Please see GATE Corner for all previous year paper/solutions/explanations, syllabus, important dates, notes, etc.

Source
https://www.geeksforgeeks.org/operating-systems-set-14/
✍
Write a Testimonial

Operating Systems | Set 13
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Set 13 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Following questions have been asked in GATE CS 2007 exam.
1) A virtual memory system uses First In First Out (FIFO) page replacement policy and allocates a fixed number of frames to a process. Consider the following statements:
P: Increasing the number of page frames allocated to a process sometimes increases the page fault rate.
Q: Some programs do not exhibit locality of reference. Which one of the following is TRUE?
(A) Both P and Q are true, and Q is the reason for P
(B) Both P and Q are true, but Q is not the reason for P.
(C) P is false, but Q is true
(D) Both P and Q are false.
Answer (B)
P is true. Increasing the number of page frames allocated to process may increases the no. of page faults (See Belady’s Anomaly).
Q is also true, but Q is not the reason for-P as Belady’s Anomaly occurs for some specific patterns of page references.

2) A single processor system has three resource types X, Y and Z, which are shared by three processes. There are 5 units of each resource type. Consider the following scenario, where the column alloc denotes
the number of units of each resource type allocated to each process, and the column request denotes the number of units of each resource type requested by a process in order to complete execution. Which of
these processes will finish LAST?
​
P0
P1
P2

alloc
X Y Z
1 2 1
2 0 1
2 2 1

request​
X Y Z​
1 0 3​
0 1 2​
1 2 0​

(A) P0
(B) P1
(C) P2
(D) None of the above, since the system is in a deadlock
Answer (C)
Once all resources (5, 4 and 3 instances of X, Y and Z respectively) are allocated, 0, 1 and 2 instances of X, Y and Z are left. Only needs of P1 can be satisfied. So P1 can finish its execution first. Once P1 is done, it
releases 2, 1 and 3 units of X, Y and Z respectively. Among P0 and P2, needs of P0 can only be satisfied. So P0 finishes its execution. Finally, P2 finishes its execution.
3) Two processes, P1 and P2, need to access a critical section of code. Consider the following synchronization construct used by the processes:Here, wants1 and wants2 are shared variables, which are
initialized to false. Which one of the following statements is TRUE about the above construct?
​
/* P1 */​
while (true) {​
wants1 = true;​
while (wants2 == true);​
/* Critical​
Section */​
wants1=false;​
}​
/* Remainder section */
​
​
/* P2 */​
while (true) {​
wants2 = true;​
while (wants1==true);​
/* Critical​
Section */​
wants2 = false;​
}​
/* Remainder section */​

​

(A) It does not ensure mutual exclusion.
(B) It does not ensure bounded waiting.
(C) It requires that processes enter the critical section in strict alternation.
(D) It does not prevent deadlocks, but ensures mutual exclusion.
Answer (D)
The above synchronization constructs don’t prevent deadlock. When both wants1 and wants2 become true, both P1 and P2 stuck forever in their while loops waiting for each other to finish.
4) Consider the following statements about user level threads and kernel level threads. Which one of the following statement is FALSE?
(A) Context switch time is longer for kernel level threads than for user level threads.
(B) User level threads do not need any hardware support.

(C) Related kernel level threads can be scheduled on different processors in a multi-processor system.
(D) Blocking one kernel level thread blocks all related threads.
Answer (D)
Since kernel level threads are managed by kernel, blocking one thread doesn’t cause all related threads to block. It’s a problem with user level threads. See this for more details.
Please see GATE Corner for all previous year paper/solutions/explanations, syllabus, important dates, notes, etc.

Source
https://www.geeksforgeeks.org/operating-systems-set-13/
✍
Write a Testimonial

Operating Systems | Set 12
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Set 12 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Following questions have been asked in GATE CS 2007 exam.
1) Consider a disk pack with 16 surfaces, 128 tracks per surface and 256 sectors per track. 512 bytes of data are stored in a bit serial manner in a sector. The capacity of the disk pack and the number of bits
required to specify a particular sector in the disk are respectively:
(A) 256 Mbyte, 19 bits
(B) 256 Mbyte, 28 bits
(C) 512 Mbyte, 20 bits
(D) 64 Gbyte, 28 bits
Answer (A)
Capacity of the disk = 16 surfaces X 128 tracks X 256 sectors X 512 bytes = 256 Mbytes.
To calculate number of bits required to access a sector, we need to know total number of sectors. Total number of sectors = 16 surfaces X 128 tracks X 256 sectors = 2^19
So the number of bits required to access a sector is 19.

2) Group 1 contains some CPU scheduling algorithms and Group 2 contains some applications. Match entries in Group 1 to entries in Group 2.
​
Group I
(P) Gang Scheduling
(Q) Rate Monotonic Scheduling
(R) Fair Share Scheduling

Group II​
(1) Guaranteed Scheduling​
(2) Real-time Scheduling​
(3) Thread Scheduling​

(A) P – 3 Q – 2 R – 1
(B) P – 1 Q – 2 R – 3
(C) P – 2 Q – 3 R – 1
(D) P – 1 Q – 3 R – 2
Answer (A)
Gang scheduling for parallel systems that schedules related threads or processes to run simultaneously on different processors.
Rate monotonic scheduling is used in real-time operating systems with a static-priority scheduling class. The static priorities are assigned on the basis of the cycle duration of the job: the shorter the cycle duration is, the
higher is the job’s priority.
Fair Share Scheduling is a scheduling strategy in which the CPU usage is equally distributed among system users or groups, as opposed to equal distribution among processes. It is also known as Guaranteed scheduling.
3) An operating system uses Shortest Remaining Time first (SRT) process scheduling algorithm. Consider the arrival times and execution times for the following processes:
​
Process
P1
P2
P3
P4

Execution time
20
25
10
15

Arrival time​
0​
15​
30​
45

What is the total waiting time for process P2?
(A) 5
(B) 15
(C) 40
(D) 55
Answer (B)
At time 0, P1 is the only process, P1 runs for 15 time units.
At time 15, P2 arrives, but P1 has the shortest remaining time. So P1 continues for 5 more time units.
At time 20, P2 is the only process. So it runs for 10 time units
At time 30, P3 is the shortest remaining time process. So it runs for 10 time units
At time 40, P2 runs as it is the only process. P2 runs for 5 time units.
At time 45, P3 arrives, but P2 has the shortest remaining time. So P2 continues for 10 more time units.
P2 completes its ececution at time 55
​
Total waiting time for P2 = Complition time - (Arrival time + Execution time)​
= 55 - (15 + 25)​
= 15 ​

Please see GATE Corner for all previous year paper/solutions/explanations, syllabus, important dates, notes, etc.

Source
https://www.geeksforgeeks.org/operating-systems-set-12/
✍
Write a Testimonial

Operating Systems | Set 11
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Set 11 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Following questions have been asked in GATE 2008 CS exam.
1) A process executes the following code
​
for (i = 0; i < n; i++) fork();

The total number of child processes created is
(A) n
(B) 2^n - 1
(C) 2^n
(D) 2^(n+1) - 1;

Answer (B)
​
F0
/
F1

// There will be 1 child process created by first fork​
\​
F1

// There will be 2 child processes created by second fork​
/ \
/ \​
F2
F2 F2
F2 // There will be 4 child processes created by third fork​
/ \
/ \ / \ / \​
...............
// and so on​

If we sum all levels of above tree for i = 0 to n-1, we get 2^n - 1. So there will be 2^n – 1 child processes. Also see this post for more details.
2) Which of the following is NOT true of deadlock prevention and deadlock avoidance schemes?
(A) In deadlock prevention, the request for resources is always granted if the resulting state is safe
(B) In deadlock avoidance, the request for resources is always granted if the result state is safe
(C) Deadlock avoidance is less restrictive than deadlock prevention
(D) Deadlock avoidance requires knowledge of resource requirements a priori
Answer (A)
Deadlock prevention scheme handles deadlock by making sure that one of the four necessary conditions don't occur. In deadlock prevention, the request for a resource may not be granted even if the resulting state is safe.
(See the Galvin book slides for more details)
3) A processor uses 36 bit physical addresses and 32 bit virtual addresses, with a page frame size of 4 Kbytes. Each page table entry is of size 4 bytes. A three level page table is used for virtual to physical
address translation, where the virtual address is used as follows
• Bits 30-31 are used to index into the first level page table
• Bits 21-29 are used to index into the second level page table
• Bits 12-20 are used to index into the third level page table, and
• Bits 0-11 are used as offset within the page
The number of bits required for addressing the next level page table (or page frame) in the page table entry of the first, second and third level page tables are respectively
(A) 20, 20 and 20
(B) 24, 24 and 24
(C) 24, 24 and 20
(D) 25, 25 and 24
Answer (D)
Virtual address size = 32 bits
Physical address size = 36 bits
Physical memory size = 2^36 bytes
Page frame size = 4K bytes = 2^12 bytes
No. of bits required to access physical memory frame = 36 - 12 = 24
So in third level of page table, 24 bits are required to access an entry.
9 bits of virtual address are used to access second level page table entry and size of pages in second level is 4 bytes. So size of second level page table is (2^9)*4 = 2^11 bytes. It means there are (2^36)/(2^11) possible
locations to store this page table. Therefore the second page table requires 25 bits to address it. Similarly, the third page table needs 25 bits to address it.

Please see GATE Corner for all previous year paper/solutions/explanations, syllabus, important dates, notes, etc.

Source
https://www.geeksforgeeks.org/operating-systems-set-11/
✍
Write a Testimonial

Operating Systems | Set 10
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Set 10 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Following questions have been asked in GATE 2008 CS exam.
1) The data blocks of a very large file in the Unix file system are allocated using
(A) contiguous allocation
(B) linked allocation
(C) indexed allocation
(D) an extension of indexed allocation
Answer (D)
The Unix file system uses an extension of indexed allocation. It uses direct blocks, single indirect blocks, double indirect blocks and triple indirect blocks. Following diagram shows implementation of Unix file system.

2) The P and V operations on counting semaphores, where s is a counting semaphore, are defined as follows:
​
P(s)
if
V(s)
if

: s = s - 1;​
(s < 0) then wait;​
: s = s + 1;​
(s <= 0) then wakeup a process waiting on s;​

Assume that Pb and Vb the wait and signal operations on binary semaphores are provided. Two binary semaphores Xb and Yb are used to implement the semaphore operations P(s) and V(s) as follows:
​
P(s) : Pb(Xb);​
s = s - 1;​
if (s < 0) {​
Vb(Xb) ;​
Pb(Yb) ;​
}​
else Vb(Xb); ​
​
​
V(s) : Pb(Xb) ;​
s = s + 1;​
if (s <= 0) Vb(Yb) ;​
Vb(Xb) ;​

The initial values of Xb and Yb are respectively
(A) 0 and 0
(B) 0 and 1
(C) 1 and 0
(D) 1 and 1
Answer (C)
Both P(s) and V(s) operations are perform Pb(xb) as first step. If Xb is 0, then all processes executing these operations will be blocked. Therefore, Xb must be 1.
If Yb is 1, it may become possible that two processes can execute P(s) one after other (implying 2 processes in critical section). Consider the case when s = 1, y = 1. So Yb must be 0.
3) Which of the following statements about synchronous and asynchronous I/O is NOT true?
(A) An ISR is invoked on completion of I/O in synchronous I/O but not in asynchronous I/O
(B) In both synchronous and asynchronous I/O, an ISR (Interrupt Service Routine) is invoked after completion of the I/O
(C) A process making a synchronous I/O call waits until I/O is complete, but a process making an asynchronous I/O call does not wait for completion of the I/O
(D) In the case of synchronous I/O, the process waiting for the completion of I/O is woken up by the ISR that is invoked after the completion of I/O
Answer (B)
An interrupt service routine will be invoked after the completion of I/O operation and it will place process from block state to ready state, because process performing I/O operation was placed in blocked state till the I/O
operation was completed in Synchronous I/O.
However, process performing I/O will not be placed in the block state and process continues to execute the remaining instructions in Asynchronous I/O, because handler function will be registered while performing the
I/O operation, when the I/O operation completed signal mechanism is used to notify the process that data is available.
So, option (B) is false.
Please see GATE Corner for all previous year paper/solutions/explanations, syllabus, important dates, notes, etc.

Source
https://www.geeksforgeeks.org/operating-systems-set-10/
✍
Write a Testimonial

Operating Systems | Set 9
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Set 9 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Following questions have been asked in GATE 2009 CS exam.
1) In the following process state transition diagram for a uniprocessor system, assume that there are always some processes in the ready state: Now consider the following statements:

I. If a process makes a transition D, it would result in another process making transition A immediately.
II. A process P2 in blocked state can make transition E while another process P1 is in running state.
III. The OS uses preemptive scheduling.
IV. The OS uses non-preemptive scheduling.
Which of the above statements are TRUE?
(A) I and II
(B) I and III
(C) II and III
(D) II and IV
Answer (C)
I is false. If a process makes a transition D, it would result in another process making transition B, not A.
II is true. A process can move to ready state when I/O completes irrespective of other process being in running state or not.
III is true because there is a transition from running to ready state.
IV is false as the OS uses preemptive scheduling.

2) The enter_CS() and leave_CS() functions to implement critical section of a process are realized using test-and-set instruction as follows:
​
void enter_CS(X)​
{​
while test-and-set(X) ;​
}​
void leave_CS(X)​
{​
X = 0;​
}

In the above solution, X is a memory location associated with the CS and is initialized to 0. Now consider the following statements:
I. The above solution to CS problem is deadlock-free
II. The solution is starvation free.
III. The processes enter CS in FIFO order.
IV More than one process can enter CS at the same time.
Which of the above statements is TRUE?
(A) I only
(B) I and II
(C) II and III
(D) IV only
Answer (A)
The above solution is a simple test-and-set solution that makes sure that deadlock doesn’t occur, but it doesn’t use any queue to avoid starvation or to have FIFO order.
3) A multilevel page table is preferred in comparison to a single level page table for translating virtual address to physical address because
(A) It reduces the memory access time to read or write a memory location.
(B) It helps to reduce the size of page table needed to implement the virtual address space of a process.
(C) It is required by the translation lookaside buffer.
(D) It helps to reduce the number of page faults in page replacement algorithms.
Answer (B)
The size of page table may become too big (See this) to fit in contiguous space. That is why page tables are typically divided in levels.
Please see GATE Corner for all previous year paper/solutions/explanations, syllabus, important dates, notes, etc.

Source
https://www.geeksforgeeks.org/operating-systems-set-9/
✍
Write a Testimonial

Operating Systems | Set 8

​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Set 8 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Following questions have been asked in GATE 2009 CS exam.
1) In which one of the following page replacement policies, Belady’s anomaly may occur?
(A) FIFO
(B) Optimal
(C) LRU
(D) MRU
Answer (A)
Belady’s anomaly proves that it is possible to have more page faults when increasing the number of page frames while using the First in First Out (FIFO) page replacement algorithm.
See the wiki page for an example of increasing page faults with number of page frames.

2) The essential content(s) in each entry of a page table is / are
(A) Virtual page number
(B) Page frame number
(C) Both virtual page number and page frame number
(D) Access right information
Answer (B)
A page table entry must contain Page frame number. Virtual page number is typically used as index in page table to get the corresponding page frame number. See this for details.
3) Consider a system with 4 types of resources R1 (3 units), R2 (2 units), R3 (3 units), R4 (2 units). A non-preemptive resource allocation policy is used. At any given instance, a request is not entertained if it
cannot be completely satisfied. Three processes P1, P2, P3 request the sources as follows if executed independently.
​
Process P1: ​
t=0: requests 2 units of R2 ​
t=1: requests 1 unit of R3 ​
t=3: requests 2 units of R1 ​
t=5: releases 1 unit of R2
​
and 1 unit of R1. ​
t=7: releases 1 unit of R3 ​
t=8: requests 2 units of R4 ​
t=10: Finishes​
​
Process P2: ​
t=0: requests 2 units of R3 ​
t=2: requests 1 unit of R4 ​
t=4: requests 1 unit of R1 ​
t=6: releases 1 unit of R3 ​
t=8: Finishes​
​
Process P3: ​
t=0: requests 1 unit of R4 ​
t=2: requests 2 units of R1 ​
t=5: releases 2 units of R1 ​
t=7: requests 1 unit of R2 ​
t=8: requests 1 unit of R3 ​
t=9: Finishes​

Which one of the following statements is TRUE if all three processes run concurrently starting at time t=0?
(A) All processes will finish without any deadlock
(B) Only P1 and P2 will be in deadlock.
(C) Only P1 and P3 will be in a deadlock.
(D) All three processes will be in deadlock
Answer (A)
We can apply the following Deadlock Detection algorithm and see that there is no process waiting indefinitely for a resource. See this for deadlock detection algorithm.
4) Consider a disk system with 100 cylinders. The requests to access the cylinders occur in following sequence:
4, 34, 10, 7, 19, 73, 2, 15, 6, 20
Assuming that the head is currently at cylinder 50, what is the time taken to satisfy all requests if it takes 1ms to move from one cylinder to adjacent one and shortest seek time first policy is used?
(A) 95ms
(B) 119ms
(C) 233ms
(D) 276ms
Answer (B)
4, 34, 10, 7, 19, 73, 2, 15, 6, 20
Since shortest seek time first policy is used, head will first move to 34. This move will cause 16*1 ms. After 34, head will move to 20 which will cause 14*1 ms. And so on. So cylinders are accessed in following order
34, 20, 19, 15, 10, 7, 6, 4, 2, 73 and total time will be (16 + 14 + 1 + 4 + 5 + 3 + 1 + 2 + 2 + 71)*1 = 119 ms.
Please see GATE Corner for all previous year paper/solutions/explanations, syllabus, important dates, notes, etc.

Source
https://www.geeksforgeeks.org/operating-systems-set-8/
✍
Write a Testimonial

Operating Systems | Set 7
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Set 7 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Following questions have been asked in GATE CS exam.
1) Let the time taken to switch between user and kernel modes of execution be t1 while the time taken to switch between two processes be t2. Which of the following is TRUE? (GATE CS 2011)
(A) t1 > t2
(B) t1 = t2
(C) t1 < t2 (D) Nothing can be said about the relation between t1 and t2 Answer: - (C) Process switching involves mode switch. Context switching can occur only in kernel mode.
2) A system uses FIFO policy for page replacement. It has 4 page frames with no pages loaded to begin with. The system first accesses 100 distinct pages in some order and then accesses the same 100 pages but
now in the reverse order. How many page faults will occur? (GATE CS 2010)
(A) 196
(B) 192
(C) 197
(D) 195
Answer (A)
Access to 100 pages will cause 100 page faults. When these pages are accessed in reverse order, the first four accesses will not cause page fault. All other access to pages will cause page faults. So total number of page
faults will be 100 + 96.
3) Which of the following statements are true? (GATE CS 2010)
I. Shortest remaining time first scheduling may cause starvation
II. Preemptive scheduling may cause starvation
III. Round robin is better than FCFS in terms of response time
(A) I only
(B) I and III only
(C) II and III only
(D) I, II and III

Answer (D)
I) Shortest remaining time first scheduling is a preemptive version of shortest job scheduling. It may cause starvation as shorter processes may keep coming and a long CPU burst process never gets CPU.
II) Preemption may cause starvation. If priority based scheduling with preemption is used, then a low priority process may never get CPU.
III) Round Robin Scheduling improves response time as all processes get CPU after a specified time.
4) Consider the methods used by processes P1 and P2 for accessing their critical sections whenever needed, as given below. The initial values of shared boolean variables S1 and S2 are randomly assigned.
​
Method Used by P1​
while (S1 == S2) ;​
Critica1 Section​
S1 = S2;​
​
Method Used by P2​
while (S1 != S2) ;​
Critica1 Section​
S2 = not (S1);​

Which one of the following statements describes the properties achieved? (GATE CS 2010)
(A) Mutual exclusion but not progress
(B) Progress but not mutual exclusion
(C) Neither mutual exclusion nor progress
(D) Both mutual exclusion and progress
Answer (A)
It can be easily observed that the Mutual Exclusion requirement is satisfied by the above solution, P1 can enter critical section only if S1 is not equal to S2, and P2 can enter critical section only if S1 is equal to S2.
Progress Requirement is not satisfied. Let us first see definition of Progress Requirement.
Progress Requirement: If no process is executing in its critical section and there exist some processes that wishes to enter their critical section, then the selection of the processes that will enter the critical section next
cannot be postponed indefinitely.
If P1 or P2 want to re-enter the critical section, then they cannot even if there is other process running in critical section.
Please see GATE Corner for all previous year paper/solutions/explanations, syllabus, important dates, notes, etc.

Source
https://www.geeksforgeeks.org/operating-systems-set-7/
✍
Write a Testimonial

Operating Systems | Set 6
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Set 6 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x

×​
Suggest a Topic

Select a Category​
menu
Following questions have been asked in GATE 2011 CS exam.
1) A thread is usually defined as a ‘light weight process’ because an operating system (OS) maintains smaller data structures for a thread than for a process. In relation to this, which of the followings is
TRUE?
(A) On per-thread basis, the OS maintains only CPU register state
(B) The OS does not maintain a separate stack for each thread
(C) On per-thread basis, the OS does not maintain virtual memory state
(D) On per thread basis, the OS maintains only scheduling and accounting information.
Answer (C)
Threads share address space of Process. Virtually memory is concerned with processes not with Threads.

2) Let the page fault service time be 10ms in a computer with average memory access time being 20ns. If one page fault is generated for every 10^6 memory accesses, what is the effective access time for the
memory?
(A) 21ns
(B) 30ns
(C) 23ns
(D) 35ns
Answer (B)
Let P be the page fault rate​
Effective Memory Access Time = p * (page fault service time) + ​
(1 - p) * (Memory access time)​
= ( 1/(10^6) )* 10 * (10^6) ns +​
(1 - 1/(10^6)) * 20 ns​
= 30 ns (approx)
​

3) An application loads 100 libraries at startup. Loading each library requires exactly one disk access. The seek time of the disk to a random location is given as 10ms. Rotational speed of disk is 6000rpm. If all
100 libraries are loaded from random locations on the disk, how long does it take to load all libraries? (The time to transfer data from the disk block once the head has been positioned at the start of the block
may be neglected)
(A) 0.50s
(B) 1.50s
(C) 1.25s
(D) 1.00s
Answer (B)
Since transfer time can be neglected, the average access time is sum of average seek time and average rotational latency. Average seek time for a random location time is given as 10 ms. The average rotational latency is
half of the time needed for complete rotation. It is given that 6000 rotations need 1 minute. So one rotation will take 60/6000 seconds which is 10 ms. Therefore average rotational latency is half of 10 ms, which is 5ms.
Average disk access time = seek time + rotational latency ​
= 10 ms + 5 ms ​
= 15 ms​
For 100 libraries, the average disk access time will be 15*100 ms​

4. Consider the following table of arrival time and burst time for three processes P0, P1 and P2.
Process
P0
P1
P2

Arrival time
0 ms
1 ms
2 ms

Burst Time​
9 ms​
4 ms​
9 ms​

The pre-emptive shortest job first scheduling algorithm is used. Scheduling is carried out only at arrival or completion of processes. What is the average waiting time for the three processes?
(A) 5.0 ms
(B) 4.33 ms
(C) 6.33 ms
(D) 7.33 ms
Answer: – (A)
Process P0 is allocated processor at 0 ms as there is no other process in ready queue. P0 is preempted after 1 ms as P1 arrives at 1 ms and burst time for P1 is less than remaining time of P0. P1 runs for 4ms. P2 arrived at
2 ms but P1 continued as burst time of P2 is longer than P1. After P1 completes, P0 is scheduled again as the remaining time for P0 is less than the burst time of P2.
P0 waits for 4 ms, P1 waits for 0 ms amd P2 waits for 11 ms. So average waiting time is (0+4+11)/3 = 5.
Please see GATE Corner for all previous year paper/solutions/explanations, syllabus, important dates, notes, etc.

Source
https://www.geeksforgeeks.org/operating-systems-set-6/
✍
Write a Testimonial

Operating Systems | Set 5
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Set 5 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu

Following questions have been asked in GATE 2012 exam.
1. A process executes the code
​
fork ();​
fork ();​
fork ();​

The total number of child processes created is
(A) 3
(B) 4
(C) 7
(D) 8

Answer (C)
Let us put some label names for the three lines
​
fork ();
fork ();
fork ();

// Line 1​
// Line 2​
// Line 3​

​
L1
/
/
L3

\
L3

// There will be 1 child process created by line 1​
\​
L2

L2
/
L3

// There will be 2 child processes created by line 2​
\​
L3

// There will be 4 child processes created by line 3​

We can also use direct formula to get the number of child processes. With n fork statements, there are always 2^n – 1 child processes. Also see this post for more details.
2. consider the 3 processes, P1, P2 and P3 shown in the table
​
Process
P1
P2
P3

Arrival time
0
1
3

Time unit required​
5​
7​
4​

The completion order of the 3 processes under the policies FCFS and RRS (round robin scheduling with CPU quantum of 2 time units) are
(A) FCFS: P1, P2, P3 RR2: P1, P2, P3
(B) FCFS: P1, P3, P2 RR2: P1, P3, P2
(C) FCFS: P1, P2, P3 RR2: P1, P3, P2
(D) FCFS: P1, P3, P2 RR2: P1, P2, P3
Answer (C)
3. Consider the virtual page reference string
1, 2, 3, 2, 4, 1, 3, 2, 4, 1
On a demand paged virtual memory system running on a computer system that main memory size of 3 pages frames which are initially empty. Let LRU, FIFO and OPTIMAL denote the number of page
faults under the corresponding page replacements policy. Then
(A) OPTIMAL < LRU < FIFO (B) OPTIMAL < FIFO < LRU (C) OPTIMAL = LRU (D) OPTIMAL = FIFO Answer (B) The OPTIMAL will be 5, FIFO 6 and LRU 9.
4. A file system with 300 GByte uses a file descriptor with 8 direct block address. 1 indirect block address and 1 doubly indirect block address. The size of each disk block is 128 Bytes and the size of each disk
block address is 8 Bytes. The maximum possible file size in this file system is
(A) 3 Kbytes
(B) 35 Kbytes
(C) 280 Bytes
(D) Dependent on the size of the disk
Answer (B)
Total number of possible addresses stored in a disk block = 128/8 = 16
Maximum number of addressable bytes due to direct address block = 8*128
Maximum number of addressable bytes due to 1 single indirect address block = 16*128
Maximum number of addressable bytes due to 1 double indirect address block = 16*16*128
The maximum possible file size = 8*128 + 16*128 + 16*16*128 = 35KB
Please see GATE Corner for all previous year paper/solutions/explanations, syllabus, important dates, notes, etc.

Source
https://www.geeksforgeeks.org/operating-systems-set-5/
✍
Write a Testimonial

Mutex vs Semaphore
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Mutex vs Semaphore - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
What are the differences between Mutex vs Semaphore? When to use mutex and when to use semaphore?
Concrete understanding of Operating System concepts is required to design/develop smart applications. Our objective is to educate the reader on these concepts and learn from other expert geeks.
As per operating system terminology, mutex and semaphore are kernel resources that provide synchronization services (also called as synchronization primitives). Why do we need such synchronization primitives? Won’t
be only one sufficient? To answer these questions, we need to understand few keywords. Please read the posts on atomicity and critical section. We will illustrate with examples to understand these concepts well, rather
than following usual OS textual description.

The producer-consumer problem:
Note that the content is generalized explanation. Practical details vary with implementation.
Consider the standard producer-consumer problem. Assume, we have a buffer of 4096 byte length. A producer thread collects the data and writes it to the buffer. A consumer thread processes the collected data from the
buffer. Objective is, both the threads should not run at the same time.
Using Mutex:
A mutex provides mutual exclusion, either producer or consumer can have the key (mutex) and proceed with their work. As long as the buffer is filled by producer, the consumer needs to wait, and vice versa.
At any point of time, only one thread can work with the entire buffer. The concept can be generalized using semaphore.
Using Semaphore:
A semaphore is a generalized mutex. In lieu of single buffer, we can split the 4 KB buffer into four 1 KB buffers (identical resources). A semaphore can be associated with these four buffers. The consumer and producer
can work on different buffers at the same time.
Misconception:
There is an ambiguity between binary semaphore and mutex. We might have come across that a mutex is binary semaphore. But they are not! The purpose of mutex and semaphore are different. May be, due to similarity in
their implementation a mutex would be referred as binary semaphore.

Strictly speaking, a mutex is locking mechanism used to synchronize access to a resource. Only one task (can be a thread or process based on OS abstraction) can acquire the mutex. It means there is ownership associated
with mutex, and only the owner can release the lock (mutex).
Semaphore is signaling mechanism (“I am done, you can carry on” kind of signal). For example, if you are listening songs (assume it as one task) on your mobile and at the same time your friend calls you, an interrupt is
triggered upon which an interrupt service routine (ISR) signals the call processing task to wakeup.
General Questions:
1. Can a thread acquire more than one lock (Mutex)?
Yes, it is possible that a thread is in need of more than one resource, hence the locks. If any lock is not available the thread will wait (block) on the lock.
2. Can a mutex be locked more than once?
A mutex is a lock. Only one state (locked/unlocked) is associated with it. However, a recursive mutex can be locked more than once (POSIX complaint systems), in which a count is associated with it, yet retains only one
state (locked/unlocked). The programmer must unlock the mutex as many number times as it was locked.
3. What happens if a non-recursive mutex is locked more than once.
Deadlock. If a thread which had already locked a mutex, tries to lock the mutex again, it will enter into the waiting list of that mutex, which results in deadlock. It is because no other thread can unlock the mutex. An
operating system implementer can exercise care in identifying the owner of mutex and return if it is already locked by same thread to prevent deadlocks.
4. Are binary semaphore and mutex same?

No. We suggest to treat them separately, as it is explained signalling vs locking mechanisms. But a binary semaphore may experience the same critical issues (e.g. priority inversion) associated with mutex. We will cover
these in later article.
A programmer can prefer mutex rather than creating a semaphore with count 1.
5. What is a mutex and critical section?
Some operating systems use the same word critical section in the API. Usually a mutex is costly operation due to protection protocols associated with it. At last, the objective of mutex is atomic access. There are other
ways to achieve atomic access like disabling interrupts which can be much faster but ruins responsiveness. The alternate API makes use of disabling interrupts.
6. What are events?
The semantics of mutex, semaphore, event, critical section, etc… are same. All are synchronization primitives. Based on their cost in using them they are different. We should consult the OS documentation for exact
details.
7. Can we acquire mutex/semaphore in an Interrupt Service Routine?
An ISR will run asynchronously in the context of current running thread. It is not recommended to query (blocking call) the availability of synchronization primitives in an ISR. The ISR are meant be short, the call to
mutex/semaphore may block the current running thread. However, an ISR can signal a semaphore or unlock a mutex.
8. What we mean by “thread blocking on mutex/semaphore” when they are not available?
Every synchronization primitive has a waiting list associated with it. When the resource is not available, the requesting thread will be moved from the running list of processor to the waiting list of the synchronization
primitive. When the resource is available, the higher priority thread on the waiting list gets the resource (more precisely, it depends on the scheduling policies).

9. Is it necessary that a thread must block always when resource is not available?
Not necessary. If the design is sure ‘what has to be done when resource is not available‘, the thread can take up that work (a different code branch). To support application requirements the OS provides non-blocking API.
For example POSIX pthread_mutex_trylock() API. When mutex is not available the function returns immediately whereas the API pthread_mutex_lock() blocks the thread till resource is available.
References:
http://www.netrino.com/node/202
http://doc.trolltech.com/4.7/qsemaphore.html
Also compare mutex/semaphores with Peterson’s algorithm and Dekker’s algorithm. A good reference is the Art of Concurrency book. Also explore reader locks and writer locks in Qt documentation.
Exercise:
Implement a program that prints a message “An instance is running” when executed more than once in the same session. For example, if we observe word application or Adobe reader in Windows, we can see only one
instance in the task manager. How to implement it?
Article compiled by Venki. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.

Source
https://www.geeksforgeeks.org/mutex-vs-semaphore/
✍
Write a Testimonial

Critical Section in Synchronization
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Critical Section in Synchronization - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Critical Section:
When more than one processes access a same code segment that segment is known as critical section. Critical section contains shared variables or resources which are needed to be synchronized to maintain consistency of
data variable.
In simple terms a critical section is group of instructions/statements or region of code that need to be executed atomically (read this post for atomicity), such as accessing a resource (file, input or output port, global data,
etc.).
In concurrent programming, if one thread tries to change the value of shared data at the same time as another thread tries to read the value (i.e. data race across threads), the result is unpredictable.
The access to such shared variable (shared memory, shared files, shared port, etc…) to be synchronized. Few programming languages have built-in support for synchronization.

It is critical to understand the importance of race condition while writing kernel mode programming (a device driver, kernel thread, etc.). since the programmer can directly access and modifying kernel data structures.

A simple solution to the critical section can be thought as shown below,
acquireLock();​
Process Critical Section​
releaseLock();

A thread must acquire a lock prior to executing a critical section. The lock can be acquired by only one thread. There are various ways to implement locks in the above pseudo code. Let us discuss them in future articles.

Improved By : VaibhavRai3, heart_bleed, RaviPRakashJi

Source
https://www.geeksforgeeks.org/g-fact-70/
✍
Write a Testimonial

Operating Systems | Set 4
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Set 4 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Following questions have been asked in GATE CS exam.

1. Using a larger block size in a fixed block size file system leads to (GATE CS 2003)
a) better disk throughput but poorer disk space utilization
b) better disk throughput and better disk space utilization
c) poorer disk throughput but better disk space utilization
d) poorer disk throughput and poorer disk space utilization
Answer (a)
If block size is large then seek time is less (fewer blocks to seek) and disk performance is improved, but remember larger block size also causes waste of disk space.

2. Consider the following statements with respect to user-level threads and kernel supported threads
i. context switch is faster with kernel-supported threads
ii. for user-level threads, a system call can block the entire process
iii. Kernel supported threads can be scheduled independently
iv. User level threads are transparent to the kernel
Which of the above statements are true? (GATE CS 2004)
a) (ii), (iii) and (iv) only
b) (ii) and (iii) only
c) (i) and (iii) only
d) (i) and (ii) only
Answer(a)
http://en.wikipedia.org/wiki/Thread_%28computer_science%29
3. The minimum number of page frames that must be allocated to a running process in a virtual memory environment is determined by (GATE CS 2004)
a) the instruction set architecture
b) page size
c) physical memory size
d) number of processes in memory
Answer (a)
Each process needs minimum number of pages based on instruction set architecture. Example IBM 370: 6 pages to handle MVC (storage to storage move) instruction
Instruction is 6 bytes, might span 2 pages.
2 pages to handle from.
2 pages to handle to.
4. In a system with 32 bit virtual addresses and 1 KB page size, use of one-level page tables for virtual to physical address translation is not practical because of (GATE CS 2003)
a) the large amount of internal fragmentation
b) the large amount of external fragmentation
c) the large memory overhead in maintaining page tables
d) the large computation overhead in the translation process
Answer (c)
Since page size is too small it will make size of page tables huge.
​
Size of page table =​
(total number of page table entries) *(size of a page table entry)​

Let us see how many entries are there in page table
​
Number of entries in page table =​
(virtual address space size)/(page size)​
= (2^32)/(2^10) ​
= 2^22​

Now, let us see how big each entry is.

If size of physical memory is 512 MB then number of bits required to address a byte in 512 MB is 29. So, there will be (512MB)/(1KB) = (2^29)/(2^10) page frames in physical memory. To address a page frame 19 bits
are required. Therefore, each entry in page table is required to have 19 bits.
​
Note that page table entry also holds auxiliary information about the page such ​
as a present bit, a dirty or modified bit, address space or process ID information, ​
amongst others. So size of page table ​
> (total number of page table entries) *(size of a page table entry)​
> (2^22 *19) bytes​
> 9.5 MB​

And this much memory is required for each process because each process maintains its own page table. Also, size of page table will be more for physical memory more than 512MB. Therefore, it is advised to use
multilevel page table for such scenarios.
References:
http://barbara.stattenfield.org/ta/cs162/section-notes/sec8.txt
http://en.wikipedia.org/wiki/Page_table
Please see GATE Corner for all previous year paper/solutions/explanations, syllabus, important dates, notes, etc.

Source
https://www.geeksforgeeks.org/operating-systems-set-4/
✍
Write a Testimonial

Operating Systems | Set 3

​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Set 3 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Following questions have been asked in GATE CS exam.
1. Suppose the time to service a page fault is on the average 10 milliseconds, while a memory access takes 1 microsecond. Then a 99.99% hit ratio results in average memory access time of (GATE CS 2000)
(a) 1.9999 milliseconds
(b) 1 millisecond
(c) 9.999 microseconds
(d) 1.9999 microseconds
Answer: (d)
Explanation:
​
Average memory access time =​
[(% of page miss)*(time to service a page fault) +​
(% of page hit)*(memory access time)]/100​

So, average memory access time in microseconds is.
(99.99*1 + 0.01*10*1000)/100 = (99.99+100)/1000 = 199.99/1000 =1.9999 µs

2. Which of the following need not necessarily be saved on a context switch between processes? (GATE CS 2000)
(a) General purpose registers
(b) Translation look-aside buffer
(c) Program counter
(d) All of the above
Answer: (b)
Explanation:
In a process context switch, the state of the first process must be saved somehow, so that, when the scheduler gets back to the execution of the first process, it can restore this state and continue.
The state of the process includes all the registers that the process may be using, especially the program counter, plus any other operating system specific data that may be necessary.
A Translation lookaside buffer (TLB) is a CPU cache that memory management hardware uses to improve virtual address translation speed. A TLB has a fixed number of slots that contain page table entries, which map
virtual addresses to physical addresses. On a context switch, some TLB entries can become invalid, since the virtual-to-physical mapping is different. The simplest strategy to deal with this is to completely flush the TLB.
References:
http://en.wikipedia.org/wiki/Context_switch
http://en.wikipedia.org/wiki/Translation_lookaside_buffer#Context_switch
3. Where does the swap space reside ? (GATE 2001)
(a) RAM
(b) Disk
(c) ROM
(d) On-chip cache
Answer: (b)
Explanation:
Swap space is an area on disk that temporarily holds a process memory image. When physical memory demand is sufficiently low, process memory images are brought back into physical memory from the swap area.
Having sufficient swap space enables the system to keep some physical memory free at all times.
References:
http://docs.hp.com/en/B2355-90672/ch06s02.html
4. Which of the following does not interrupt a running process? (GATE CS 2001)
(a) A device
(b) Timer
(c) Scheduler process
(d) Power failure
Answer: (c)
Explanation:
Scheduler process doesn’t interrupt any process, it’s Job is to select the processes for following three purposes.
Long-term scheduler(or job scheduler) –selects which processes should be brought into the ready queue
Short-term scheduler(or CPU scheduler) –selects which process should be executed next and allocates CPU.
Mid-term Scheduler (Swapper)- present in all systems with virtual memory, temporarily removes processes from main memory and places them on secondary memory (such as a disk drive) or vice versa. The mid-term
scheduler may decide to swap out a process which has not been active for some time, or a process which has a low priority, or a process which is page faulting frequently, or a process which is taking up a large amount of
memory in order to free up main memory for other processes, swapping the process back in later when more memory is available, or when the process has been unblocked and is no longer waiting for a resource.
5. Which of the following scheduling algorithms is non-preemptive? (GATE CS 2002)
a) Round Robin
b) First-In First-Out
c) Multilevel Queue Scheduling
d) Multilevel Queue Scheduling with Feedback
Answer: (b)

Source

https://www.geeksforgeeks.org/operating-systems-set-3/
✍
Write a Testimonial

Operating Systems | Set 2
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Set 2 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Following questions have been asked in GATE CS exam.
1. Consider a machine with 64 MB physical memory and a 32-bit virtual address space. If the page size is 4KB, what is the approximate size of the page table? (GATE 2001)
(a) 16 MB
(b) 8 MB
(c) 2 MB
(d) 24 MB
Answer: (c)
Explanation:
A page entry is used to get address of physical memory. Here we assume that single level of Paging is happening. So the resulting page table will contain entries for all the pages of the Virtual address space.
​
Number of entries in page table = ​
(virtual address space size)/(page size)

​

Using above formula we can say that there will be 2^(32-12) = 2^20 entries in page table.
No. of bits required to address the 64MB Physical memory = 26.
So there will be 2^(26-12) = 2^14 page frames in the physical memory. And page table needs to store the address of all these 2^14 page frames. Therefore, each page table entry will contain 14 bits address of the page
frame and 1 bit for valid-invalid bit.
Since memory is byte addressable. So we take that each page table entry is 16 bits i.e. 2 bytes long.

​
Size of page table = ​
(total number of page table entries) *(size of a page table entry) ​
= (2^20 *2) = 2MB​

For the clarity of the concept, please see the following figure. As per our question, here p = 20, d = 12 and f = 14.

2. Consider Peterson’s algorithm for mutual exclusion between two concurrent processes i and j. The program executed by process is shown below.
​
repeat
​
flag [i] = true; ​
turn = j; ​
while ( P ) do no-op; ​
Enter critical section, perform actions, then exit critical ​
section ​
flag [ i ] = false; ​
Perform other non-critical section actions. ​
until false;

For the program to guarantee mutual exclusion, the predicate P in the while loop should be (GATE 2001)
a) flag [j] = true and turn = i
b) flag [j] = true and turn = j
c) flag [i] = true and turn = j
d) flag [i] = true and turn = i
Answer: (b)
Basically, Peterson’s algorithm provides guaranteed mutual exclusion by using the two following constructs – flag[] and turn. flag[] controls that the willingness of a process to be entered in critical section. While turn
controls the process that is allowed to be entered in critical section. So by replacing P with the following,
flag [j] = true and turn = j
process i will not enter critical section if process j wants to enter critical section and it is process j’s turn to enter critical section. The same concept can be extended for more than two processes. For details, refer the
following.
References:
http://en.wikipedia.org/wiki/Peterson%27s_algorithm
3 More than one word are put in one cache block to (GATE 2001)
(a) exploit the temporal locality of reference in a program
(b) exploit the spatial locality of reference in a program
(c) reduce the miss penalty
(d) none of the above
Answer: (b)
Temporal locality refers to the reuse of specific data and/or resources within relatively small time durations. Spatial locality refers to the use of data elements within relatively close storage locations.

To exploit the spatial locality, more than one word are put into cache block.
References:
http://en.wikipedia.org/wiki/Locality_of_reference
4. Which of the following statements is false? (GATE 2001)
a) Virtual memory implements the translation of a program’s address space into physical memory address space
b) Virtual memory allows each program to exceed the size of the primary memory
c) Virtual memory increases the degree of multiprogramming
d) Virtual memory reduces the context switching overhead
Answer: (d)
In a system with virtual memory context switch includes extra overhead in switching of address spaces.
References:
http://www.itee.adfa.edu.au/~spike/CSA2/Lectures00/lecture.vm.htm
5. Consider a set of n tasks with known runtimes r1, r2, … rn to be run on a uniprocessor machine. Which of the following processor scheduling algorithms will result in the maximum throughput? (GATE
2001)
(a) Round-Robin
(b) Shortest-Job-First
(c) Highest-Response-Ratio-Next
(d) First-Come-First-Served
Answer: (b)

Source
https://www.geeksforgeeks.org/operating-systems-set-2/
✍
Write a Testimonial

Operating Systems | Set 1
​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ Operating Systems | Set 1 - GeeksforGeeks​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​ ​
GeeksforGeeks​
x
×​
Suggest a Topic

Select a Category​
menu
Following questions have been asked in GATE CS exam.
1. Which of the following is NOT a valid deadlock prevention scheme? (GATE CS 2000)
(a) Release all resources before requesting a new resource
(b) Number the resources uniquely and never request a lower numbered resource than the last one requested.
(c) Never request a resource after releasing any resource
(d) Request and all required resources be allocated before execution.
Answer: (c)
References:
http://www.cs.jhu.edu/~yairamir/cs418/os4/sld013.htm
http://en.wikipedia.org/wiki/Deadlock

2. Let m[0]…m[4] be mutexes (binary semaphores) and P[0] …. P[4] be processes.
Suppose each process P[i] executes the following:
​
wait (m[i]); wait(m[(i+1) mode 4]);​
​
------​
​
release (m[i]); release (m[(i+1)mod 4]);​

This could cause (GATE CS 2000)
(a) Thrashing
(b) Deadlock
(c) Starvation, but not deadlock
(d) None of the above
Answer: (b)
Explanation:
You can easily see a deadlock in a situation where..
P[0] has acquired m[0] and waiting for m[1]
P[1] has acquired m[1] and waiting for m[2]
P[2] has acquired m[2] and waiting for m[3]
P[3] has acquired m[3] and waiting for m[0]
3. A graphics card has on board memory of 1 MB. Which of the following modes can the
card not support? (GATE CS 2000)
(a) 1600 x 400 resolution with 256 colours on a 17 inch monitor
(b) 1600 x 400 resolution with 16 million colours on a 14 inch monitor

(c) 800 x 400 resolution with 16 million colours on a 17 inch monitor
(d) 800 x 800 resolution with 256 colours on a 14 inch monitor
Answer: (b)
Explanation:
Monitor size doesn’t matter here. So, we can easily deduce that answer should be (b) as this has the highest memory requirements. Let us verify it.
Number of bits required to store a 16M colors pixel = ceil(log2(16*1000000)) = 24
Number of bytes required for 1600 x 400 resolution with 16M colors = (1600 * 400 * 24)/8 which is 192000000 (greater than 1MB).
4 Consider a virtual memory system with FIFO page replacement policy. For an arbitrary page access pattern, increasing the number of page frames in main memory will (GATE CS 2001)
a) Always decrease the number of page faults
b) Always increase the number of page faults
c) Some times increase the number of page faults
d) Never affect the number of page faults
Answer: (c)
Explanation:
Incrementing the number of page frames doesn’t always decrease the page faults (Belady’s Anomaly). For details see http://en.wikipedia.org/wiki/Belady%27s_anomaly
5. Which of the following requires a device driver? (GATE CS 2001)
a) Register
b) Cache
c) Main memory
d) Disk
Answer: (d)

Improved By : ShaliniSingh13

Source
https://www.geeksforgeeks.org/operating-systems-set-1/
✍
Write a Testimonial

